# Soluci√≥n: Modelo Sub-Entrenado

## Problema Detectado

Tu modelo `pewter_brock_battle.zip` fue entrenado con **solo ~1023 pasos** (una prueba r√°pida), lo cual es insuficiente para aprender estrategias de combate efectivas.

**S√≠ntomas:**
- ‚úÖ Ventana de PyBoy se abre (pantalla gris/est√°tica)
- ‚ùå Reward constante en 1843.04 (solo recompensa inicial, luego 0)
- ‚ùå Episodio termina en exactamente 2048 pasos (l√≠mite del entorno)
- ‚ùå El agente repite la misma acci√≥n sin progreso

## Causa Ra√≠z

Con solo 1023 pasos de entrenamiento:
- El modelo apenas explor√≥ el espacio de estados
- No aprendi√≥ secuencias de acciones efectivas
- PPO requiere miles de rollouts para convergencia

**M√≠nimo recomendado**: 40,000 pasos (lo que configuraste en `combat_plan_local`)

## Soluci√≥n Paso a Paso

### 1. Entrenar el Modelo Completo

Abre `Local_Train.ipynb` y ejecuta la **Celda 18** (Secci√≥n 6):

```python
# Esta celda ejecuta el plan completo de combate
combat_runs_local = train_plan(
    agent_key='combat',
    plan=combat_plan_local,  # Ya configurado con 40,000 pasos
    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,
    headless=DEFAULT_HEADLESS_LOCAL
)
```

**Tiempo estimado**: 
- GPU (RTX 3050): ~30-60 minutos
- CPU: ~2-4 horas

### 2. Verificar el Modelo Entrenado

Despu√©s del entrenamiento, verifica:

```python
from stable_baselines3 import PPO
model = PPO.load("models_local/combat/pewter_brock_battle.zip")
print(f"Pasos de entrenamiento: {model.num_timesteps:,}")
# Deber√≠a mostrar: ~40,000
```

### 3. Probar el Modelo Entrenado

```bash
python run_combat_agent_interactive.py --scenario pewter_brock --phase battle
```

**Resultado esperado:**
```
üì¶ Cargando modelo desde: ...\pewter_brock_battle.zip
   Pasos de entrenamiento del modelo: 40,960
   ‚úÖ Modelo bien entrenado

üéÆ Iniciando episodio (m√°x 10000 pasos)...

Paso 100/10000 | Reward: 2143.52 (+45.30) | Acci√≥n: A
Paso 200/10000 | Reward: 2398.76 (+32.18) | Acci√≥n: UP
üéØ Evento: battle_won
```

## Alternativa: Entrenar Baseline Ligero

Si solo quieres **comparar** sin esperar 40k pasos, usa la **Secci√≥n 11** de `Local_Train.ipynb`:

```python
# Entrena un baseline simple con los mismos 40k pasos
baseline_ligero_path = train_lightweight_baseline(
    scenario_id='pewter_brock',
    phase_name='battle',
    timesteps=40_000
)
```

Esto te dar√° un modelo PPO gen√©rico para comparar contra tu Combat Agent especializado.

## Por Qu√© 40,000 Pasos?

| Pasos | Estado del Modelo | Uso Recomendado |
|-------|-------------------|-----------------|
| 1,000 | Sin aprendizaje | Solo pruebas de c√≥digo |
| 10,000 | Aprendizaje b√°sico | Debugging r√°pido |
| 40,000 | **Competente** | **Evaluaci√≥n real** ‚úÖ |
| 100,000 | Experto | Publicaciones/benchmarks |
| 200,000+ | Maestr√≠a | Competiciones |

Tu configuraci√≥n actual (`40_000`) est√° en el punto √≥ptimo para:
- ‚úÖ Aprendizaje significativo
- ‚úÖ Tiempo de entrenamiento razonable
- ‚úÖ Comparaciones justas

## Verificaci√≥n Post-Entrenamiento

Despu√©s de entrenar 40k pasos, deber√≠as ver:

1. **Rewards variados**: No constantes, van cambiando
2. **Acciones diversas**: No solo una acci√≥n repetida
3. **Progreso visible**: Movimiento, combate, uso de items
4. **Episodios m√°s cortos**: Termina antes de 2048 pasos si gana

## Troubleshooting

### "El entrenamiento tarda mucho"
- ‚úÖ Verifica GPU activa: `torch.cuda.is_available()` debe ser `True`
- ‚úÖ Usa `headless=True` en el plan de entrenamiento
- ‚úÖ Reduce a 20,000 pasos si necesitas resultados r√°pidos

### "Quiero ver el progreso del entrenamiento"
La barra de progreso deber√≠a aparecer autom√°ticamente (ya instalaste tqdm/rich).

### "El modelo entrenado sigue atascado"
- Verifica el escenario: `pewter_brock_battle.state` debe existir
- Revisa los logs de entrenamiento: ¬øaument√≥ el reward promedio?
- Prueba con m√°s pasos: 60,000 o 100,000

---

**üéØ Acci√≥n Inmediata**: Ejecuta la celda 18 de `Local_Train.ipynb` ahora para entrenar el modelo completo.
