# Plan de Acci√≥n Inmediato - Proyecto Combat Agent

## üéØ Objetivo
Entrenar un agente PPO especializado en combates y compararlo cient√≠ficamente con el PPO baseline del repositorio original.

---

## üìÖ Cronograma Sugerido (3-5 d√≠as)

### D√≠a 1: Setup y Verificaci√≥n ‚úÖ

**Tareas:**
1. ‚úÖ Leer `README.md` y `TECHNICAL_ANALYSIS.md` para entender el proyecto
2. ‚úÖ Instalar dependencias: `pip install -r requirements.txt`
3. ‚úÖ Verificar que tienes la ROM y archivos `.state`:
   ```powershell
   Test-Path ..\PokemonRed.gb
   Test-Path ..\has_pokedex_nballs.state
   ```
4. ‚úÖ Ejecutar prueba r√°pida (100K steps, ~10 min):
   ```bash
   python train_combat_agent.py --timesteps 100000 --num-envs 4 --headless
   ```

**Criterio de √âxito:**
- ‚úÖ Script corre sin errores
- ‚úÖ Ves output como: `step: 1000  victories: 2.00  hp_conserve: 15.00  W/L: 2/1`
- ‚úÖ Se crea directorio `sessions/combat_session_XXXXX/`

---

### D√≠a 2: Entrenamiento Combat Agent üöÄ

**Tareas:**
1. Lanzar entrenamiento completo (1M steps, ~2-3 horas con 16 CPUs):
   ```bash
   python train_combat_agent.py \
       --timesteps 1000000 \
       --num-envs 16 \
       --session-name combat_v1 \
       --checkpoint-freq 100000 \
       --headless
   ```

2. Mientras entrena, monitorear en otra terminal:
   ```bash
   cd sessions/combat_v1
   tensorboard --logdir .
   # Abrir: http://localhost:6006
   ```

3. Verificar checkpoints cada 100K steps:
   ```bash
   ls sessions/combat_v1/combat_agent_*.zip
   # Deber√≠as ver: combat_agent_100000_steps.zip, ..., combat_agent_1000000_steps.zip
   ```

**Criterio de √âxito:**
- ‚úÖ Entrenamiento completa 1M steps
- ‚úÖ Recompensas incrementan con el tiempo (ver TensorBoard)
- ‚úÖ Win Rate aumenta (de ~40% inicial a >70% final)
- ‚úÖ Modelo final guardado: `combat_agent_final.zip`

---

### D√≠a 3: Entrenar Baseline PPO (para comparaci√≥n)

**Opci√≥n A: Usar modelo pre-existente del repositorio original**
```bash
# Si ya entrenaron en PokemonRedExperiments antes:
ls ../PokemonRedExperiments/baselines/session_*/poke_*.zip
# Usar el m√°s reciente
```

**Opci√≥n B: Entrenar baseline desde cero**
```bash
cd ../PokemonRedExperiments/baselines

# Editar run_baseline_parallel.py:
# Cambiar: num_cpu = 16 (l√≠nea ~35)
# Cambiar: learn_steps = 10 (l√≠nea ~52, para 1M steps total)

python run_baseline_parallel.py
```

**Criterio de √âxito:**
- ‚úÖ Tienes un modelo baseline PPO entrenado por ~1M steps
- ‚úÖ Modelo guardado en: `../PokemonRedExperiments/baselines/session_XXXXX/poke_YYYYY_steps.zip`

---

### D√≠a 4: Evaluaci√≥n y Comparaci√≥n üìä

**Tareas:**
1. Ejecutar comparaci√≥n cient√≠fica:
   ```bash
   cd PokemonCombatAgent
   
   python compare_agents.py \
       --combat-agent sessions/combat_v1/combat_agent_final \
       --baseline-agent ../PokemonRedExperiments/baselines/session_XXXXX/poke_1000000_steps \
       --episodes 100 \
       --output-dir comparison_results
   ```

2. Analizar resultados:
   ```bash
   # Ver resumen
   cat comparison_results/summary.json
   
   # Ver comparaci√≥n detallada
   python -c "import pandas as pd; df = pd.read_csv('comparison_results/comparison_results.csv'); print(df)"
   ```

3. Visualizar m√©tricas:
   ```python
   import pandas as pd
   import matplotlib.pyplot as plt
   
   combat = pd.read_csv('comparison_results/combat_agent_metrics.csv')
   baseline = pd.read_csv('comparison_results/baseline_agent_metrics.csv')
   
   # Gr√°fico Win Rate
   plt.figure(figsize=(10, 6))
   plt.hist([combat['win_rate'], baseline['win_rate']], label=['Combat Agent', 'Baseline'])
   plt.xlabel('Win Rate')
   plt.ylabel('Frequency')
   plt.legend()
   plt.title('Win Rate Distribution')
   plt.savefig('win_rate_comparison.png')
   ```

**Criterio de √âxito:**
- ‚úÖ Combat Agent tiene **mayor Win Rate** que Baseline (esperado: +15-20%)
- ‚úÖ Combat Agent **conserva m√°s HP** (esperado: +20-30%)
- ‚úÖ Diferencias son **estad√≠sticamente significativas** (p < 0.05)

---

### D√≠a 5: An√°lisis Cualitativo y Reporte üìù

**Tareas:**
1. Ver agente jugando interactivamente:
   ```bash
   python demo_interactive.py --model sessions/combat_v1/combat_agent_final --episodes 5
   ```

2. Analizar comportamientos espec√≠ficos:
   - ¬øUsa curaci√≥n apropiadamente (solo cuando HP < 50%)?
   - ¬øCambia Pok√©mon cuando tiene desventaja de tipo?
   - ¬øEvita combates innecesarios cuando est√° d√©bil?

3. Crear reporte final con:
   - **Introducci√≥n**: Problema (agente generalista vs especialista)
   - **Metodolog√≠a**: Arquitectura, recompensas, entrenamiento
   - **Resultados**: Tablas comparativas, gr√°ficos
   - **An√°lisis**: Por qu√© funciona mejor (recompensas enfocadas)
   - **Conclusiones**: Combat-specialized PPO > Baseline PPO
   - **Trabajo Futuro**: Agentes para puzzles, exploraci√≥n, etc.

**Criterio de √âxito:**
- ‚úÖ Tienes evidencia visual de que el agente juega inteligentemente
- ‚úÖ Reporte con datos cuantitativos (tablas, p-values)
- ‚úÖ Reporte con datos cualitativos (videos, observaciones)

---

## üìã Checklist de Entregables

### C√≥digo
- [ ] `PokemonCombatAgent/` - Proyecto completo
- [ ] `sessions/combat_v1/combat_agent_final.zip` - Modelo entrenado
- [ ] `comparison_results/` - Resultados de evaluaci√≥n

### Datos
- [ ] `combat_agent_metrics.csv` - M√©tricas del combat agent (100 episodios)
- [ ] `baseline_agent_metrics.csv` - M√©tricas del baseline (100 episodios)
- [ ] `comparison_results.csv` - Comparaci√≥n estad√≠stica

### Visualizaciones
- [ ] Gr√°fico: Win Rate (Combat vs Baseline)
- [ ] Gr√°fico: HP Conservation (Combat vs Baseline)
- [ ] Gr√°fico: Evoluci√≥n de recompensas durante entrenamiento (TensorBoard)
- [ ] Video: Agente jugando (opcional, usar `demo_interactive.py`)

### Documentaci√≥n
- [ ] README.md - Explicaci√≥n del proyecto
- [ ] TECHNICAL_ANALYSIS.md - Por qu√© funciona vs TEL351
- [ ] Reporte Final (PDF) - Con toda la evidencia

---

## üö® Problemas Comunes y Soluciones

### Problema 1: "Training very slow"
**S√≠ntoma:** 1000 steps toman >10 minutos

**Soluci√≥n:**
```bash
# Reducir entornos paralelos y action_freq
python train_combat_agent.py --num-envs 4 --action-freq 12
```

### Problema 2: "Agent not learning (reward stuck)"
**S√≠ntoma:** Reward se queda en 0-10 constantemente

**Diagn√≥stico:**
```bash
# Ver si est√° en combates
grep "W/L:" sessions/combat_v1/agent_stats_*.csv.gz
```

**Soluci√≥n:**
```bash
# Aumentar exploraci√≥n
python train_combat_agent.py --ent-coef 0.02

# O usar estado inicial diferente (m√°s cerca de combates)
python train_combat_agent.py --init-state ../pewter_gym_entrance.state
```

### Problema 3: "Out of memory"
**S√≠ntoma:** `CUDA out of memory` o `MemoryError`

**Soluci√≥n:**
```bash
# Reducir batch size y n√∫mero de entornos
python train_combat_agent.py --num-envs 4 --batch-size 256
```

### Problema 4: "ROM not found"
**S√≠ntoma:** `FileNotFoundError: ../PokemonRed.gb`

**Soluci√≥n:**
```powershell
# Verificar que ROM est√° en directorio correcto
Copy-Item "C:\ruta\a\PokemonRed.gb" "..\PokemonRed.gb"
```

---

## üéØ Metas de Desempe√±o Esperadas

### M√©tricas M√≠nimas Aceptables
- **Win Rate**: >70% (vs ~60% baseline)
- **HP Conservation**: >60% (vs ~40% baseline)
- **Deaths per Episode**: <1.0 (vs ~1.5 baseline)

### M√©tricas Excelentes
- **Win Rate**: >85%
- **HP Conservation**: >75%
- **Deaths per Episode**: <0.5
- **p-value**: <0.01 (muy significativo)

---

## üìû Pr√≥ximos Pasos si Todo Funciona

1. **Publicar en GitHub** (si es proyecto p√∫blico)
2. **Crear demo video** (YouTube, mostrar agente vs humano/baseline)
3. **Extender a otros tipos**:
   - Puzzle Agent (para resolver laberintos r√°pido)
   - Explorer Agent (para encontrar objetos raros)
   - Hybrid Agent (combina combat + puzzle + exploration)
4. **Paper acad√©mico** (si es para tesis o publicaci√≥n)

---

## üî¨ Validaci√≥n Cient√≠fica

### Hip√≥tesis
H0: Combat Agent tiene mismo desempe√±o que Baseline PPO
H1: Combat Agent tiene mejor desempe√±o que Baseline PPO en combates

### Test Estad√≠stico
- **M√©todo**: t-test pareado (100 episodios cada uno)
- **Significancia**: Œ± = 0.05
- **Potencia**: >0.80 (con 100 episodios)

### Criterio de Rechazo
Si p-value < 0.05 en **al menos 3 de las 5 m√©tricas principales** ‚Üí Rechazamos H0

---

**¬°Manos a la obra!** üöÄ

Sigue este plan paso a paso y tendr√°s un proyecto completo, funcional y cient√≠ficamente validado en menos de una semana.

**Dudas o problemas:** Revisar `TECHNICAL_ANALYSIS.md` para entender por qu√© ciertas cosas se hicieron de cierta manera.
