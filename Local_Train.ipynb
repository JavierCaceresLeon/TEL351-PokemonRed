{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81290335",
   "metadata": {},
   "source": [
    "# Entrenamiento Local de Agentes Pokémon\n",
    "Este cuaderno coordina el entrenamiento local de los agentes especializados y del agente híbrido usando las utilidades de `advanced_agents`. Cada sección describe qué configura o ejecuta para que puedas seguir el flujo sin consultar otros archivos. Para más detalles sobre los scripts equivalentes por lotes revisa `README_LOCAL_TRAINING.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635e6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de trabajo: c:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# FIX: Resolver conflicto de OpenMP (Error #15) que causa crash del kernel\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import types\n",
    "import importlib\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Configuración de rutas locales\n",
    "project_path = os.getcwd()\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "baselines_path = os.path.join(project_path, 'baselines')\n",
    "if baselines_path not in sys.path:\n",
    "    sys.path.append(baselines_path)\n",
    "\n",
    "print(f\"Directorio de trabajo: {project_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1438f",
   "metadata": {},
   "source": [
    "## 1. Configuración de entorno\n",
    "Inicializa rutas y variables de entorno necesarias para que PyBoy y Stable-Baselines3 funcionen sin conflictos (por ejemplo, se habilita `KMP_DUPLICATE_LIB_OK` para evitar errores de OpenMP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71d552",
   "metadata": {},
   "source": [
    "## 1.1 Optimización con Numba (Opcional)\n",
    "Para acelerar los cálculos de recompensa (especialmente el cálculo de percentiles en el historial de pérdidas), se recomienda instalar `numba`. Si no está instalado, el código usará una versión estándar de Python más lenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea20176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba instalado: 0.62.1\n",
      "tqdm, rich e ipywidgets disponibles (barra de progreso activada)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numba\n",
    "    print(f\"Numba instalado: {numba.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Numba no detectado. Instalando...\")\n",
    "    !pip install numba\n",
    "    print(\"Instalación completada. Por favor reinicia el kernel si es necesario.\")\n",
    "\n",
    "# Verificar e instalar dependencias para la barra de progreso\n",
    "try:\n",
    "    import tqdm\n",
    "    import rich\n",
    "    import ipywidgets\n",
    "    print(f\"tqdm, rich e ipywidgets disponibles (barra de progreso activada)\")\n",
    "except ImportError:\n",
    "    print(\"Instalando dependencias para habilitar la barra de progreso...\")\n",
    "    !pip install tqdm rich ipywidgets\n",
    "    print(\"Instalación completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ca310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch GPU disponible: True\n",
      "   Dispositivo: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch GPU disponible: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   Dispositivo: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"PyTorch está usando CPU. El entrenamiento será lento.\")\n",
    "except OSError as e:\n",
    "    print(f\"ERROR CRÍTICO DETECTADO: {e}\")\n",
    "    if \"126\" in str(e) or \"caffe2_nvrtc.dll\" in str(e):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"   ¡NO TE PREOCUPES! ESTE ERROR ES ESPERADO SI LA VERSIÓN FALLA\")\n",
    "        print(\"   SOLUCIÓN: Ejecuta la SIGUIENTE CELDA para reparar PyTorch.\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163a7c5",
   "metadata": {},
   "source": [
    "## 1.2 Solución de Problemas de GPU\n",
    "Si la celda anterior indica que **PyTorch está usando CPU**, es probable que tengas instalada una versión incorrecta de PyTorch o que falten los drivers de CUDA.\n",
    "Para arreglarlo en tu **RTX 3050**, ejecuta la siguiente celda para reinstalar una versión estable de PyTorch con soporte CUDA 12.4 (compatible con tus drivers actuales).\n",
    "**Nota:** Después de la instalación, deberás reiniciar el kernel del notebook (Botón \"Restart\" en la barra superior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09339d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Torch: 2.4.1+cu121\n",
      "Versión de CUDA en Torch: 12.1\n",
      "¿CUDA disponible?: True\n",
      "GPU Detectada: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "# VERIFICACIÓN DE INSTALACIÓN\n",
    "# Si esta celda falla con \"WinError 126\" o \"ModuleNotFoundError\", \n",
    "# SIGNIFICA QUE LA INSTALACIÓN ANTERIOR FALLÓ POR BLOQUEO DE ARCHIVOS.\n",
    "#\n",
    "# SOLUCIÓN:\n",
    "# 1. Cierra este notebook o Reinicia el Kernel (Botón Restart ↻ arriba).\n",
    "# 2. Abre una TERMINAL en VS Code (Ctrl+Ñ).\n",
    "# 3. Copia y pega los comandos que te dio el asistente para reinstalar Torch manualmente.\n",
    "# 4. Vuelve aquí y ejecuta esta celda.\n",
    "\n",
    "import torch\n",
    "print(f\"Versión de Torch: {torch.__version__}\")\n",
    "print(f\"Versión de CUDA en Torch: {torch.version.cuda}\")\n",
    "print(f\"¿CUDA disponible?: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Detectada: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠️ GPU no detectada. Si tienes una RTX, reinstala Torch desde la terminal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30026b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RELOAD MODULES ---\n",
    "def reload_modules():\n",
    "    modules_to_reload = [\n",
    "        'v2.red_gym_env_v2',\n",
    "        'advanced_agents.features',\n",
    "        'advanced_agents.wrappers',\n",
    "        'advanced_agents.base',\n",
    "        'advanced_agents.train_agents',\n",
    "        'advanced_agents.combat_apex_agent',\n",
    "        'advanced_agents.puzzle_speed_agent',\n",
    "        'advanced_agents.hybrid_sage_agent',\n",
    "        'advanced_agents.transition_models'\n",
    "    ]\n",
    "    for mod_name in modules_to_reload:\n",
    "        if mod_name in sys.modules:\n",
    "            try:\n",
    "                importlib.reload(sys.modules[mod_name])\n",
    "                print(f\"Recargado: {mod_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo recargar {mod_name}: {e}\")\n",
    "\n",
    "reload_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaed01c",
   "metadata": {},
   "source": [
    "## 2. Recarga de módulos\n",
    "Permite refrescar los módulos clave de `advanced_agents` y del entorno RedGym cada vez que hagas cambios en el código fuente sin tener que reiniciar el kernel. Ejecuta esta celda si modificas archivos Python relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0901eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar events.json si es necesario\n",
    "events_source = os.path.join(project_path, 'baselines', 'events.json')\n",
    "events_dest = os.path.join(project_path, 'events.json')\n",
    "if os.path.exists(events_source) and not os.path.exists(events_dest):\n",
    "    shutil.copy(events_source, events_dest)\n",
    "    print(f\"Copiado events.json a {events_dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf1ae3",
   "metadata": {},
   "source": [
    "## 3. Sincronización de `events.json`\n",
    "Garantiza que el archivo de eventos requerido por PyBoy esté disponible en la raíz del proyecto copiándolo desde `baselines/events.json` cuando falta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0d2b4",
   "metadata": {},
   "source": [
    "## 4. Utilidades de entrenamiento\n",
    "Define el registro de agentes, valida que existan los archivos `.state`, construye las configuraciones de entorno y expone `train_single_run`/`train_plan`, que son los puntos de entrada para disparar los entrenamientos desde las celdas siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f060908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Using SDL2 binaries from pysdl2-dll 2.30.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "import types\n",
    "import importlib\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "import os\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "try:\n",
    "    from advanced_agents.train_agents import _base_env_config\n",
    "    from advanced_agents.combat_apex_agent import CombatApexAgent, CombatAgentConfig\n",
    "    from advanced_agents.puzzle_speed_agent import PuzzleSpeedAgent, PuzzleAgentConfig\n",
    "    from advanced_agents.hybrid_sage_agent import HybridSageAgent, HybridAgentConfig\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ ERROR DE IMPORTACIÓN: {e}\")\n",
    "    raise e\n",
    "except OSError as e:\n",
    "    print(f\"⚠️ ERROR CRÍTICO DE PYTORCH: {e}\")\n",
    "    if \"126\" in str(e) or \"caffe2_nvrtc.dll\" in str(e):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"   ¡TU INSTALACIÓN DE PYTORCH ESTÁ ROTA!\")\n",
    "        print(\"   El kernel tiene archivos bloqueados o la versión es incompatible.\")\n",
    "        print(\"   \")\n",
    "        print(\"   SOLUCIÓN DEFINITIVA:\")\n",
    "        print(\"   1. Abre la terminal (Ctrl+Ñ)\")\n",
    "        print(\"   2. Ejecuta: ./repair_torch.ps1\")\n",
    "        print(\"   3. Reinicia el Kernel (Botón Restart ↻)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    raise e\n",
    "\n",
    "# --- Cargar escenarios ---\n",
    "SCENARIO_PATH = os.path.join(project_path, 'gym_scenarios', 'scenarios.json')\n",
    "with open(SCENARIO_PATH, 'r') as f:\n",
    "    scenarios_data = json.load(f)\n",
    "\n",
    "SCENARIOS: Dict[str, Dict] = {scenario['id']: scenario for scenario in scenarios_data['scenarios']}\n",
    "\n",
    "AGENT_REGISTRY = {\n",
    "    'combat': {\n",
    "        'agent_cls': CombatApexAgent,\n",
    "        'config_cls': CombatAgentConfig,\n",
    "        'default_phase': 'battle'\n",
    "    },\n",
    "    'puzzle': {\n",
    "        'agent_cls': PuzzleSpeedAgent,\n",
    "        'config_cls': PuzzleAgentConfig,\n",
    "        'default_phase': 'puzzle'\n",
    "    },\n",
    "    'hybrid': {\n",
    "        'agent_cls': HybridSageAgent,\n",
    "        'config_cls': HybridAgentConfig,\n",
    "        'default_phase': 'battle'\n",
    "    }\n",
    "}\n",
    "\n",
    "MODELS_DIR = os.path.join(project_path, 'models_local')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "def resolve_phase(scenario_id: str, phase_name: Optional[str]) -> Dict:\n",
    "    scenario = SCENARIOS.get(scenario_id)\n",
    "    if scenario is None:\n",
    "        raise ValueError(f\"Escenario {scenario_id} no encontrado en {SCENARIO_PATH}\")\n",
    "    target_phase = phase_name or AGENT_REGISTRY['combat']['default_phase']\n",
    "    selected_phase = next((p for p in scenario['phases'] if p['name'] == target_phase), None)\n",
    "    if selected_phase is None:\n",
    "        raise ValueError(f\"Fase {target_phase} no encontrada en el escenario {scenario_id}\")\n",
    "    return selected_phase\n",
    "\n",
    "def ensure_state_file(state_file_path: str) -> str:\n",
    "    abs_path = os.path.join(project_path, state_file_path) if not os.path.isabs(state_file_path) else state_file_path\n",
    "    if not os.path.exists(abs_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontró el archivo de estado requerido: {abs_path}. \"\n",
    "            \"Genera los .state con generate_gym_states.py o ajusta la ruta.\"\n",
    "        )\n",
    "    return abs_path\n",
    "\n",
    "def build_env_overrides(state_file_path: str, headless: bool) -> Dict:\n",
    "    return {\n",
    "        'init_state': state_file_path,\n",
    "        'headless': headless,\n",
    "        'save_video': False,\n",
    "        'gb_path': os.path.join(project_path, 'PokemonRed.gb'),\n",
    "        'session_path': os.path.join(project_path, 'sessions', f\"local_{os.path.basename(state_file_path)}\"),\n",
    "        'render_mode': 'rgb_array' if headless else 'human',\n",
    "        'fast_video': headless\n",
    "    }\n",
    "\n",
    "def _patch_callbacks(agent, additional_callbacks: Optional[List] = None):\n",
    "    base_callbacks_method = agent.extra_callbacks\n",
    "\n",
    "    def _patched_callbacks(self):\n",
    "        callbacks = list(base_callbacks_method())\n",
    "        if additional_callbacks:\n",
    "            callbacks.extend(additional_callbacks)\n",
    "        return callbacks\n",
    "\n",
    "    agent.extra_callbacks = types.MethodType(_patched_callbacks, agent)\n",
    "\n",
    "def train_single_run(\n",
    "    agent_key: str,\n",
    "    scenario_id: str,\n",
    "    phase_name: str,\n",
    "    total_timesteps: int = 200_000,\n",
    "    headless: bool = False,\n",
    "    additional_callbacks: Optional[List] = None\n",
    "):\n",
    "    registry_entry = AGENT_REGISTRY.get(agent_key)\n",
    "    if registry_entry is None:\n",
    "        raise ValueError(f\"Agente desconocido: {agent_key}\")\n",
    "\n",
    "    phase = resolve_phase(scenario_id, phase_name)\n",
    "    state_file_path = ensure_state_file(phase['state_file'])\n",
    "\n",
    "    env_overrides = build_env_overrides(state_file_path, headless=headless)\n",
    "    config = registry_entry['config_cls'](\n",
    "        env_config=_base_env_config(env_overrides),\n",
    "        total_timesteps=total_timesteps\n",
    "    )\n",
    "\n",
    "    agent = registry_entry['agent_cls'](config)\n",
    "\n",
    "    env_for_check = agent.make_env()\n",
    "    obs_space = getattr(env_for_check, 'observation_space', None)\n",
    "    if isinstance(obs_space, spaces.Dict):\n",
    "        print(\"Observación Dict detectada -> MultiInputPolicy\")\n",
    "        agent.policy_name = types.MethodType(lambda self: \"MultiInputPolicy\", agent)\n",
    "    env_for_check.close()\n",
    "\n",
    "    if additional_callbacks:\n",
    "        _patch_callbacks(agent, additional_callbacks)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== Entrenando {agent_key.upper()} en {scenario_id} ({phase_name}) por {total_timesteps:,} pasos ===\")\n",
    "    runtime = agent.train()\n",
    "\n",
    "    agent_dir = os.path.join(MODELS_DIR, agent_key)\n",
    "    os.makedirs(agent_dir, exist_ok=True)\n",
    "    model_path = os.path.join(agent_dir, f\"{scenario_id}_{phase_name}.zip\")\n",
    "    runtime.model.save(model_path)\n",
    "    print(f\"Modelo guardado en {model_path}\")\n",
    "\n",
    "    return runtime\n",
    "\n",
    "def train_plan(\n",
    "    agent_key: str,\n",
    "    plan: List[Dict],\n",
    "    default_timesteps: int = 200_000,\n",
    "    headless: bool = False,\n",
    "    callback_factory: Optional[callable] = None\n",
    ") -> Dict[tuple, object]:\n",
    "    results = {}\n",
    "    total_runs = len(plan)\n",
    "    for run_idx, entry in enumerate(plan, start=1):\n",
    "        scenario_id = entry['scenario']\n",
    "        phase_name = entry.get('phase') or AGENT_REGISTRY[agent_key]['default_phase']\n",
    "        run_timesteps = entry.get('timesteps', default_timesteps)\n",
    "        callbacks = None\n",
    "        if callback_factory is not None:\n",
    "            callbacks = callback_factory(entry)\n",
    "        print(f\"\\n>>> [{agent_key.upper()}] Ejecución {run_idx}/{total_runs}\")\n",
    "        runtime = train_single_run(\n",
    "            agent_key=agent_key,\n",
    "            scenario_id=scenario_id,\n",
    "            phase_name=phase_name,\n",
    "            total_timesteps=run_timesteps,\n",
    "            headless=headless,\n",
    "            additional_callbacks=callbacks\n",
    "        )\n",
    "        results[(scenario_id, phase_name)] = runtime\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0dc9a",
   "metadata": {},
   "source": [
    "## 5. Planes de entrenamiento\n",
    "Ajusta aquí qué escenarios, fases y pasos quieres cubrir para cada agente. Usa esto como checklist antes de lanzar ejecuciones largas; puedes sobreescribir timesteps por fila y alternar `headless` para ver la ventana del emulador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64a9ad",
   "metadata": {},
   "source": [
    "### Configura planes de entrenamiento locales\n",
    "Especifica los escenarios, fases y timesteps que quieres para cada agente. Puedes ejecutar cada bloque por separado y combinar headless=True/False según quieras ver la ventana del emulador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18da45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: Para pruebas rápidas con pocos pasos (ej. 200), considera reducir n_steps\n",
    "# en la configuración del agente, ya que PPO hace rollouts completos de n_steps=1024 por defecto.\n",
    "# Para entrenamiento real, usa valores como 40_000+ timesteps.\n",
    "\n",
    "combat_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"battle\", \"timesteps\": 40_000},\n",
    "    # {\"scenario\": \"cerulean_misty\", \"phase\": \"battle\", \"timesteps\": 50_000},\n",
    "]\n",
    "\n",
    "puzzle_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"puzzle\", \"timesteps\": 40_000},\n",
    "    # {\"scenario\": \"cerulean_misty\", \"phase\": \"puzzle\", \"timesteps\": 50_000},\n",
    "]\n",
    "\n",
    "hybrid_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"battle\", \"timesteps\": 50_000},\n",
    "    # {\"scenario\": \"vermillion_lt_surge\", \"phase\": \"battle\", \"timesteps\": 60_000},\n",
    "]\n",
    "\n",
    "DEFAULT_TIMESTEPS_LOCAL = 40_000\n",
    "DEFAULT_HEADLESS_LOCAL = False  # Cambia a True si no necesitas la ventana SDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066e2c",
   "metadata": {},
   "source": [
    "## 6. Ejecutar plan de combate\n",
    "\n",
    "**IMPORTANTE**: Si tu entrenamiento anterior mostró `value_loss > 1000` o `explained_variance < 0.1`, el modelo **no aprendió correctamente**. \n",
    "\n",
    "**Síntomas de entrenamiento fallido:**\n",
    "- value_loss = 3200 (debería estar cerca de 0)\n",
    "- explained_variance = 0.036 (debería ser >0.5)\n",
    "- Reward constante en evaluación\n",
    "- Episodios terminan en timeout sin progreso\n",
    "\n",
    "**Solución**: La siguiente celda usa parámetros **estabilizados** automáticamente. Solo ejecútala para re-entrenar con configuración robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [COMBAT] Ejecución 1/1\n",
      "Observación Dict detectada -> MultiInputPolicy\n",
      "\n",
      "=== Entrenando COMBAT en pewter_brock (battle) por 40,000 pasos ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7926656b249436582dd09dff9853b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 5    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 191  |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 405          |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007673993  |\n",
      "|    clip_fraction        | 0.195        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0006688237 |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.58e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    value_loss           | 3.3e+03      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 4              |\n",
      "|    iterations           | 3              |\n",
      "|    time_elapsed         | 618            |\n",
      "|    total_timesteps      | 3072           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.009874553    |\n",
      "|    clip_fraction        | 0.3            |\n",
      "|    clip_range           | 0.15           |\n",
      "|    entropy_loss         | -1.92          |\n",
      "|    explained_variance   | -1.1920929e-07 |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0317        |\n",
      "|    n_updates            | 20             |\n",
      "|    policy_gradient_loss | -0.00729       |\n",
      "|    value_loss           | 0.00461        |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 831          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066226544 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.0013555288 |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 3.29e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1043        |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014401462 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    value_loss           | 0.000994    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1255         |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073117116 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.0129413605 |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.54e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 3.27e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1467        |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010050582 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    value_loss           | 0.000591    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1679        |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009176165 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.015097022 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.53e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    value_loss           | 3.27e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 1892         |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069254516 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.0          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0262      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00371     |\n",
      "|    value_loss           | 0.000445     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 2104        |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006389723 |\n",
      "|    clip_fraction        | 0.0894      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.016813517 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.52e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 3.26e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2316        |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009003445 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    value_loss           | 0.000292    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4          |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 2528       |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00857245 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.92      |\n",
      "|    explained_variance   | 0.01816523 |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00577   |\n",
      "|    value_loss           | 3.26e+03   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 2740          |\n",
      "|    total_timesteps      | 13312         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0055834865  |\n",
      "|    clip_fraction        | 0.146         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.92         |\n",
      "|    explained_variance   | 5.9604645e-08 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0306       |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.00292      |\n",
      "|    value_loss           | 0.000548      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2953        |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00931727  |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.019625008 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.5e+03     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 3.25e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 3165          |\n",
      "|    total_timesteps      | 15360         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007498509   |\n",
      "|    clip_fraction        | 0.246         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.92         |\n",
      "|    explained_variance   | 5.9604645e-08 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0215       |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00521      |\n",
      "|    value_loss           | 0.000224      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 3377        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006463942 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.020989478 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.49e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    value_loss           | 3.25e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 4              |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 3588           |\n",
      "|    total_timesteps      | 17408          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.009252979    |\n",
      "|    clip_fraction        | 0.342          |\n",
      "|    clip_range           | 0.15           |\n",
      "|    entropy_loss         | -1.92          |\n",
      "|    explained_variance   | -1.1920929e-07 |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0326        |\n",
      "|    n_updates            | 160            |\n",
      "|    policy_gradient_loss | -0.0101        |\n",
      "|    value_loss           | 0.000705       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 3800         |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073879464 |\n",
      "|    clip_fraction        | 0.231        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.022357464  |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 3.24e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 4013          |\n",
      "|    total_timesteps      | 19456         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0074165924  |\n",
      "|    clip_fraction        | 0.191         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.91         |\n",
      "|    explained_variance   | 5.9604645e-08 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0325       |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.00607      |\n",
      "|    value_loss           | 0.00063       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 4225        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009384371 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.023714006 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    value_loss           | 3.24e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 4438        |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006461496 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    value_loss           | 0.000108    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 4651        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008232214 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.025066972 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 3.23e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 4              |\n",
      "|    iterations           | 23             |\n",
      "|    time_elapsed         | 4863           |\n",
      "|    total_timesteps      | 23552          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.004975764    |\n",
      "|    clip_fraction        | 0.15           |\n",
      "|    clip_range           | 0.15           |\n",
      "|    entropy_loss         | -1.91          |\n",
      "|    explained_variance   | -1.1920929e-07 |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0329        |\n",
      "|    n_updates            | 220            |\n",
      "|    policy_gradient_loss | -0.00483       |\n",
      "|    value_loss           | 0.000482       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 5076         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065936847 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.026413202  |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00726     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000469    |\n",
      "|    value_loss           | 3.23e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 5288        |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009861588 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 5501         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069283834 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.027794242  |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0315      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00271     |\n",
      "|    value_loss           | 3.22e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 4              |\n",
      "|    iterations           | 27             |\n",
      "|    time_elapsed         | 5713           |\n",
      "|    total_timesteps      | 27648          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.006899758    |\n",
      "|    clip_fraction        | 0.189          |\n",
      "|    clip_range           | 0.15           |\n",
      "|    entropy_loss         | -1.93          |\n",
      "|    explained_variance   | -2.3841858e-07 |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0325        |\n",
      "|    n_updates            | 260            |\n",
      "|    policy_gradient_loss | -0.00461       |\n",
      "|    value_loss           | 0.000769       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 5925        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006442425 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.029028237 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 3.22e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 6137          |\n",
      "|    total_timesteps      | 29696         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009296153   |\n",
      "|    clip_fraction        | 0.193         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.92         |\n",
      "|    explained_variance   | 5.9604645e-08 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0261       |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00734      |\n",
      "|    value_loss           | 0.00479       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 6349         |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055149514 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.030292451  |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0262      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 3.22e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 6562        |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017431054 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 0.000128    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 6774        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005536956 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.03149408  |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.42e+03    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 3.21e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 6987        |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009741454 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.0         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 0.000685    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 7200        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009235102 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.032627106 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.41e+03    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    value_loss           | 3.21e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 7413          |\n",
      "|    total_timesteps      | 35840         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00914288    |\n",
      "|    clip_fraction        | 0.195         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.86         |\n",
      "|    explained_variance   | 1.7881393e-07 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0209       |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.00565      |\n",
      "|    value_loss           | 0.000643      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 7625        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00937172  |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.034033835 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    value_loss           | 3.2e+03     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 4              |\n",
      "|    iterations           | 37             |\n",
      "|    time_elapsed         | 7836           |\n",
      "|    total_timesteps      | 37888          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0071684606   |\n",
      "|    clip_fraction        | 0.14           |\n",
      "|    clip_range           | 0.15           |\n",
      "|    entropy_loss         | -1.84          |\n",
      "|    explained_variance   | -1.1920929e-07 |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0319        |\n",
      "|    n_updates            | 360            |\n",
      "|    policy_gradient_loss | -0.00321       |\n",
      "|    value_loss           | 0.000568       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 8047        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013177071 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.035197556 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 3.2e+03     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 8258          |\n",
      "|    total_timesteps      | 39936         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009246286   |\n",
      "|    clip_fraction        | 0.212         |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.82         |\n",
      "|    explained_variance   | 1.1920929e-07 |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0392       |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00546      |\n",
      "|    value_loss           | 0.00176       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 8455        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011068063 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.036417663 |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0234     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    value_loss           | 3.2e+03     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en c:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\\models_local\\combat\\pewter_brock_battle.zip\n"
     ]
    }
   ],
   "source": [
    "# ==================== ENTRENAMIENTO CON PARÁMETROS ESTABLES ====================\n",
    "# Si tu entrenamiento anterior falló (value_loss alto), esta versión usa parámetros\n",
    "# más conservadores que garantizan convergencia.\n",
    "# =================================================================================\n",
    "\n",
    "from advanced_agents.combat_apex_agent import CombatApexAgent, CombatAgentConfig\n",
    "\n",
    "def train_combat_stable(scenario_id='pewter_brock', phase_name='battle', timesteps=40_000):\n",
    "    \"\"\"Entrena CombatApexAgent con parámetros estabilizados.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"   ENTRENAMIENTO ESTABLE - COMBAT APEX AGENT\")\n",
    "    print(f\"   Escenario: {scenario_id} | Fase: {phase_name}\")\n",
    "    print(f\"   Pasos: {timesteps:,}\")\n",
    "    print(f\"   Parámetros: LR reducido, clipping conservador, gradientes limitados\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Configurar entorno\n",
    "    phase = resolve_phase(scenario_id, phase_name)\n",
    "    state_file_path = ensure_state_file(phase['state_file'])\n",
    "    env_overrides = build_env_overrides(state_file_path, headless=True)\n",
    "    base_config = _base_env_config(env_overrides)\n",
    "    \n",
    "    # Configuración ESTABLE (parámetros ajustados para evitar divergencia)\n",
    "    agent_config = CombatAgentConfig(\n",
    "        env_config=base_config,\n",
    "        total_timesteps=timesteps,\n",
    "        learning_rate=1e-4,      # Más conservador que 2.5e-4\n",
    "        n_steps=512,             # Actualizaciones más frecuentes\n",
    "        batch_size=128,          # Batches más pequeños\n",
    "        gamma=0.998,             # Menos influencia del futuro\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.1,          # Clipping más estricto\n",
    "        vf_coef=0.25,            # Menos peso a la función de valor\n",
    "        ent_coef=0.01,           # Entropía para exploración\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Crear agente\n",
    "    agent = CombatApexAgent(agent_config)\n",
    "    \n",
    "    # Verificar espacio de observaciones\n",
    "    env_check = agent.make_env()\n",
    "    from gymnasium import spaces\n",
    "    if isinstance(env_check.observation_space, spaces.Dict):\n",
    "        print(\"Observación Dict detectada -> MultiInputPolicy\")\n",
    "        agent.policy_name = lambda: \"MultiInputPolicy\"\n",
    "    env_check.close()\n",
    "    \n",
    "    # Entrenar\n",
    "    print(f\"\\n🚀 Iniciando entrenamiento estable...\")\n",
    "    runtime = agent.train()\n",
    "    \n",
    "    # Guardar\n",
    "    save_dir = os.path.join(MODELS_DIR, 'combat')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{scenario_id}_{phase_name}_stable.zip\")\n",
    "    runtime.model.save(save_path)\n",
    "    \n",
    "    print(f\"\\nModelo ESTABLE guardado en: {save_path}\")\n",
    "    print(f\"Revisa los logs - deberías ver:\")\n",
    "    print(f\"   - value_loss < 100 (idealmente < 10)\")\n",
    "    print(f\"   - explained_variance > 0.3 (mejorando hacia 0.7+)\")\n",
    "    print(f\"   - approx_kl < 0.05\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# EJECUTAR ENTRENAMIENTO ESTABLE\n",
    "combat_model_stable = train_combat_stable(\n",
    "    scenario_id='pewter_brock',\n",
    "    phase_name='battle', \n",
    "    timesteps=40_000\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ENTRENAMIENTO COMPLETO\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Modelo guardado en: {combat_model_stable}\")\n",
    "print(f\"\\n🎮 Para probarlo:\")\n",
    "print(f\"python run_combat_agent_interactive.py --scenario pewter_brock --phase battle\")\n",
    "print(f\"(Renombra el archivo _stable.zip a .zip si es necesario)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f6bb1",
   "metadata": {},
   "source": [
    "## 7. Ejecutar plan de puzzles\n",
    "Corre el plan `puzzle_plan_local` usando `PuzzleSpeedAgent` y guarda salidas en `models_local/puzzle/`. Útil para medir tiempos de navegación y resolución de puzzles previos al combate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c309e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [PUZZLE] Ejecución 1/1\n",
      "Observación Dict detectada -> MultiInputPolicy\n",
      "\n",
      "=== Entrenando PUZZLE en pewter_brock (puzzle) por 200 pasos ===\n",
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RedGymEnv' object has no attribute 'seen_coords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m puzzle_runs_local \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpuzzle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpuzzle_plan_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_TIMESTEPS_LOCAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_HEADLESS_LOCAL\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 149\u001b[0m, in \u001b[0;36mtrain_plan\u001b[1;34m(agent_key, plan, default_timesteps, headless, callback_factory)\u001b[0m\n\u001b[0;32m    147\u001b[0m         callbacks \u001b[38;5;241m=\u001b[39m callback_factory(entry)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>> [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_key\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Ejecución \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheadless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     results[(scenario_id, phase_name)] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[1;32mIn[7], line 122\u001b[0m, in \u001b[0;36mtrain_single_run\u001b[1;34m(agent_key, scenario_id, phase_name, total_timesteps, headless, additional_callbacks)\u001b[0m\n\u001b[0;32m    118\u001b[0m     _patch_callbacks(agent, additional_callbacks)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Entrenando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_key\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscenario_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) por \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pasos ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m runtime \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m agent_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODELS_DIR, agent_key)\n\u001b[0;32m    125\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(agent_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\\advanced_agents\\base.py:122\u001b[0m, in \u001b[0;36mAdvancedAgent.train\u001b[1;34m(self, run_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     use_progress_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_progress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Only save the env if it has a save method (e.g. VecNormalize)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:113\u001b[0m, in \u001b[0;36mVecTransposeImage.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict]:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Reset all environments\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observations, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_observations(observations)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds[env_idx], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmaybe_options)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\\advanced_agents\\wrappers.py:213\u001b[0m, in \u001b[0;36mPuzzleRewardWrapper.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\\advanced_agents\\wrappers.py:176\u001b[0m, in \u001b[0;36m_PuzzleShaper.reset\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    174\u001b[0m base_env \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39munwrapped\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_badges \u001b[38;5;241m=\u001b[39m base_env\u001b[38;5;241m.\u001b[39mget_badges()\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_coord_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen_coords\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RedGymEnv' object has no attribute 'seen_coords'"
     ]
    }
   ],
   "source": [
    "puzzle_runs_local = train_plan(\n",
    "    agent_key='puzzle',\n",
    "    plan=puzzle_plan_local,\n",
    "    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,\n",
    "    headless=DEFAULT_HEADLESS_LOCAL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b8c0e",
   "metadata": {},
   "source": [
    "## 8. Ejecutar plan híbrido\n",
    "Activa `HybridSageAgent` sobre los escenarios definidos en `hybrid_plan_local`, mezclando comportamientos de combate y navegación y almacenando resultados en `models_local/hybrid/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c901491",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_runs_local = train_plan(\n",
    "    agent_key='hybrid',\n",
    "    plan=hybrid_plan_local,\n",
    "    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,\n",
    "    headless=DEFAULT_HEADLESS_LOCAL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198425e",
   "metadata": {},
   "source": [
    "## 9. Guardado manual (opcional)\n",
    "Fragmento de ejemplo para guardar un modelo entrenado con un nombre personalizado. Solo úsalo si traes a la sesión variables como `model`, `AGENT_TYPE`, `SCENARIO_ID` y `PHASE_NAME`; de lo contrario producirá errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e84f117",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AGENT_TYPE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_local\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mAGENT_TYPE\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCENARIO_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPHASE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AGENT_TYPE' is not defined"
     ]
    }
   ],
   "source": [
    "# Guardar modelo\n",
    "save_dir = \"models_local\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"{AGENT_TYPE}_{SCENARIO_ID}_{PHASE_NAME}\")\n",
    "model.save(save_path)\n",
    "print(f\"Modelo guardado en {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e81b",
   "metadata": {},
   "source": [
    "## 10. Comparación con Baseline (PPO v2)\n",
    "\n",
    "Esta sección permite comparar el desempeño de tus agentes entrenados (Combat, Puzzle, Hybrid) contra un baseline.\n",
    "\n",
    "**IMPORTANTE - Limitaciones de RAM (16GB):**\n",
    "- El modelo `poke_26214400.zip` (26M pasos) requiere >10GB solo para cargarlo\n",
    "- **Alternativa recomendada**: Entrenar tu propio baseline ligero (40k-100k pasos) en lugar de usar el modelo pesado\n",
    "- O simplemente evaluar solo tus modelos locales sin comparación (ver celda siguiente)\n",
    "\n",
    "**Alternativa para comparar sin .zip pesado:**\n",
    "Puedes usar `run_pretrained_interactive.py` como baseline ejecutándolo manualmente y registrando las métricas, pero esta sección automatiza la evaluación de **tus modelos** sin necesidad del baseline gigante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from v2.red_gym_env_v2 import RedGymEnv\n",
    "\n",
    "def load_baseline_model(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"No se encontró el modelo baseline en: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return PPO.load(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando baseline: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_agent_model(model, env, num_episodes=1):\n",
    "    \"\"\"Ejecuta episodios de evaluación y retorna métricas promedio.\"\"\"\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        # Manejar diferentes formatos de reset()\n",
    "        reset_result = env.reset()\n",
    "        obs = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 5000  # Límite de pasos por episodio\n",
    "        \n",
    "        while not done and not truncated and step_count < max_steps:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            step_result = env.step(action)\n",
    "            \n",
    "            # Manejar diferentes formatos de step()\n",
    "            if len(step_result) == 5:\n",
    "                obs, reward, done, truncated, info = step_result\n",
    "            elif len(step_result) == 4:\n",
    "                obs, reward, done, info = step_result\n",
    "                truncated = False\n",
    "            else:\n",
    "                raise ValueError(f\"Formato inesperado de step(): {len(step_result)} valores\")\n",
    "            \n",
    "            # Convertir reward a escalar\n",
    "            reward_scalar = float(reward.item() if hasattr(reward, 'item') else reward)\n",
    "            total_reward += reward_scalar\n",
    "            step_count += 1\n",
    "            \n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_count)\n",
    "        \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(steps)\n",
    "    }\n",
    "\n",
    "def run_comparison_lightweight(plans_dict, baseline_path=None, headless=True, skip_baseline=False):\n",
    "    \"\"\"\n",
    "    Versión optimizada para RAM limitada (<=16GB con Windows ocupando 10GB).\n",
    "    skip_baseline=True: Solo evalúa tus modelos locales (recomendado para 16GB RAM)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Cargar Baseline solo si se solicita y existe\n",
    "    baseline_model = None\n",
    "    if not skip_baseline and baseline_path:\n",
    "        print(f\"Intentando cargar modelo baseline desde: {baseline_path}\")\n",
    "        baseline_model = load_baseline_model(baseline_path)\n",
    "        if not baseline_model:\n",
    "            print(\"No se pudo cargar el baseline. Solo se evaluarán modelos locales.\")\n",
    "    else:\n",
    "        print(\"Modo sin baseline activado (ahorra ~10GB RAM)\")\n",
    "    \n",
    "    for agent_key, plan in plans_dict.items():\n",
    "        for entry in plan:\n",
    "            scenario_id = entry['scenario']\n",
    "            phase_name = entry.get('phase') or AGENT_REGISTRY[agent_key]['default_phase']\n",
    "            \n",
    "            print(f\"\\n--- Evaluando {agent_key.upper()} en {scenario_id} ({phase_name}) ---\")\n",
    "            \n",
    "            # 1. Preparar Configuración Común\n",
    "            phase = resolve_phase(scenario_id, phase_name)\n",
    "            state_file_path = ensure_state_file(phase['state_file'])\n",
    "            env_overrides = build_env_overrides(state_file_path, headless=headless)\n",
    "            base_config = _base_env_config(env_overrides)\n",
    "            \n",
    "            # ---------------------------------------------------------\n",
    "            # 2. Evaluar Agente Local (con su propio wrapper/env)\n",
    "            # ---------------------------------------------------------\n",
    "            registry_entry = AGENT_REGISTRY[agent_key]\n",
    "            agent_config = registry_entry['config_cls'](\n",
    "                env_config=base_config,\n",
    "                total_timesteps=1000 \n",
    "            )\n",
    "            local_agent_wrapper = registry_entry['agent_cls'](agent_config)\n",
    "            \n",
    "            local_model_path = os.path.join(MODELS_DIR, agent_key, f\"{scenario_id}_{phase_name}.zip\")\n",
    "            \n",
    "            if os.path.exists(local_model_path):\n",
    "                print(f\"Cargando modelo local: {local_model_path}\")\n",
    "                try:\n",
    "                    env_local = local_agent_wrapper.make_env()\n",
    "                    local_agent_wrapper.model = PPO.load(local_model_path)\n",
    "                    \n",
    "                    print(f\"Ejecutando evaluación...\")\n",
    "                    metrics_local = evaluate_agent_model(local_agent_wrapper.model, env_local)\n",
    "                    env_local.close()\n",
    "                    \n",
    "                    print(f\"Local: Reward={metrics_local['mean_reward']:.2f}, Steps={metrics_local['mean_steps']:.0f}\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Agent': agent_key.upper(),\n",
    "                        'Scenario': scenario_id,\n",
    "                        'Phase': phase_name,\n",
    "                        'Model': 'Local (Specialized)',\n",
    "                        'Reward': metrics_local['mean_reward'],\n",
    "                        'Steps': metrics_local['mean_steps']\n",
    "                    })\n",
    "                    \n",
    "                    # Liberar memoria\n",
    "                    del local_agent_wrapper.model\n",
    "                    del env_local\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluando local: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(f\"No existe modelo local en {local_model_path}\")\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # 3. Evaluar Baseline solo si está disponible\n",
    "            # ---------------------------------------------------------\n",
    "            if baseline_model:\n",
    "                print(\"Evaluando Baseline...\")\n",
    "                try:\n",
    "                    env_baseline = RedGymEnv(base_config)\n",
    "                    metrics_baseline = evaluate_agent_model(baseline_model, env_baseline)\n",
    "                    env_baseline.close()\n",
    "                    \n",
    "                    print(f\"Baseline: Reward={metrics_baseline['mean_reward']:.2f}, Steps={metrics_baseline['mean_steps']:.0f}\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Agent': agent_key.upper(),\n",
    "                        'Scenario': scenario_id,\n",
    "                        'Phase': phase_name,\n",
    "                        'Model': 'Baseline (PPO v2)',\n",
    "                        'Reward': metrics_baseline['mean_reward'],\n",
    "                        'Steps': metrics_baseline['mean_steps']\n",
    "                    })\n",
    "                    \n",
    "                    del env_baseline\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluando baseline: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results) if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36c7f218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo sin baseline activado (ahorra ~10GB RAM)\n",
      "\n",
      "--- Evaluando COMBAT en pewter_brock (battle) ---\n",
      "Cargando modelo local: c:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\\models_local\\combat\\pewter_brock_battle.zip\n",
      "Error evaluando local: too many values to unpack (expected 2)\n",
      "\n",
      "No se generaron resultados. Verifica que existan modelos entrenados.\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURACIÓN DE COMPARACIÓN ====================\n",
    "# Para sistemas con 16GB RAM (con Windows usando ~10GB):\n",
    "# skip_baseline=True: Solo evalúa tus modelos (ahorra ~10GB)\n",
    "# skip_baseline=False: Intenta cargar el baseline (requiere >20GB RAM total)\n",
    "# ========================================================================\n",
    "\n",
    "BASELINE_MODEL_PATH = os.path.join(project_path, 'v2', 'runs', 'poke_26214400.zip')\n",
    "\n",
    "comparison_plans = {\n",
    "    'combat': combat_plan_local,\n",
    "    # 'puzzle': puzzle_plan_local,   # Comenta para evaluar menos modelos\n",
    "    # 'hybrid': hybrid_plan_local,   # Comenta para evaluar menos modelos\n",
    "}\n",
    "\n",
    "# IMPORTANTE: skip_baseline=True para ahorrar RAM\n",
    "df_results = run_comparison_lightweight(\n",
    "    comparison_plans, \n",
    "    baseline_path=BASELINE_MODEL_PATH, \n",
    "    headless=True,\n",
    "    skip_baseline=True  # Cambia a False solo si tienes >24GB RAM\n",
    ")\n",
    "\n",
    "if df_results is not None and not df_results.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           RESULTADOS DE EVALUACIÓN\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Guardar CSV\n",
    "    csv_path = \"evaluacion_modelos_locales.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nResultados guardados en: {csv_path}\")\n",
    "else:\n",
    "    print(\"\\nNo se generaron resultados. Verifica que existan modelos entrenados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43e56f",
   "metadata": {},
   "source": [
    "## 11. Entrenar Baseline Ligero (Opcional - Alternativa al modelo pesado)\n",
    "\n",
    "Si quieres comparar tus agentes especializados con un baseline PPO genérico **sin usar el modelo gigante de 26M pasos**, puedes entrenar tu propio baseline ligero aquí. Este será un modelo estándar de `v2/red_gym_env_v2.py` entrenado con los **mismos 40k pasos** que tus agentes especializados para una comparación justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from v2.red_gym_env_v2 import RedGymEnv\n",
    "\n",
    "def train_lightweight_baseline(scenario_id='pewter_brock', phase_name='battle', timesteps=40_000):\n",
    "    \"\"\"\n",
    "    Entrena un baseline PPO simple (sin wrappers especializados) para comparación justa.\n",
    "    Usa el mismo número de pasos que tus agentes especializados.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"   ENTRENANDO BASELINE LIGERO (PPO Genérico)\")\n",
    "    print(f\"   Escenario: {scenario_id} | Fase: {phase_name}\")\n",
    "    print(f\"   Pasos: {timesteps:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Preparar configuración del entorno (igual que tus agentes)\n",
    "    phase = resolve_phase(scenario_id, phase_name)\n",
    "    state_file_path = ensure_state_file(phase['state_file'])\n",
    "    env_overrides = build_env_overrides(state_file_path, headless=True)\n",
    "    base_config = _base_env_config(env_overrides)\n",
    "    \n",
    "    # Crear entorno estándar (sin wrappers especializados)\n",
    "    env = RedGymEnv(base_config)\n",
    "    \n",
    "    # Crear modelo PPO con configuración similar a tus agentes\n",
    "    model = PPO(\n",
    "        \"CnnPolicy\",  # Política estándar para imágenes\n",
    "        env,\n",
    "        learning_rate=2.5e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=256,\n",
    "        gamma=0.999,\n",
    "        verbose=1,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Entrenar\n",
    "    try:\n",
    "        import tqdm, rich\n",
    "        model.learn(total_timesteps=timesteps, progress_bar=True)\n",
    "    except ImportError:\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "    # Guardar\n",
    "    baseline_dir = os.path.join(MODELS_DIR, 'baseline_lightweight')\n",
    "    os.makedirs(baseline_dir, exist_ok=True)\n",
    "    baseline_path = os.path.join(baseline_dir, f\"{scenario_id}_{phase_name}.zip\")\n",
    "    model.save(baseline_path)\n",
    "    \n",
    "    print(f\"\\nBaseline ligero guardado en: {baseline_path}\")\n",
    "    env.close()\n",
    "    \n",
    "    return baseline_path\n",
    "\n",
    "# Entrenar baseline (descomenta para ejecutar)\n",
    "# baseline_ligero_path = train_lightweight_baseline(\n",
    "#     scenario_id='pewter_brock',\n",
    "#     phase_name='battle',\n",
    "#     timesteps=40_000  # Mismo número de pasos que tus agentes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2107a20",
   "metadata": {},
   "source": [
    "### Comparar con Baseline Ligero\n",
    "\n",
    "Una vez entrenado el baseline ligero, puedes compararlo con tus agentes especializados usando esta celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b83ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al baseline ligero que acabas de entrenar\n",
    "BASELINE_LIGERO_PATH = os.path.join(project_path, 'models_local', 'baseline_lightweight', 'pewter_brock_battle.zip')\n",
    "\n",
    "# Comparar (solo si el baseline ligero existe)\n",
    "if os.path.exists(BASELINE_LIGERO_PATH):\n",
    "    print(\"Comparando con Baseline Ligero (entrenado con los mismos 40k pasos)\")\n",
    "    \n",
    "    df_comparison = run_comparison_lightweight(\n",
    "        {'combat': combat_plan_local},\n",
    "        baseline_path=BASELINE_LIGERO_PATH,\n",
    "        headless=True,\n",
    "        skip_baseline=False  # Ahora SÍ cargamos el baseline (es ligero)\n",
    "    )\n",
    "    \n",
    "    if df_comparison is not None and not df_comparison.empty:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"     COMPARACIÓN: AGENTE ESPECIALIZADO vs BASELINE LIGERO\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_comparison.to_string(index=False))\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calcular mejora\n",
    "        if len(df_comparison) == 2:\n",
    "            reward_especializado = df_comparison[df_comparison['Model'].str.contains('Specialized')]['Reward'].values[0]\n",
    "            reward_baseline = df_comparison[df_comparison['Model'].str.contains('Baseline')]['Reward'].values[0]\n",
    "            mejora = ((reward_especializado - reward_baseline) / abs(reward_baseline)) * 100\n",
    "            print(f\"\\nMejora del agente especializado: {mejora:+.1f}%\")\n",
    "        \n",
    "        df_comparison.to_csv(\"comparacion_especializado_vs_baseline_ligero.csv\", index=False)\n",
    "else:\n",
    "    print(f\"Baseline ligero no encontrado en: {BASELINE_LIGERO_PATH}\")\n",
    "    print(\"Ejecuta primero la celda anterior para entrenar el baseline ligero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando en: Pewter City Gym - Brock\n",
      "Modelo: pewter_brock_battle.zip\n",
      "Modo Headless: False\n",
      "Cargando pesos del modelo...\n",
      "Inyectando configuración de equipo e inventario...\n",
      "🌀 Programando Warp a Mapa 54 (4, 13)...\n",
      "Calentando motor para warp (3s)...\n",
      "Inyectando configuración de equipo e inventario...\n",
      "🌀 Programando Warp a Mapa 54 (4, 13)...\n",
      "Calentando motor para warp (3s)...\n",
      "📊 Métricas iniciadas para CombatApex_Local en Gimnasio 1\n",
      "\n",
      "Iniciando ejecución del agente...\n",
      "📊 Métricas iniciadas para CombatApex_Local en Gimnasio 1\n",
      "\n",
      "Iniciando ejecución del agente...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 245\u001b[0m\n\u001b[0;32m    242\u001b[0m SCENARIO_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgym_scenarios\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgym1_pewter_brock\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Ejecutar (Cambiado a headless=False para ver qué pasa)\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_gym_scenario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCENARIO_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 172\u001b[0m, in \u001b[0;36mevaluate_gym_scenario\u001b[1;34m(model_path, scenario_path, headless)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIniciando ejecución del agente...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# Predecir\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Ejecutar\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     step_result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:365\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[1;32m--> 365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:276\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_tensor, vectorized_env\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\utils.py:487\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(obs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized type of observation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\javi1\\anaconda3\\envs\\pokeenv\\lib\\site-packages\\stable_baselines3\\common\\utils.py:487\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(obs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized type of observation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# EVALUACIÓN DE AGENTE EN ESCENARIO DE GIMNASIO (HEADLESS + MÉTRICAS)\n",
    "# =================================================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from gym_scenarios.gym_metrics import GymMetricsTracker\n",
    "\n",
    "# Importar direcciones de memoria necesarias para la inyección\n",
    "# (Copiadas de memory_addresses.py para asegurar disponibilidad)\n",
    "PARTY_SIZE_ADDRESS = 0xD163\n",
    "PARTY_ADDRESSES = [0xD164, 0xD165, 0xD166, 0xD167, 0xD168, 0xD169]\n",
    "LEVELS_ADDRESSES = [0xD18C, 0xD1B8, 0xD1E4, 0xD210, 0xD23C, 0xD268]\n",
    "HP_ADDRESSES = [0xD16C, 0xD198, 0xD1C4, 0xD1F0, 0xD21C, 0xD248]\n",
    "MAX_HP_ADDRESSES = [0xD18D, 0xD1B9, 0xD1E5, 0xD211, 0xD23D, 0xD269]\n",
    "MONEY_ADDRESS_1 = 0xD347\n",
    "MONEY_ADDRESS_2 = 0xD348\n",
    "MONEY_ADDRESS_3 = 0xD349\n",
    "BADGE_COUNT_ADDRESS = 0xD356\n",
    "BAG_ITEMS_START = 0xD31E\n",
    "BAG_ITEM_COUNT = 0xD31D\n",
    "\n",
    "def get_base_env(env):\n",
    "    \"\"\"Obtiene el entorno base (RedGymEnv) de un wrapper o VecEnv.\"\"\"\n",
    "    if hasattr(env, 'envs'): # DummyVecEnv\n",
    "        env = env.envs[0]\n",
    "    if hasattr(env, 'unwrapped'):\n",
    "        return env.unwrapped\n",
    "    return env\n",
    "\n",
    "def inject_gym_config(env, config):\n",
    "    \"\"\"Inyecta la configuración del equipo e inventario en la memoria del emulador.\"\"\"\n",
    "    base_env = get_base_env(env)\n",
    "    pyboy = base_env.pyboy\n",
    "    \n",
    "    def write_mem(addr, val):\n",
    "        if hasattr(pyboy, \"set_memory_value\"):\n",
    "            pyboy.set_memory_value(addr, val & 0xFF)\n",
    "        else:\n",
    "            pyboy.memory[addr] = val & 0xFF\n",
    "\n",
    "    def write_word(addr, val):\n",
    "        write_mem(addr, (val >> 8) & 0xFF)\n",
    "        write_mem(addr + 1, val & 0xFF)\n",
    "\n",
    "    def write_bcd(val):\n",
    "        return ((val // 10) << 4) | (val % 10)\n",
    "\n",
    "    print(\"Inyectando configuración de equipo e inventario...\")\n",
    "\n",
    "    # 1. Equipo\n",
    "    team = config.get('player_team', [])\n",
    "    write_mem(PARTY_SIZE_ADDRESS, len(team))\n",
    "    for i, poke in enumerate(team):\n",
    "        slot = poke.get('slot', 1) - 1\n",
    "        if 0 <= slot < 6:\n",
    "            write_mem(PARTY_ADDRESSES[slot], poke.get('species_id', 0))\n",
    "            write_mem(LEVELS_ADDRESSES[slot], poke.get('level', 5))\n",
    "            write_word(HP_ADDRESSES[slot], poke.get('current_hp', 20))\n",
    "            write_word(MAX_HP_ADDRESSES[slot], poke.get('max_hp', 20))\n",
    "\n",
    "    # 2. Items\n",
    "    items = config.get('bag_items', [])\n",
    "    item_count = min(len(items), 20)\n",
    "    write_mem(BAG_ITEM_COUNT, item_count)\n",
    "    for i, item in enumerate(items[:20]):\n",
    "        base = BAG_ITEMS_START + (i * 2)\n",
    "        write_mem(base, item.get('item_id', 0))\n",
    "        write_mem(base + 1, item.get('quantity', 1))\n",
    "    write_mem(BAG_ITEMS_START + (item_count * 2), 0xFF)\n",
    "\n",
    "    # 3. Dinero y Medallas\n",
    "    money = config.get('money', 0)\n",
    "    write_mem(MONEY_ADDRESS_1, write_bcd(money // 10000))\n",
    "    write_mem(MONEY_ADDRESS_2, write_bcd((money // 100) % 100))\n",
    "    write_mem(MONEY_ADDRESS_3, write_bcd(money % 100))\n",
    "    write_mem(BADGE_COUNT_ADDRESS, config.get('badge_bits', 0))\n",
    "\n",
    "    # 4. Warp Seguro\n",
    "    start_pos = config.get('start_position', {'x': 4, 'y': 13})\n",
    "    map_id = config.get('map_id', 0)\n",
    "    \n",
    "    print(f\"🌀 Programando Warp a Mapa {map_id} ({start_pos['x']}, {start_pos['y']})...\")\n",
    "    write_mem(0xD365, map_id)          # wWarpDestMap\n",
    "    write_mem(0xD366, start_pos['x'])  # wWarpDestX\n",
    "    write_mem(0xD367, start_pos['y'])  # wWarpDestY\n",
    "    \n",
    "    if hasattr(pyboy, \"get_memory_value\"):\n",
    "        current_wd72d = pyboy.get_memory_value(0xD12B)\n",
    "    else:\n",
    "        current_wd72d = pyboy.memory[0xD12B]\n",
    "    write_mem(0xD12B, current_wd72d | 0x08) # Trigger Warp\n",
    "    write_mem(0xD35D, 0x00) # Reset script state\n",
    "\n",
    "    return map_id, start_pos\n",
    "\n",
    "def evaluate_gym_scenario(model_path, scenario_path, headless=True):\n",
    "    \"\"\"Ejecuta la evaluación completa de un escenario de gimnasio.\"\"\"\n",
    "    \n",
    "    # Rutas\n",
    "    state_file = os.path.join(scenario_path, \"gym_scenario.state\")\n",
    "    config_file = os.path.join(scenario_path, \"team_config.json\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Modelo no encontrado: {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(state_file):\n",
    "        print(f\"Estado no encontrado: {state_file}\")\n",
    "        return\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuración no encontrada: {config_file}\")\n",
    "        return\n",
    "\n",
    "    # Cargar Configuración\n",
    "    with open(config_file, 'r') as f:\n",
    "        team_config = json.load(f)\n",
    "        \n",
    "    print(f\"\\nEvaluando en: {team_config.get('gym_name', 'Unknown Gym')}\")\n",
    "    print(f\"Modelo: {os.path.basename(model_path)}\")\n",
    "    print(f\"Modo Headless: {headless}\")\n",
    "\n",
    "    # Configurar Entorno\n",
    "    env_overrides = build_env_overrides(state_file, headless=headless)\n",
    "    env_overrides['max_steps'] = 2048 * 5 \n",
    "    base_config = _base_env_config(env_overrides)\n",
    "    \n",
    "    # Instanciar Agente\n",
    "    agent_config = CombatAgentConfig(env_config=base_config, total_timesteps=1000)\n",
    "    agent_wrapper = CombatApexAgent(agent_config)\n",
    "    \n",
    "    # Cargar Modelo\n",
    "    print(\"Cargando pesos del modelo...\")\n",
    "    agent_wrapper.model = PPO.load(model_path)\n",
    "    \n",
    "    # Crear Entorno\n",
    "    env = agent_wrapper.make_env()\n",
    "    \n",
    "    # --- FIX: Manejo de VecEnv vs Env estándar ---\n",
    "    try:\n",
    "        obs, _ = env.reset()\n",
    "    except ValueError:\n",
    "        # Si falla, es probable que sea un VecEnv que solo retorna obs\n",
    "        obs = env.reset()\n",
    "    \n",
    "    # Inyectar Configuración\n",
    "    target_map, target_pos = inject_gym_config(env, team_config)\n",
    "    base_env = get_base_env(env) # Referencia directa para lecturas de memoria\n",
    "    \n",
    "    # Calentamiento para Warp\n",
    "    print(\"Calentando motor para warp (3s)...\")\n",
    "    for _ in range(180):\n",
    "        base_env.pyboy.tick(1, False)\n",
    "        if not headless:\n",
    "            env.render() # Mantener ventana viva\n",
    "            \n",
    "    # Verificar si el Warp funcionó\n",
    "    current_map = base_env.read_m(0xD35E)\n",
    "    if current_map != target_map:\n",
    "        print(f\"⚠️ ADVERTENCIA: El Warp falló. Mapa actual: {current_map}, Esperado: {target_map}\")\n",
    "        print(\"   Reintentando inyección...\")\n",
    "        inject_gym_config(env, team_config)\n",
    "        for _ in range(60):\n",
    "            base_env.pyboy.tick(1, False)\n",
    "            if not headless: env.render()\n",
    "        \n",
    "    # Inicializar Tracker\n",
    "    tracker = GymMetricsTracker(\n",
    "        gym_number=team_config.get('gym_number', 1),\n",
    "        agent_name=\"CombatApex_Local\",\n",
    "        gym_name=team_config.get('gym_name', \"\")\n",
    "    )\n",
    "    tracker.start()\n",
    "    \n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    print(\"\\nIniciando ejecución del agente...\")\n",
    "    while not done and not truncated:\n",
    "        \n",
    "        # --- FIX: Asegurar que obs no sea tupla antes de predict ---\n",
    "        if isinstance(obs, tuple):\n",
    "            # print(f\"DEBUG: Obs es tupla {type(obs)}, corrigiendo...\")\n",
    "            obs = obs[0]\n",
    "            \n",
    "        # Predecir\n",
    "        action, _ = agent_wrapper.model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Ejecutar\n",
    "        step_result = env.step(action)\n",
    "        \n",
    "        # --- FIX: Unpacking flexible para VecEnv ---\n",
    "        if len(step_result) == 4:\n",
    "            obs, reward, done, info = step_result\n",
    "            truncated = False\n",
    "            # Si es VecEnv, reward/done son arrays\n",
    "            if isinstance(done, (list, np.ndarray)): done = done[0]\n",
    "            if isinstance(reward, (list, np.ndarray)): reward = reward[0]\n",
    "            if isinstance(info, (list, np.ndarray)): info = info[0]\n",
    "        else:\n",
    "            obs, reward, done, truncated, info = step_result\n",
    "        \n",
    "        # Renderizar\n",
    "        if not headless:\n",
    "            env.render()\n",
    "            \n",
    "        # Registrar métricas (Usando base_env para leer memoria)\n",
    "        # FIX: Asegurar tipos nativos de Python para evitar errores de JSON serialization\n",
    "        game_state = {\n",
    "            'x': int(base_env.read_m(0xD362)),\n",
    "            'y': int(base_env.read_m(0xD361)),\n",
    "            'map': int(base_env.read_m(0xD35E)),\n",
    "            'hp': [int(base_env.read_m(HP_ADDRESSES[i])) for i in range(6)],\n",
    "            'in_battle': bool(base_env.read_m(0xD057) != 0)\n",
    "        }\n",
    "        \n",
    "        # FIX: Convertir acción y reward a escalares nativos\n",
    "        action_scalar = action.item() if isinstance(action, np.ndarray) else action\n",
    "        reward_scalar = float(reward)\n",
    "        \n",
    "        tracker.record_step(action_scalar, reward_scalar, game_state)\n",
    "        \n",
    "        # Lógica de batalla\n",
    "        if game_state['in_battle'] and not tracker.battle_started:\n",
    "            tracker.record_battle_start()\n",
    "        elif not game_state['in_battle'] and tracker.battle_started:\n",
    "            tracker.record_battle_end(won=True) \n",
    "            \n",
    "        # Seguridad\n",
    "        if game_state['map'] != target_map and not game_state['in_battle']:\n",
    "             pass\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    # Finalizar\n",
    "    tracker.end(success=tracker.battle_won)\n",
    "    tracker.save_metrics(output_dir=\"metrics_evaluation\")\n",
    "    \n",
    "    stats = tracker.get_summary_stats()\n",
    "    print(\"\\nResumen de Evaluación:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "    \n",
    "    # --- DIAGNÓSTICO AUTOMÁTICO ---\n",
    "    if not stats['battle_won']:\n",
    "        print(\"\\n⚠️ DIAGNÓSTICO DE FALLO:\")\n",
    "        print(\"   'FALLO' significa que el agente no ganó la batalla del gimnasio.\")\n",
    "        if stats['unique_tiles_explored'] <= 1:\n",
    "            print(\"   🔴 EL AGENTE ESTÁ INMÓVIL: Solo exploró 1 baldosa.\")\n",
    "            print(\"   Sugerencia: Revisa si el modelo aprendió correctamente o si el Warp lo dejó atrapado.\")\n",
    "        elif stats['battle_steps'] == 0:\n",
    "            print(\"   🟠 NO ENTRÓ A BATALLA: El agente se movió pero no inició el combate.\")\n",
    "            \n",
    "    return stats\n",
    "\n",
    "# --- EJECUTAR EVALUACIÓN ---\n",
    "MODEL_PATH = os.path.join(project_path, 'models_local', 'combat', 'pewter_brock_battle.zip')\n",
    "SCENARIO_PATH = os.path.join(project_path, 'gym_scenarios', 'gym1_pewter_brock')\n",
    "\n",
    "# Ejecutar (Cambiado a headless=False para ver qué pasa)\n",
    "stats = evaluate_gym_scenario(MODEL_PATH, SCENARIO_PATH, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c658e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
