{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81290335",
   "metadata": {},
   "source": [
    "# Entrenamiento Local de Agentes Pokémon\n",
    "Este cuaderno coordina el entrenamiento local de los agentes especializados y del agente híbrido usando las utilidades de `advanced_agents`. Cada sección describe qué configura o ejecuta para que puedas seguir el flujo sin consultar otros archivos. Para más detalles sobre los scripts equivalentes por lotes revisa `README_LOCAL_TRAINING.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635e6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de trabajo: c:\\Users\\javi1\\Documents\\repos_git\\TEL351-PokemonRed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# FIX: Resolver conflicto de OpenMP (Error #15) que causa crash del kernel\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import types\n",
    "import importlib\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Configuración de rutas locales\n",
    "project_path = os.getcwd()\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "baselines_path = os.path.join(project_path, 'baselines')\n",
    "if baselines_path not in sys.path:\n",
    "    sys.path.append(baselines_path)\n",
    "\n",
    "print(f\"Directorio de trabajo: {project_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1438f",
   "metadata": {},
   "source": [
    "## 1. Configuración de entorno\n",
    "Inicializa rutas y variables de entorno necesarias para que PyBoy y Stable-Baselines3 funcionen sin conflictos (por ejemplo, se habilita `KMP_DUPLICATE_LIB_OK` para evitar errores de OpenMP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30026b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♻️ Recargado: v2.red_gym_env_v2\n",
      "♻️ Recargado: advanced_agents.features\n",
      "♻️ Recargado: advanced_agents.wrappers\n",
      "♻️ Recargado: advanced_agents.base\n",
      "♻️ Recargado: advanced_agents.train_agents\n",
      "♻️ Recargado: advanced_agents.combat_apex_agent\n",
      "♻️ Recargado: advanced_agents.puzzle_speed_agent\n",
      "♻️ Recargado: advanced_agents.hybrid_sage_agent\n",
      "♻️ Recargado: advanced_agents.transition_models\n"
     ]
    }
   ],
   "source": [
    "# --- RELOAD MODULES ---\n",
    "def reload_modules():\n",
    "    modules_to_reload = [\n",
    "        'v2.red_gym_env_v2',\n",
    "        'advanced_agents.features',\n",
    "        'advanced_agents.wrappers',\n",
    "        'advanced_agents.base',\n",
    "        'advanced_agents.train_agents',\n",
    "        'advanced_agents.combat_apex_agent',\n",
    "        'advanced_agents.puzzle_speed_agent',\n",
    "        'advanced_agents.hybrid_sage_agent',\n",
    "        'advanced_agents.transition_models'\n",
    "    ]\n",
    "    for mod_name in modules_to_reload:\n",
    "        if mod_name in sys.modules:\n",
    "            try:\n",
    "                importlib.reload(sys.modules[mod_name])\n",
    "                print(f\"♻️ Recargado: {mod_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ No se pudo recargar {mod_name}: {e}\")\n",
    "\n",
    "reload_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaed01c",
   "metadata": {},
   "source": [
    "## 2. Recarga de módulos\n",
    "Permite refrescar los módulos clave de `advanced_agents` y del entorno RedGym cada vez que hagas cambios en el código fuente sin tener que reiniciar el kernel. Ejecuta esta celda si modificas archivos Python relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0901eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar events.json si es necesario\n",
    "events_source = os.path.join(project_path, 'baselines', 'events.json')\n",
    "events_dest = os.path.join(project_path, 'events.json')\n",
    "if os.path.exists(events_source) and not os.path.exists(events_dest):\n",
    "    shutil.copy(events_source, events_dest)\n",
    "    print(f\"Copiado events.json a {events_dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf1ae3",
   "metadata": {},
   "source": [
    "## 3. Sincronización de `events.json`\n",
    "Garantiza que el archivo de eventos requerido por PyBoy esté disponible en la raíz del proyecto copiándolo desde `baselines/events.json` cuando falta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0d2b4",
   "metadata": {},
   "source": [
    "## 4. Utilidades de entrenamiento\n",
    "Define el registro de agentes, valida que existan los archivos `.state`, construye las configuraciones de entorno y expone `train_single_run`/`train_plan`, que son los puntos de entrada para disparar los entrenamientos desde las celdas siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f060908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import types\n",
    "import importlib\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "try:\n",
    "    from advanced_agents.train_agents import _base_env_config\n",
    "    from advanced_agents.combat_apex_agent import CombatApexAgent, CombatAgentConfig\n",
    "    from advanced_agents.puzzle_speed_agent import PuzzleSpeedAgent, PuzzleAgentConfig\n",
    "    from advanced_agents.hybrid_sage_agent import HybridSageAgent, HybridAgentConfig\n",
    "except ImportError as e:\n",
    "    print(\"ERROR CRÍTICO: Fallo en imports.\")\n",
    "    raise e\n",
    "\n",
    "# --- Cargar escenarios ---\n",
    "SCENARIO_PATH = os.path.join(project_path, 'gym_scenarios', 'scenarios.json')\n",
    "with open(SCENARIO_PATH, 'r') as f:\n",
    "    scenarios_data = json.load(f)\n",
    "\n",
    "SCENARIOS: Dict[str, Dict] = {scenario['id']: scenario for scenario in scenarios_data['scenarios']}\n",
    "\n",
    "AGENT_REGISTRY = {\n",
    "    'combat': {\n",
    "        'agent_cls': CombatApexAgent,\n",
    "        'config_cls': CombatAgentConfig,\n",
    "        'default_phase': 'battle'\n",
    "    },\n",
    "    'puzzle': {\n",
    "        'agent_cls': PuzzleSpeedAgent,\n",
    "        'config_cls': PuzzleAgentConfig,\n",
    "        'default_phase': 'puzzle'\n",
    "    },\n",
    "    'hybrid': {\n",
    "        'agent_cls': HybridSageAgent,\n",
    "        'config_cls': HybridAgentConfig,\n",
    "        'default_phase': 'battle'\n",
    "    }\n",
    "}\n",
    "\n",
    "MODELS_DIR = os.path.join(project_path, 'models_local')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "def resolve_phase(scenario_id: str, phase_name: Optional[str]) -> Dict:\n",
    "    scenario = SCENARIOS.get(scenario_id)\n",
    "    if scenario is None:\n",
    "        raise ValueError(f\"Escenario {scenario_id} no encontrado en {SCENARIO_PATH}\")\n",
    "    target_phase = phase_name or AGENT_REGISTRY['combat']['default_phase']\n",
    "    selected_phase = next((p for p in scenario['phases'] if p['name'] == target_phase), None)\n",
    "    if selected_phase is None:\n",
    "        raise ValueError(f\"Fase {target_phase} no encontrada en el escenario {scenario_id}\")\n",
    "    return selected_phase\n",
    "\n",
    "def ensure_state_file(state_file_path: str) -> str:\n",
    "    abs_path = os.path.join(project_path, state_file_path) if not os.path.isabs(state_file_path) else state_file_path\n",
    "    if not os.path.exists(abs_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontró el archivo de estado requerido: {abs_path}. \"\n",
    "            \"Genera los .state con generate_gym_states.py o ajusta la ruta.\"\n",
    "        )\n",
    "    return abs_path\n",
    "\n",
    "def build_env_overrides(state_file_path: str, headless: bool) -> Dict:\n",
    "    return {\n",
    "        'init_state': state_file_path,\n",
    "        'headless': headless,\n",
    "        'save_video': False,\n",
    "        'gb_path': os.path.join(project_path, 'PokemonRed.gb'),\n",
    "        'session_path': os.path.join(project_path, 'sessions', f\"local_{os.path.basename(state_file_path)}\"),\n",
    "        'render_mode': 'rgb_array' if headless else 'human',\n",
    "        'fast_video': headless\n",
    "    }\n",
    "\n",
    "def _patch_callbacks(agent, additional_callbacks: Optional[List] = None):\n",
    "    base_callbacks_method = agent.extra_callbacks\n",
    "\n",
    "    def _patched_callbacks(self):\n",
    "        callbacks = list(base_callbacks_method())\n",
    "        if additional_callbacks:\n",
    "            callbacks.extend(additional_callbacks)\n",
    "        return callbacks\n",
    "\n",
    "    agent.extra_callbacks = types.MethodType(_patched_callbacks, agent)\n",
    "\n",
    "def train_single_run(\n",
    "    agent_key: str,\n",
    "    scenario_id: str,\n",
    "    phase_name: str,\n",
    "    total_timesteps: int = 200_000,\n",
    "    headless: bool = False,\n",
    "    additional_callbacks: Optional[List] = None\n",
    "):\n",
    "    registry_entry = AGENT_REGISTRY.get(agent_key)\n",
    "    if registry_entry is None:\n",
    "        raise ValueError(f\"Agente desconocido: {agent_key}\")\n",
    "\n",
    "    phase = resolve_phase(scenario_id, phase_name)\n",
    "    state_file_path = ensure_state_file(phase['state_file'])\n",
    "\n",
    "    env_overrides = build_env_overrides(state_file_path, headless=headless)\n",
    "    config = registry_entry['config_cls'](\n",
    "        env_config=_base_env_config(env_overrides),\n",
    "        total_timesteps=total_timesteps\n",
    "    )\n",
    "\n",
    "    agent = registry_entry['agent_cls'](config)\n",
    "\n",
    "    env_for_check = agent.make_env()\n",
    "    obs_space = getattr(env_for_check, 'observation_space', None)\n",
    "    if isinstance(obs_space, spaces.Dict):\n",
    "        print(\"Observación Dict detectada -> MultiInputPolicy\")\n",
    "        agent.policy_name = types.MethodType(lambda self: \"MultiInputPolicy\", agent)\n",
    "    env_for_check.close()\n",
    "\n",
    "    if additional_callbacks:\n",
    "        _patch_callbacks(agent, additional_callbacks)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== Entrenando {agent_key.upper()} en {scenario_id} ({phase_name}) por {total_timesteps:,} pasos ===\")\n",
    "    runtime = agent.train()\n",
    "\n",
    "    agent_dir = os.path.join(MODELS_DIR, agent_key)\n",
    "    os.makedirs(agent_dir, exist_ok=True)\n",
    "    model_path = os.path.join(agent_dir, f\"{scenario_id}_{phase_name}.zip\")\n",
    "    runtime.model.save(model_path)\n",
    "    print(f\"Modelo guardado en {model_path}\")\n",
    "\n",
    "    return runtime\n",
    "\n",
    "def train_plan(\n",
    "    agent_key: str,\n",
    "    plan: List[Dict],\n",
    "    default_timesteps: int = 200_000,\n",
    "    headless: bool = False,\n",
    "    callback_factory: Optional[callable] = None\n",
    ") -> Dict[tuple, object]:\n",
    "    results = {}\n",
    "    total_runs = len(plan)\n",
    "    for run_idx, entry in enumerate(plan, start=1):\n",
    "        scenario_id = entry['scenario']\n",
    "        phase_name = entry.get('phase') or AGENT_REGISTRY[agent_key]['default_phase']\n",
    "        run_timesteps = entry.get('timesteps', default_timesteps)\n",
    "        callbacks = None\n",
    "        if callback_factory is not None:\n",
    "            callbacks = callback_factory(entry)\n",
    "        print(f\"\\n>>> [{agent_key.upper()}] Ejecución {run_idx}/{total_runs}\")\n",
    "        runtime = train_single_run(\n",
    "            agent_key=agent_key,\n",
    "            scenario_id=scenario_id,\n",
    "            phase_name=phase_name,\n",
    "            total_timesteps=run_timesteps,\n",
    "            headless=headless,\n",
    "            additional_callbacks=callbacks\n",
    "        )\n",
    "        results[(scenario_id, phase_name)] = runtime\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0dc9a",
   "metadata": {},
   "source": [
    "## 5. Planes de entrenamiento\n",
    "Ajusta aquí qué escenarios, fases y pasos quieres cubrir para cada agente. Usa esto como checklist antes de lanzar ejecuciones largas; puedes sobreescribir timesteps por fila y alternar `headless` para ver la ventana del emulador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64a9ad",
   "metadata": {},
   "source": [
    "### Configura planes de entrenamiento locales\n",
    "Especifica los escenarios, fases y timesteps que quieres para cada agente. Puedes ejecutar cada bloque por separado y combinar headless=True/False según quieras ver la ventana del emulador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18da45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combat_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"battle\", \"timesteps\": 200_000},\n",
    "    # {\"scenario\": \"cerulean_misty\", \"phase\": \"battle\", \"timesteps\": 220_000},\n",
    "]\n",
    "\n",
    "puzzle_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"puzzle\", \"timesteps\": 180_000},\n",
    "    # {\"scenario\": \"cerulean_misty\", \"phase\": \"puzzle\", \"timesteps\": 200_000},\n",
    "]\n",
    "\n",
    "hybrid_plan_local = [\n",
    "    {\"scenario\": \"pewter_brock\", \"phase\": \"battle\", \"timesteps\": 220_000},\n",
    "    # {\"scenario\": \"vermillion_lt_surge\", \"phase\": \"battle\", \"timesteps\": 250_000},\n",
    "]\n",
    "\n",
    "DEFAULT_TIMESTEPS_LOCAL = 200_000\n",
    "DEFAULT_HEADLESS_LOCAL = False  # Cambia a True si no necesitas la ventana SDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066e2c",
   "metadata": {},
   "source": [
    "## 6. Ejecutar plan de combate\n",
    "Lanza las corridas definidas en `combat_plan_local`. Cada iteración verifica el `.state`, arma el entorno y guarda el modelo en `models_local/combat/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [COMBAT] Ejecución 1/1\n",
      "Observación Dict detectada -> MultiInputPolicy\n",
      "\n",
      "=== Entrenando COMBAT en pewter_brock (battle) por 200,000 pasos ===\n"
     ]
    }
   ],
   "source": [
    "combat_runs_local = train_plan(\n",
    "    agent_key='combat',\n",
    "    plan=combat_plan_local,\n",
    "    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,\n",
    "    headless=DEFAULT_HEADLESS_LOCAL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f6bb1",
   "metadata": {},
   "source": [
    "## 7. Ejecutar plan de puzzles\n",
    "Corre el plan `puzzle_plan_local` usando `PuzzleSpeedAgent` y guarda salidas en `models_local/puzzle/`. Útil para medir tiempos de navegación y resolución de puzzles previos al combate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c309e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_runs_local = train_plan(\n",
    "    agent_key='puzzle',\n",
    "    plan=puzzle_plan_local,\n",
    "    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,\n",
    "    headless=DEFAULT_HEADLESS_LOCAL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b8c0e",
   "metadata": {},
   "source": [
    "## 8. Ejecutar plan híbrido\n",
    "Activa `HybridSageAgent` sobre los escenarios definidos en `hybrid_plan_local`, mezclando comportamientos de combate y navegación y almacenando resultados en `models_local/hybrid/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c901491",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_runs_local = train_plan(\n",
    "    agent_key='hybrid',\n",
    "    plan=hybrid_plan_local,\n",
    "    default_timesteps=DEFAULT_TIMESTEPS_LOCAL,\n",
    "    headless=DEFAULT_HEADLESS_LOCAL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198425e",
   "metadata": {},
   "source": [
    "## 9. Guardado manual (opcional)\n",
    "Fragmento de ejemplo para guardar un modelo entrenado con un nombre personalizado. Solo úsalo si traes a la sesión variables como `model`, `AGENT_TYPE`, `SCENARIO_ID` y `PHASE_NAME`; de lo contrario producirá errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo\n",
    "save_dir = \"models_local\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"{AGENT_TYPE}_{SCENARIO_ID}_{PHASE_NAME}\")\n",
    "model.save(save_path)\n",
    "print(f\"Modelo guardado en {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e81b",
   "metadata": {},
   "source": [
    "## 10. Comparación con Baseline (PPO v2)\n",
    "Esta sección permite comparar el desempeño de tus agentes entrenados (Combat, Puzzle, Hybrid) contra el modelo PPO genérico de v2 (`poke_26214400.zip`). Se ejecutarán episodios de evaluación en los mismos escenarios para contrastar recompensas y pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from v2.red_gym_env_v2 import RedGymEnv\n",
    "\n",
    "def load_baseline_model(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"No se encontró el modelo baseline en: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return PPO.load(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando baseline: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_agent_model(model, env, num_episodes=3):\n",
    "    \"\"\"Ejecuta episodios de evaluación y retorna métricas promedio.\"\"\"\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            # Usar predict del modelo SB3\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_count)\n",
    "        \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(steps)\n",
    "    }\n",
    "\n",
    "def run_comparison(plans_dict, baseline_path, headless=True):\n",
    "    results = []\n",
    "    \n",
    "    # Cargar Baseline una vez\n",
    "    baseline_model = load_baseline_model(baseline_path)\n",
    "    if not baseline_model:\n",
    "        print(\"No se puede proceder sin el modelo baseline.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Modelo Baseline cargado desde: {baseline_path}\")\n",
    "    \n",
    "    for agent_key, plan in plans_dict.items():\n",
    "        for entry in plan:\n",
    "            scenario_id = entry['scenario']\n",
    "            phase_name = entry.get('phase') or AGENT_REGISTRY[agent_key]['default_phase']\n",
    "            \n",
    "            print(f\"\\n--- Comparando {agent_key.upper()} vs BASELINE en {scenario_id} ({phase_name}) ---\")\n",
    "            \n",
    "            # 1. Preparar Configuración Común\n",
    "            phase = resolve_phase(scenario_id, phase_name)\n",
    "            state_file_path = ensure_state_file(phase['state_file'])\n",
    "            env_overrides = build_env_overrides(state_file_path, headless=headless)\n",
    "            base_config = _base_env_config(env_overrides)\n",
    "            \n",
    "            # ---------------------------------------------------------\n",
    "            # 2. Evaluar Agente Local (con su propio wrapper/env)\n",
    "            # ---------------------------------------------------------\n",
    "            registry_entry = AGENT_REGISTRY[agent_key]\n",
    "            # Config dummy para instanciar el agente y crear su env\n",
    "            agent_config = registry_entry['config_cls'](\n",
    "                env_config=base_config,\n",
    "                total_timesteps=1000 \n",
    "            )\n",
    "            local_agent_wrapper = registry_entry['agent_cls'](agent_config)\n",
    "            \n",
    "            # Intentar cargar el modelo entrenado localmente\n",
    "            local_model_path = os.path.join(MODELS_DIR, agent_key, f\"{scenario_id}_{phase_name}.zip\")\n",
    "            \n",
    "            if os.path.exists(local_model_path):\n",
    "                print(f\"Cargando modelo local: {local_model_path}\")\n",
    "                try:\n",
    "                    # Crear entorno específico del agente (con wrappers)\n",
    "                    env_local = local_agent_wrapper.make_env()\n",
    "                    \n",
    "                    # Cargar pesos en el modelo del agente\n",
    "                    local_agent_wrapper.model = PPO.load(local_model_path)\n",
    "                    \n",
    "                    metrics_local = evaluate_agent_model(local_agent_wrapper.model, env_local)\n",
    "                    env_local.close()\n",
    "                    print(f\"Local: R={metrics_local['mean_reward']:.2f}, Steps={metrics_local['mean_steps']:.1f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluando local: {e}\")\n",
    "                    metrics_local = None\n",
    "            else:\n",
    "                print(f\"No existe modelo local en {local_model_path}. Saltando.\")\n",
    "                metrics_local = None\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # 3. Evaluar Baseline (con entorno estándar RedGymEnv)\n",
    "            # ---------------------------------------------------------\n",
    "            print(\"Evaluando Baseline...\")\n",
    "            try:\n",
    "                # Crear entorno estándar para el baseline\n",
    "                env_baseline = RedGymEnv(base_config)\n",
    "                \n",
    "                metrics_baseline = evaluate_agent_model(baseline_model, env_baseline)\n",
    "                env_baseline.close()\n",
    "                print(f\"Baseline: R={metrics_baseline['mean_reward']:.2f}, Steps={metrics_baseline['mean_steps']:.1f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluando baseline: {e}\")\n",
    "                metrics_baseline = {'mean_reward': 0, 'mean_steps': 0}\n",
    "            \n",
    "            # ---------------------------------------------------------\n",
    "            # 4. Registrar Resultados\n",
    "            # ---------------------------------------------------------\n",
    "            if metrics_local:\n",
    "                results.append({\n",
    "                    'Agent': agent_key.upper(),\n",
    "                    'Scenario': scenario_id,\n",
    "                    'Phase': phase_name,\n",
    "                    'Model': 'Local (Specialized)',\n",
    "                    'Reward': metrics_local['mean_reward'],\n",
    "                    'Steps': metrics_local['mean_steps']\n",
    "                })\n",
    "                \n",
    "            results.append({\n",
    "                'Agent': agent_key.upper(), # Agrupador\n",
    "                'Scenario': scenario_id,\n",
    "                'Phase': phase_name,\n",
    "                'Model': 'Baseline (PPO v2)',\n",
    "                'Reward': metrics_baseline['mean_reward'],\n",
    "                'Steps': metrics_baseline['mean_steps']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al modelo baseline (ajusta si es necesario)\n",
    "BASELINE_MODEL_PATH = os.path.join(project_path, 'v2', 'runs', 'poke_26214400.zip')\n",
    "\n",
    "# Definir qué comparar (puedes reusar los planes definidos arriba)\n",
    "comparison_plans = {\n",
    "    'combat': combat_plan_local,\n",
    "    'puzzle': puzzle_plan_local,\n",
    "    'hybrid': hybrid_plan_local\n",
    "}\n",
    "\n",
    "# Ejecutar comparación (headless=True para ir rápido)\n",
    "df_results = run_comparison(comparison_plans, BASELINE_MODEL_PATH, headless=True)\n",
    "\n",
    "if df_results is not None and not df_results.empty:\n",
    "    print(\"\\n=== RESULTADOS DE COMPARACIÓN ===\")\n",
    "    print(df_results)\n",
    "    \n",
    "    # Opcional: Guardar CSV\n",
    "    df_results.to_csv(\"comparacion_local_vs_baseline.csv\", index=False)\n",
    "else:\n",
    "    print(\"No se generaron resultados (¿faltan modelos?).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
