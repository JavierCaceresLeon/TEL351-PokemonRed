# Entrenamiento de Agentes de Aprendizaje por Refuerzo para Pok√©mon Red

Este proyecto implementa un entorno de aprendizaje por refuerzo para entrenar agentes de IA que jueguen Pok√©mon Red autom√°ticamente. El agente aprende a navegar por el mundo del juego, capturar Pok√©mon, luchar en batallas y completar objetivos usando t√©cnicas de aprendizaje profundo.

## Descripci√≥n General del Proyecto

El proyecto utiliza PyBoy (un emulador de Game Boy) junto con Stable Baselines3 para crear un entorno de gimnasio donde los agentes pueden interactuar con Pok√©mon Red. El agente observa las pantallas del juego y aprende pol√≠ticas √≥ptimas mediante algoritmos como PPO (Proximal Policy Optimization).

## An√°lisis T√©cnico Detallado para Agentes Inteligentes

### Paradigma de Aprendizaje por Refuerzo vs B√∫squeda Tradicional

Este proyecto representa una evoluci√≥n desde algoritmos de b√∫squeda tradicionales (como Tab√∫ Search, Greedy, A*) hacia t√©cnicas de aprendizaje por refuerzo profundo. A diferencia de los m√©todos de b√∫squeda que requieren conocimiento expl√≠cito del espacio de estados y funciones heur√≠sticas, el agente aprende impl√≠citamente las estrategias √≥ptimas a trav√©s de la interacci√≥n con el entorno.

#### Comparaci√≥n con M√©todos de B√∫squeda Tradicionales:

**B√∫squeda Tradicional (Tab√∫, Greedy, etc.):**
- Requiere modelado expl√≠cito del espacio de estados
- Funci√≥n heur√≠stica definida manualmente
- Exploraci√≥n determin√≠stica o semi-determin√≠stica
- Conocimiento del dominio incorporado por el programador
- Escalabilidad limitada en espacios de estados grandes

**Aprendizaje por Refuerzo (PPO):**
- Aprendizaje autom√°tico de pol√≠ticas √≥ptimas
- Exploraci√≥n estoc√°stica con balance autom√°tico exploraci√≥n/explotaci√≥n
- Adaptaci√≥n a entornos complejos y parcialmente observables
- Generalizaci√≥n a estados no vistos durante entrenamiento
- Escalabilidad mejorada mediante aproximaci√≥n de funciones

### Arquitectura del Agente Inteligente

#### Algoritmo Principal: PPO (Proximal Policy Optimization)

PPO es un algoritmo de Policy Gradient que resuelve el problema de estabilidad en el entrenamiento de pol√≠ticas. Sus caracter√≠sticas principales:

**Funci√≥n Objetivo PPO:**
```
L^CLIP(Œ∏) = E[min(rt(Œ∏)At, clip(rt(Œ∏), 1-Œµ, 1+Œµ)At)]
```
Donde:
- rt(Œ∏) = œÄ_Œ∏(at|st) / œÄ_Œ∏_old(at|st) (ratio de probabilidades)
- At = ventaja estimada en el tiempo t
- Œµ = par√°metro de clipping (t√≠picamente 0.2)

**Ventajas de PPO en este dominio:**
1. **Estabilidad**: Evita cambios dr√°sticos en la pol√≠tica
2. **Eficiencia de muestra**: Reutiliza datos de episodios anteriores
3. **Paralelizaci√≥n**: Entrenamiento en m√∫ltiples entornos simult√°neos (64 procesos)
4. **Robustez**: Manejo efectivo de espacios de acci√≥n discretos

#### Espacio de Estados y Observaciones

El agente recibe observaciones multi-modales que incluyen:

```python
observation_space = spaces.Dict({
    "screens": Box(0, 255, (72, 80, frame_stacks), uint8),     # Frames del juego
    "health": Box(0, 1),                                        # Salud normalizada  
    "level": Box(-1, 1, (enc_freqs,)),                        # Niveles codificados
    "badges": MultiBinary(8),                                   # Vector de medallas
    "events": MultiBinary((event_flags_end - event_flags_start) * 8), # Flags de eventos
    "map": Box(0, 255, (coords_pad*4, coords_pad*4, 1)),      # Mapa de exploraci√≥n
    "recent_actions": MultiDiscrete([n_actions] * frame_stacks) # Historial de acciones
})
```

**T√©cnicas de Representaci√≥n Avanzadas:**

1. **Codificaci√≥n Fourier para Niveles:**
```python
def fourier_encode(self, value):
    # Mapea valores continuos a representaci√≥n sinusoidal
    # Permite mejor generalizaci√≥n en espacios continuos
    return np.array([np.sin(2 * np.pi * value * freq) for freq in self.freqs])
```

2. **Downsampling de Im√°genes:**
- Reducci√≥n de 144x160 a 72x80 pixels
- Preserva informaci√≥n espacial cr√≠tica
- Reduce dimensionalidad computacional

3. **Stack de Frames:**
- Mantiene historial temporal de 3 frames
- Permite detecci√≥n de movimiento y din√°micas temporales

#### Espacio de Acciones

```python
valid_actions = [
    PRESS_ARROW_DOWN, PRESS_ARROW_LEFT, PRESS_ARROW_RIGHT, PRESS_ARROW_UP,
    PRESS_BUTTON_A, PRESS_BUTTON_B, PRESS_BUTTON_START
]
```

**Estrategia de Ejecuci√≥n de Acciones:**
- Presi√≥n de bot√≥n por 8 ticks
- Liberaci√≥n autom√°tica
- Continuaci√≥n por remaining ticks (24 total por acci√≥n)
- Previene acciones "pegadas" que causaban problemas en V1

### Sistema de Recompensas: Ingenier√≠a de Recompensas Sofisticada

El sistema de recompensas en V2 representa una evoluci√≥n significativa desde sistemas heur√≠sticos simples:

#### Componentes de Recompensa Detallados:

```python
state_scores = {
    "event": reward_scale * update_max_event_rew() * 4,        # Eventos del juego
    "heal": reward_scale * total_healing_rew * 10,             # Curaci√≥n
    "badge": reward_scale * get_badges() * 10,                 # Medallas de gimnasio  
    "explore": reward_scale * explore_weight * len(seen_coords) * 0.1, # Exploraci√≥n
    "stuck": reward_scale * get_current_coord_count_reward() * -0.05   # Anti-bucle
}
```

**1. Recompensa de Exploraci√≥n (Clave de V2):**
- **V1**: Comparaci√≥n de frames con k-NN (computacionalmente costoso)
- **V2**: Basada en coordenadas √∫nicas visitadas
- **Ventaja**: Evita redundancia visual, enfoca en diversidad espacial
- **Implementaci√≥n**: `len(seen_coords)` - conteo de ubicaciones (x,y,map) √∫nicas

**2. Recompensa de Eventos:**
- Monitoreo de 319 flags de eventos del juego (0xD747 a 0xD886)
- Cada bit representa un evento espec√≠fico (obtener Pok√©dex, hablar con NPCs, etc.)
- Recompensa acumulativa que incentiva progreso narrativo

**3. Sistema Anti-Bloqueo:**
- **Problema en V1**: Agente se quedaba en men√∫s o bucles
- **Soluci√≥n V2**: Recompensa negativa por estar demasiado tiempo en misma coordenada
- **Implementaci√≥n**: Penalizaci√≥n progresiva basada en frecuencia de visita

**4. Recompensa de Curaci√≥n:**
- Incentiva mantener Pok√©mon con vida
- Bonus cuadr√°tico: `heal_amount * heal_amount`
- Penalizaci√≥n por muerte: `died_count` incrementa

**5. Medallas como Objetivos de Alto Nivel:**
- Recompensa m√°xima (10x) por obtener medallas de gimnasio
- Representa objetivos a largo plazo en el juego

#### Funci√≥n de Recompensa Total:

```python
def update_reward(self):
    current_reward = sum([val for _, val in self.progress_reward.items()])
    step_reward = current_reward - self.total_reward
    self.total_reward = current_reward
    return step_reward
```

**Caracter√≠sticas Avanzadas:**
- **Recompensa Delta**: Solo cambios incrementales, no valores absolutos
- **Normalizaci√≥n**: Todas las recompensas escaladas por `reward_scale`
- **Balance**: Pesos ajustados para evitar dominancia de una componente

### Estrategias de Entrenamiento Avanzadas

#### Paralelizaci√≥n Masiva

```python
num_cpu = 64  # 64 entornos paralelos
env = SubprocVecEnv([make_env(i, env_config) for i in range(num_cpu)])
```

**Ventajas:**
- **Diversidad de Experiencia**: 64 trayectorias simult√°neas
- **Estabilidad de Gradientes**: Promediado sobre m√∫ltiples entornos
- **Eficiencia**: Paralelizaci√≥n a nivel de CPU
- **Exploraci√≥n**: Diferentes semillas iniciales por proceso

#### Configuraci√≥n PPO Optimizada

```python
model = PPO("MultiInputPolicy", env,
    n_steps=2048,        # Pasos por actualizaci√≥n de pol√≠tica
    batch_size=512,      # Tama√±o de lote para optimizaci√≥n
    n_epochs=1,          # √âpocas por actualizaci√≥n (previene overfitting)
    gamma=0.997,         # Factor de descuento (horizonte largo)
    ent_coef=0.01,       # Coeficiente de entrop√≠a (exploraci√≥n)
    learning_rate=3e-4   # Tasa de aprendizaje
)
```

**Justificaciones T√©cnicas:**
- **n_steps=2048**: Balance entre varianza y bias en estimaci√≥n de gradientes
- **n_epochs=1**: Previene overfitting en datos de episodios
- **gamma=0.997**: Horizonte temporal largo (recompensas a 300+ pasos)
- **ent_coef=0.01**: Mantiene exploraci√≥n sin sacrificar convergencia

#### T√©cnicas de Regularizaci√≥n

1. **Clipping de Pol√≠tica**: Limita cambios dr√°sticos en œÄ(a|s)
2. **Normalizaci√≥n de Ventajas**: Estabiliza entrenamiento
3. **Gradient Clipping**: Previene explosi√≥n de gradientes
4. **Value Function Clipping**: Estabiliza cr√≠tico de valor

### Manejo del Problema de Men√∫s (V1 vs V2)

#### Problema Original (V1):
- **S√≠ntoma**: Agente se quedaba en men√∫ principal
- **Causa**: Falta de diversidad en estados de men√∫ durante entrenamiento
- **Enfoque**: Penalizaciones manuales (soluci√≥n parcial)

#### Soluci√≥n Elegante (V2):
- **Exploraci√≥n Basada en Coordenadas**: Fuerza diversidad espacial
- **Anti-Stuck Reward**: Penalizaci√≥n autom√°tica por permanencia
- **Mejor Representaci√≥n de Estados**: Incluye contexto de acciones recientes
- **Estados Iniciales Diversos**: Entrenamiento desde m√∫ltiples puntos

### Innovaciones Potenciales para Extensi√≥n

#### 1. B√∫squeda H√≠brida RL + Tradicional
```python
class HybridAgent:
    def __init__(self, rl_policy, search_planner):
        self.rl_policy = rl_policy
        self.search_planner = search_planner
    
    def select_action(self, state):
        # RL para exploraci√≥n general
        rl_action = self.rl_policy.predict(state)
        
        # B√∫squeda para objetivos espec√≠ficos
        if self.in_specific_context(state):
            return self.search_planner.plan(state, objective)
        return rl_action
```

#### 2. Curriculum Learning
- Entrenamiento progresivo desde objetivos simples a complejos
- Inicio en Pallet Town ‚Üí Viridian City ‚Üí ... ‚Üí Elite Four

#### 3. Hierarchical Reinforcement Learning
- Pol√≠ticas de alto nivel (objetivos) y bajo nivel (movimientos)
- Descomposici√≥n de tarea en sub-objetivos

#### 4. Meta-Learning
- Adaptaci√≥n r√°pida a nuevas versiones de Pok√©mon
- Transfer learning entre diferentes juegos de la serie

### M√©tricas de Evaluaci√≥n del Agente

#### M√©tricas de Progreso:
1. **Diversidad Espacial**: N√∫mero de coordenadas √∫nicas visitadas
2. **Progreso Narrativo**: Flags de eventos completados
3. **Eficiencia**: Tiempo promedio para alcanzar objetivos
4. **Robustez**: Varianza en rendimiento entre ejecuciones

#### M√©tricas de Comportamiento:
1. **Entrop√≠a de Pol√≠tica**: Medida de exploraci√≥n vs explotaci√≥n
2. **Distribuci√≥n de Acciones**: Evitar dominancia de acciones espec√≠ficas
3. **Persistencia de Objetivos**: Capacidad de mantener planes a largo plazo

### An√°lisis de Sesiones y Uso de analyze_session.py

#### Uso Detallado del Script de An√°lisis:

```bash
# An√°lisis b√°sico de una sesi√≥n
python analyze_session.py v2/session_752558fa

# Desde el directorio v2
cd v2
python ../analyze_session.py session_752558fa
```

**El script proporciona:**

1. **Estad√≠sticas B√°sicas:**
   - Total de pasos ejecutados
   - Duraci√≥n en tiempo real
   - Ubicaciones √∫nicas visitadas
   - Progreso de niveles y medallas

2. **An√°lisis de Mapas:**
   - Distribuci√≥n de tiempo por mapa
   - Identificaci√≥n de √°reas problem√°ticas
   - Patrones de movimiento

3. **Visualizaciones Autom√°ticas:**
   - Gr√°fico de progreso de exploraci√≥n
   - Evoluci√≥n de niveles de Pok√©mon
   - Tendencias de salud del party

4. **Exportaci√≥n de Datos:**
   - Gr√°ficos en formato PNG
   - Datos procesables para an√°lisis posteriores

#### Estructura de Datos de Sesi√≥n:

```python
agent_stats_data = {
    'step': int,           # N√∫mero de paso
    'x': int, 'y': int,    # Coordenadas del jugador
    'map': int,            # ID del mapa actual
    'last_action': int,    # √öltima acci√≥n tomada
    'levels_sum': int,     # Suma de niveles del party
    'hp': float,           # Fracci√≥n de HP total
    'coord_count': int,    # Coordenadas √∫nicas visitadas
    'badge': int,          # N√∫mero de medallas
    'event': float,        # Puntuaci√≥n de eventos
}
```

### Guardado de Sesiones sin Ctrl+C

#### Problema:
Usar Ctrl+C interrumpe el proceso antes del guardado autom√°tico, resultando en p√©rdida de datos.

#### Soluci√≥n 1: Script de Sesi√≥n Controlada

```bash
# Ejecutar sesi√≥n con guardado autom√°tico cada 500 pasos
python run_controlled_session.py 10000 500

# Par√°metros: [max_steps] [save_frequency] [checkpoint_path]
python run_controlled_session.py 5000 250 v2/runs/poke_26214400
```

**Caracter√≠sticas del script:**
- **Guardado Autom√°tico**: Cada N pasos especificados
- **Interrupci√≥n Graceful**: Maneja Ctrl+C correctamente
- **Preservaci√≥n de Datos**: Fuerza guardado antes de terminar
- **Reportes de Progreso**: Estad√≠sticas en tiempo real

#### Soluci√≥n 2: Modificaci√≥n de Par√°metros

```python
# En la configuraci√≥n del entorno, ajustar:
env_config = {
    'max_steps': 5000,          # Episodios m√°s cortos
    'early_stop': True,         # Parada temprana por falta de progreso
    'save_final_state': True,   # Forzar guardado al terminar
}
```

#### Soluci√≥n 3: Uso de TensorBoard para Monitoreo

```bash
# Terminal 1: Ejecutar entrenamiento/sesi√≥n
cd v2
python baseline_fast_v2.py

# Terminal 2: Monitorear en tiempo real
cd v2/runs
tensorboard --logdir .
# Abrir localhost:6006 en navegador
```

## Instalaci√≥n y Configuraci√≥n R√°pida

### Requisitos Previos
- Python 3.10+ (recomendado)
- ffmpeg instalado y disponible en la l√≠nea de comandos
- ROM de Pok√©mon Red legalmente obtenida (1MB, sha1: `ea9bcae617fdf159b045185467ae58b2e4a48b9a`)

### Pasos de Instalaci√≥n

1. **Clonar el repositorio original:**
   ```bash
   git clone https://github.com/PWhiddy/PokemonRedExperiments.git
   cd PokemonRedExperiments
   ```

2. **Crear y activar entorno conda:**
   ```bash
   conda create -n pokeenv python=3.10
   conda activate pokeenv
   ```

3. **Actualizar herramientas de Python:**
   ```bash
   pip install --upgrade pip setuptools wheel
   ```

4. **Colocar la ROM de Pok√©mon Red:**
   - Copiar la ROM legalmente obtenida al directorio base
   - Renombrarla a `PokemonRed.gb`
   - Verificar con: `shasum PokemonRed.gb` (debe coincidir con el hash mencionado arriba)

5. **Instalar dependencias:**
   ```bash
   cd baselines  # o v2 para la versi√≥n mejorada
   pip install -r requirements.txt
   ```
   Para macOS con V2: usar `v2/macos_requirements.txt`

6. **Ejecutar el modelo preentrenado:**
   ```bash
   python run_pretrained_interactive.py
   ```

## Estructura del Proyecto y Descripci√≥n de Archivos

### Directorio Ra√≠z
```
TEL351-PokemonRed/
‚îú‚îÄ‚îÄ PokemonRed.gb              # ROM del juego (debe ser proporcionada por el usuario)
‚îú‚îÄ‚îÄ README.md                  # Este archivo de documentaci√≥n
‚îú‚îÄ‚îÄ analyze_session.py         # Script para analizar sesiones de juego
‚îú‚îÄ‚îÄ run_controlled_session.py  # Script para sesiones con guardado autom√°tico
‚îú‚îÄ‚îÄ *.state                    # Estados guardados del juego
‚îî‚îÄ‚îÄ assets/                    # Recursos gr√°ficos y multimedia
```

#### Estados del Juego (*.state)
- **`init.state`**: Estado inicial b√°sico del juego
- **`has_pokedex.state`**: Estado donde el jugador ya tiene la Pok√©dex
- **`has_pokedex_nballs.state`**: Estado con Pok√©dex y Pok√©balls
- **`fast_text_start.state`**: Estado optimizado para texto r√°pido

### Directorio `baselines/` (Versi√≥n Original)

**Archivos principales:**
- **`red_gym_env.py`**: Entorno de gimnasio principal donde el agente interact√∫a
- **`run_pretrained_interactive.py`**: Script para ejecutar el modelo preentrenado
- **`memory_addresses.py`**: Direcciones de memoria para acceder a datos del juego
- **`requirements.txt`**: Dependencias de Python requeridas

### Directorio `v2/` (Versi√≥n Mejorada - Recomendada)

**Mejoras de la V2:**
- Entrenamiento m√°s r√°pido y eficiente en memoria
- Sistema de recompensas basado en coordenadas
- Mejor manejo de men√∫s y estados bloqueados
- Streaming al mapa habilitado por defecto

**Archivos principales:**
- **`red_gym_env_v2.py`**: Entorno optimizado con mejoras t√©cnicas
- **`baseline_fast_v2.py`**: Script principal de entrenamiento
- **`run_pretrained_interactive.py`**: Ejecutor del modelo preentrenado para V2

## Uso de analyze_session.py

### Sintaxis B√°sica

```bash
# Analizar una sesi√≥n espec√≠fica
python analyze_session.py v2/session_752558fa

# Si ya est√°s en el directorio v2/
cd v2
python ../analyze_session.py session_752558fa
```

### Informaci√≥n Proporcionada

El script muestra:

1. **Estad√≠sticas B√°sicas:**
   - Total de pasos ejecutados
   - Duraci√≥n estimada en minutos de juego
   - Ubicaciones √∫nicas visitadas
   - Nivel m√°ximo alcanzado
   - Medallas obtenidas
   - N√∫mero de muertes

2. **An√°lisis de Mapas:**
   - Lista de mapas visitados con nombres
   - Tiempo gastado en cada ubicaci√≥n
   - Patrones de movimiento

3. **Gr√°ficos Autom√°ticos:**
   - Progreso de exploraci√≥n vs tiempo
   - Evoluci√≥n de niveles de Pok√©mon
   - Fluctuaciones de salud del party
   - Guardado autom√°tico como PNG

4. **Archivos de Datos:**
   - Conteo de screenshots disponibles
   - Estados finales capturados
   - Archivos JSON con res√∫menes

### Interpretaci√≥n de Resultados

```bash
# Ejemplo de salida
Analizando sesi√≥n: session_752558fa
==================================================
Estad√≠sticas B√°sicas:
  ‚Ä¢ Total de pasos: 8,459
  ‚Ä¢ Duraci√≥n: ~141.0 minutos de juego  
  ‚Ä¢ Ubicaciones √∫nicas: 342
  ‚Ä¢ Nivel m√°ximo alcanzado: 15
  ‚Ä¢ Medallas obtenidas: 1
  ‚Ä¢ Muertes: 2

Mapas visitados (15):
  ‚Ä¢ Pallet Town: 1,203 pasos
  ‚Ä¢ Route 1: 856 pasos
  ‚Ä¢ Viridian City: 2,341 pasos
  ...

Gr√°fico guardado en: session_752558fa/analysis_plot.png
Screenshots disponibles: 169
Estados finales: 3
```

## Guardar Sesiones sin Interrupci√≥n Manual

### Problema con Ctrl+C

Cuando usas Ctrl+C para terminar una sesi√≥n interactiva:
- El proceso se interrumpe abruptamente
- No se ejecutan las rutinas de guardado
- Se pierden estad√≠sticas y datos de la sesi√≥n
- Los directorios de sesi√≥n quedan vac√≠os

### Soluci√≥n 1: Script de Sesi√≥n Controlada

Usa el script `run_controlled_session.py` incluido:

```bash
# Ejecutar sesi√≥n de 10,000 pasos con guardado cada 500 pasos
python run_controlled_session.py 10000 500

# Especificar modelo personalizado
python run_controlled_session.py 5000 250 v2/runs/poke_26214400

# Par√°metros: [max_steps] [save_frequency] [checkpoint_path]
```

**Ventajas del script controlado:**
- Guardado autom√°tico peri√≥dico
- Manejo correcto de Ctrl+C (guardado antes de terminar)
- Reportes de progreso en tiempo real
- Preservaci√≥n completa de datos de sesi√≥n

### Soluci√≥n 2: Configurar L√≠mites de Tiempo

```python
# Modificar par√°metros en run_pretrained_interactive.py
env_config = {
    'max_steps': 5000,          # Episodios m√°s cortos
    'save_final_state': True,   # Forzar guardado al terminar
    'print_rewards': True,      # Ver progreso en consola
}
```

### Soluci√≥n 3: Monitoreo con TensorBoard

```bash
# Terminal 1: Ejecutar sesi√≥n/entrenamiento
cd v2
python baseline_fast_v2.py  # o run_pretrained_interactive.py

# Terminal 2: Monitorear progreso en tiempo real
cd v2/runs  # o directorio de sesi√≥n correspondiente
tensorboard --logdir .

# Abrir navegador en localhost:6006
```

Esto permite:
- Ver m√©tricas en tiempo real
- Decidir cu√°ndo terminar basado en progreso
- Obtener datos incluso si se interrumpe el proceso

## Entrenamiento del Modelo

### Versi√≥n V2 (Recomendada)
```bash
cd v2
python baseline_fast_v2.py
```

### Versi√≥n Original
```bash
cd baselines
python run_baseline_parallel_fast.py
```

## Uso Interactivo

Una vez ejecutando `run_pretrained_interactive.py`:
- **Teclas de flecha**: Movimiento
- **A y S**: Botones A y B del Game Boy
- **Pausar IA**: Editar `agent_enabled.txt` (cambiar a `False`)

## Personalizaci√≥n Avanzada

### Modificar Recompensas
Editar las funciones de recompensa en `red_gym_env_v2.py` para cambiar el comportamiento del agente.

### Cambiar Estado Inicial
Modificar `init_state` en la configuraci√≥n del entorno para comenzar desde diferentes puntos del juego.

### Agregar Nuevas Observaciones
A√±adir direcciones de memoria en `memory_addresses.py` y modificar las observaciones del entorno.

## Notas Importantes

- El archivo `PokemonRed.gb` DEBE estar en el directorio principal
- El directorio actual DEBE ser `baselines/` o `v2/` al ejecutar scripts
- Python 3.10+ es altamente recomendado para compatibilidad
- Para GPUs AMD, seguir la gu√≠a de instalaci√≥n de PyTorch con ROCm

## Recursos Adicionales

- [Video explicativo en YouTube](https://youtu.be/DcYLT37ImBY)
- [Servidor Discord del proyecto](http://discord.gg/RvadteZk4G)
- [Visualizaci√≥n en vivo del mapa](https://pwhiddy.github.io/pokerl-map-viz/)
- [Repositorio original](https://github.com/PWhiddy/PokemonRedExperiments)

## Soluci√≥n de Problemas

1. **Error de ROM**: Verificar que `PokemonRed.gb` est√© en el directorio correcto con el hash correcto
2. **Problemas de dependencias**: Usar entorno virtual y versi√≥n espec√≠fica de Python
3. **Errores de SDL**: Instalar bibliotecas SDL por separado si es necesario
4. **Rendimiento lento**: Considerar usar la versi√≥n V2 y ajustar `action_freq`
5. **Error PyBoy V2 - APIs obsoletas**: 
   - **Problema 1 - Memoria**: `AttributeError: 'PyBoy' object has no attribute 'memory'`
     ```python
     # En v2/red_gym_env_v2.py, cambiar:
     return self.pyboy.memory[addr]
     # A:
     return self.pyboy.get_memory_value(addr)
     ```
   - **Problema 2 - Pantalla**: `AttributeError: 'PyBoy' object has no attribute 'screen'`
     ```python
     # Descomentar y corregir en v2/red_gym_env_v2.py:
     self.screen = self.pyboy.botsupport_manager().screen()
     # Y cambiar:
     game_pixels_render = self.pyboy.screen.ndarray[:,:,0:1]
     # A:
     game_pixels_render = self.screen.screen_ndarray()[:,:,0:1]
     ```
   - **Problema 3 - Tick**: `TypeError: tick() takes exactly 0 positional arguments`
     ```python
     # Cambiar de:
     self.pyboy.tick(press_step, render_screen)
     # A:
     self.pyboy.tick()
     ```
   - **Nota**: Estos errores ya est√°n corregidos en este repositorio
6. **Error "Could not deserialize object tensorboard_log"**: 
   - Es una advertencia, no afecta la ejecuci√≥n
   - Relacionado con compatibilidad de rutas entre Windows y sistemas Unix

---

*Este proyecto es una implementaci√≥n educativa de aprendizaje por refuerzo aplicado a videojuegos retro. Requiere una copia legal de Pok√©mon Red.*
```

### üéÆ Sesiones de Juego Interactivas

Cuando ejecutas `run_pretrained_interactive.py`, se generan varios tipos de archivos:

#### Directorios de Sesi√≥n
- **Ubicaci√≥n**: `v2/session_[ID_√öNICO]/` (ej: `session_752558fa/`)
- **Contenido**: Screenshots, estad√≠sticas, estados finales
- **Cu√°ndo se crean**: Solo cuando el agente completa una sesi√≥n completa o llega al l√≠mite de pasos

#### Archivos Generados por Sesi√≥n:
```
session_[ID]/
‚îú‚îÄ‚îÄ curframe_[ID].jpeg          # Screenshot del frame actual (cada 50 pasos)
‚îú‚îÄ‚îÄ agent_stats_[ID].csv.gz     # Estad√≠sticas detalladas del agente
‚îú‚îÄ‚îÄ all_runs_[ID].json          # Resumen de todas las ejecuciones
‚îî‚îÄ‚îÄ final_states/               # Estados finales cuando termina
    ‚îú‚îÄ‚îÄ frame_r[score]_full.jpeg
    ‚îî‚îÄ‚îÄ frame_r[score]_small.jpeg
```

#### ‚≠ê Archivos Clave para Analizar el Comportamiento:

**1. `agent_stats_[ID].csv.gz`** - Contiene datos paso a paso:
- Posici√≥n (x, y) del jugador
- Mapa actual
- √öltima acci√≥n tomada
- Niveles de Pok√©mon
- Puntos de vida (HP)
- Exploraci√≥n realizada
- Medallas obtenidas
- Eventos completados

**2. `curframe_[ID].jpeg`** - Screenshots autom√°ticos cada 50 pasos

**3. `all_runs_[ID].json`** - Puntuaciones de recompensas

### ü§ñ C√≥mo Est√° Entrenado el Agente V2

#### Algoritmo: PPO (Proximal Policy Optimization)
El agente usa PPO, un algoritmo de aprendizaje por refuerzo que:
- Balancea exploraci√≥n vs explotaci√≥n
- Actualiza la pol√≠tica de forma estable
- Aprende de m√∫ltiples entornos en paralelo

#### Configuraci√≥n de Entrenamiento (baseline_fast_v2.py):
```python
env_config = {
    'action_freq': 24,              # Ejecuta acci√≥n cada 24 frames
    'max_steps': 2048 * 80,         # ~163,840 pasos m√°ximo por episodio
    'init_state': '../init.state',  # Inicia desde el estado inicial
    'reward_scale': 0.5,            # Escala de recompensas
    'explore_weight': 0.25,         # Peso de recompensa por exploraci√≥n
    'headless': True,               # Sin interfaz gr√°fica durante entrenamiento
}

# Entrenamiento en paralelo con 64 CPU cores
num_cpu = 64
# Modelo PPO con par√°metros optimizados
model = PPO("MultiInputPolicy", env, 
    n_steps=train_steps_batch,      # 2048 pasos por actualizaci√≥n
    batch_size=512,                 # Tama√±o de lote
    n_epochs=1,                     # √âpocas por actualizaci√≥n  
    gamma=0.997,                    # Factor de descuento
    ent_coef=0.01,                  # Coeficiente de entrop√≠a
)
```

#### Sistema de Recompensas Sofisticado:
1. **Exploraci√≥n por Coordenadas**: Recompensa por visitar nuevas ubicaciones
2. **Progreso de Niveles**: Bonificaci√≥n por subir niveles de Pok√©mon
3. **Eventos del Juego**: Puntos por completar objetivos (obtener Pok√©dex, etc.)
4. **Salud del Party**: Penalizaci√≥n por morir, bonificaci√≥n por curarse
5. **Medallas**: Gran recompensa por obtener medallas de gimnasio
6. **Batallas**: Recompensa por enfrentar oponentes de alto nivel

#### Observaciones del Agente:
- **Pantallas**: Frames del juego procesados (72x80 pixels)
- **Salud**: Estado de HP normalizado
- **Niveles**: Niveles de Pok√©mon codificados
- **Medallas**: Vector binario de medallas obtenidas
- **Eventos**: Flags de eventos del juego
- **Mapa**: Representaci√≥n de ubicaci√≥n actual
- **Acciones Recientes**: Historial de acciones pasadas

### ÔøΩ Script de An√°lisis Autom√°tico

Hemos incluido un script para analizar f√°cilmente las sesiones:

```bash
# Analizar una sesi√≥n espec√≠fica
python analyze_session.py v2/session_752558fa

# Si ya est√°s en el directorio v2/
cd v2
python ../analyze_session.py session_752558fa
```

**El script muestra:**
- Estad√≠sticas b√°sicas (pasos, duraci√≥n, exploraci√≥n)
- Mapas visitados y tiempo en cada uno
- Progreso de niveles y salud
- Gr√°ficos de progreso (guardados como PNG)
- Resumen de recompensas
- Conteo de screenshots disponibles

### üéÆ Consejos para Generar Sesiones Completas

Para que se guarden datos completos:
1. **Deja que el agente complete m√°s pasos**: No termines inmediatamente con Ctrl+C
2. **Configura l√≠mites apropiados**: El agente guarda datos al completar episodios
3. **Usa modo no interactivo**: Para entrenar y generar datos autom√°ticamente

#### 1. Usar Pandas para Analizar Estad√≠sticas:
```python
import pandas as pd
import gzip

# Cargar estad√≠sticas de una sesi√≥n
df = pd.read_csv('session_[ID]/agent_stats_[ID].csv.gz', compression='gzip')

# Ver progreso del agente
print(df[['step', 'x', 'y', 'map', 'levels_sum', 'badge']].head(20))

# Analizar exploraci√≥n
print(f"Ubicaciones √∫nicas visitadas: {df[['x', 'y', 'map']].drop_duplicates().shape[0]}")

# Ver progreso de niveles
print(df['levels_sum'].plot())
```

#### 2. Examinar Screenshots:
Las im√°genes `curframe_*.jpeg` muestran exactamente lo que vio el agente en diferentes momentos.

#### 3. Usar TensorBoard para M√©tricas Detalladas:
```bash
cd v2/runs
tensorboard --logdir .
```
Navegar a `localhost:6006` para ver:
- Curvas de recompensa
- P√©rdidas del modelo
- M√©tricas de exploraci√≥n
- Progreso de entrenamiento

### üìà Por Qu√© V2 es tan Efectivo

1. **Exploraci√≥n Basada en Coordenadas**: En lugar de comparar frames (computacionalmente costoso), usa coordenadas del mapa
2. **Recompensas Balanceadas**: Sistema de puntuaci√≥n que incentiva tanto exploraci√≥n como progreso
3. **Entrenamiento Masivo en Paralelo**: 64 entornos simult√°neos aceleran el aprendizaje
4. **Observaciones Ricas**: El agente tiene acceso a m√∫ltiples tipos de informaci√≥n del juego
5. **Modelo Preentrenado**: `poke_26214400.zip` representa 26,214,400 pasos de entrenamiento (‚âà18 horas de juego continuo)do## üö® Soluci√≥n de Problemas

1. **Error de ROM**: Verificar que `PokemonRed.gb` est√© en el directorio correcto con el hash correcto
2. **Problemas de dependencias**: Usar entorno virtual y versi√≥n espec√≠fica de Python
3. **Errores de SDL**: Instalar bibliotecas SDL por separado si es necesario
4. **Rendimiento lento**: Considerar usar la versi√≥n V2 y ajustar `action_freq`
5. **Error PyBoy V2 - APIs obsoletas**: 
   - **Problema 1 - Memoria**: `AttributeError: 'PyBoy' object has no attribute 'memory'`
     ```python
     # En v2/red_gym_env_v2.py, l√≠nea ~461, cambiar:
     return self.pyboy.memory[addr]
     # A:
     return self.pyboy.get_memory_value(addr)
     ```
   - **Problema 2 - Pantalla**: `AttributeError: 'PyBoy' object has no attribute 'screen'`
     ```python
     # En v2/red_gym_env_v2.py, descomentar l√≠nea ~117:
     self.screen = self.pyboy.botsupport_manager().screen()
     # Y en l√≠nea ~171, cambiar:
     game_pixels_render = self.pyboy.screen.ndarray[:,:,0:1]
     # A:
     game_pixels_render = self.screen.screen_ndarray()[:,:,0:1]
     ```
   - **Problema 3 - Tick**: `TypeError: tick() takes exactly 0 positional arguments`
     ```python
     # En v2/red_gym_env_v2.py, m√©todo run_action_on_emulator, cambiar:
     self.pyboy.tick(press_step, render_screen)
     # A loops individuales:
     for i in range(press_step):
         self.pyboy.tick()
     ```
   - **Nota**: Estos errores ya est√°n corregidos en este repositorio
6. **Error "Could not deserialize object tensorboard_log"**: 
   - Es una advertencia, no afecta la ejecuci√≥n
   - Relacionado con compatibilidad de rutas entre Windows y sistemas Unixuego, capturar Pok√©mon, luchar en batallas y completar objetivos usando t√©cnicas de aprendizaje profundo.

## Descripci√≥n General del Proyecto

El proyecto utiliza PyBoy (un emulador de Game Boy) junto con Stable Baselines3 para crear un entorno de gimnasio donde los agentes pueden interactuar con Pok√©mon Red. El agente observa las pantallas del juego y aprende pol√≠ticas √≥ptimas mediante algoritmos como PPO (Proximal Policy Optimization).

## Instalaci√≥n y Configuraci√≥n R√°pida

### Requisitos Previos
- Python 3.10+ (recomendado)
- ffmpeg instalado y disponible en la l√≠nea de comandos
- ROM de Pok√©mon Red legalmente obtenida (1MB, sha1: `ea9bcae617fdf159b045185467ae58b2e4a48b9a`)

### Pasos de Instalaci√≥n

1. **Clonar el repositorio original:**
   ```bash
   git clone https://github.com/PWhiddy/PokemonRedExperiments.git
   cd PokemonRedExperiments
   ```

2. **Crear y activar entorno conda:**
   ```bash
   conda create -n pokeenv python=3.10
   conda activate pokeenv
   ```

3. **Actualizar herramientas de Python:**
   ```bash
   pip install --upgrade pip setuptools wheel
   ```

4. **Colocar la ROM de Pok√©mon Red:**
   - Copiar la ROM legalmente obtenida al directorio base
   - Renombrarla a `PokemonRed.gb`
   - Verificar con: `shasum PokemonRed.gb` (debe coincidir con el hash mencionado arriba)

5. **Instalar dependencias:**
   ```bash
   cd baselines
   pip install -r requirements.txt
   ```
   Para macOS con V2: usar `v2/macos_requirements.txt`

6. **Ejecutar el modelo preentrenado:**
   ```bash
   python run_pretrained_interactive.py
   ```

## Estructura del Proyecto y Descripci√≥n de Archivos

### Directorio Ra√≠z
```
TEL351-PokemonRed/
‚îú‚îÄ‚îÄ PokemonRed.gb              # ROM del juego (debe ser proporcionada por el usuario)
‚îú‚îÄ‚îÄ README.md                  # Este archivo de documentaci√≥n
‚îú‚îÄ‚îÄ README_BASE.md             # Documentaci√≥n original del proyecto
‚îú‚îÄ‚îÄ LICENSE                    # Licencia del proyecto
‚îú‚îÄ‚îÄ windows-setup-guide.md     # Gu√≠a espec√≠fica para Windows
‚îú‚îÄ‚îÄ VisualizeProgress.ipynb    # Notebook para visualizar el progreso del entrenamiento
‚îî‚îÄ‚îÄ *.state                    # Estados guardados del juego
```

#### Estados del Juego (*.state)
- **`init.state`**: Estado inicial b√°sico del juego
- **`has_pokedex.state`**: Estado donde el jugador ya tiene la Pok√©dex
- **`has_pokedex_nballs.state`**: Estado con Pok√©dex y Pok√©balls
- **`fast_text_start.state`**: Estado optimizado para texto r√°pido

### Directorio `baselines/` (Versi√≥n Original)

**Archivos principales de entrenamiento:**
- **`red_gym_env.py`**: **ARCHIVO CLAVE** - Define el entorno de gimnasio principal donde el agente interact√∫a con el juego
- **`run_baseline_parallel.py`**: Script para entrenar m√∫ltiples agentes en paralelo
- **`run_baseline_parallel_fast.py`**: Versi√≥n optimizada del entrenamiento paralelo
- **`run_pretrained_interactive.py`**: **EJECUTAR MODELO** - Script para ejecutar el modelo preentrenado de forma interactiva

**Archivos de configuraci√≥n del agente:**
- **`memory_addresses.py`**: **CONFIGURACI√ìN DEL JUEGO** - Define las direcciones de memoria para acceder a datos del juego (posici√≥n, salud, dinero, etc.)
- **`agent_enabled.txt`**: Archivo de control para pausar/reanudar la IA durante la ejecuci√≥n
- **`global_map.py`**: Manejo del mapa global del juego
- **`map_data.json`**: Datos del mapa en formato JSON

**Archivos de utilidades:**
- **`stream_agent_wrapper.py`**: Wrapper para transmitir sesiones de entrenamiento en vivo
- **`tensorboard_callback.py`**: Callbacks para logging con TensorBoard
- **`tile_vids_to_grid.py`**: Utilidad para crear videos en cuadr√≠cula
- **`render_all_needed_grids.py`**: Renderizado de cuadr√≠culas necesarias
- **`delete_empty_imgs.txt`**: Script para limpiar im√°genes vac√≠as

**Archivos de datos:**
- **`requirements.txt`**: Dependencias de Python requeridas
- **`events.json`**: Eventos del juego en formato JSON
- **`saves_to_record.txt`**: Lista de estados guardados para grabar

### Directorio `v2/` (Versi√≥n Mejorada - Recomendada)

**Mejoras de la V2:**
- Entrenamiento m√°s r√°pido y eficiente en memoria
- Alcanza Cerulean City
- Streaming al mapa habilitado por defecto
- Recompensa de exploraci√≥n basada en coordenadas en lugar de KNN de frames

**Archivos principales:**
- **`red_gym_env_v2.py`**: **ENTORNO MEJORADO** - Versi√≥n optimizada del entorno de gimnasio
- **`baseline_fast_v2.py`**: **ENTRENAMIENTO V2** - Script principal de entrenamiento de la versi√≥n 2
- **`run_pretrained_interactive.py`**: Ejecutor del modelo preentrenado para V2
- **`requirements.txt`** / **`macos_requirements.txt`**: Dependencias espec√≠ficas para cada SO

### Directorio `visualization/`

**Notebooks y scripts de visualizaci√≥n:**
- **`Agent_Visualization.ipynb`**: Visualizaci√≥n del comportamiento del agente
- **`BetterMapVis.ipynb`**: Visualizaci√≥n mejorada del mapa
- **`BetterMapVis_script_version.py`**: Versi√≥n en script de la visualizaci√≥n del mapa
- **`BetterMapVis_script_version_FLOW.py`**: Visualizaci√≥n con flujo de movimiento
- **`Create_Video_Grids.ipynb`**: Creaci√≥n de videos en cuadr√≠cula
- **`Map_Stitching.ipynb`**: Uni√≥n de mapas
- **`MapWalkingVis.ipynb`**: Visualizaci√≥n de caminatas en el mapa

### Directorio `clip_experiment/`

Experimentos con CLIP (Contrastive Language-Image Pre-training):
- **`Interacting_with_CLIP_Pokemon.ipynb`**: Notebook para experimentos con CLIP
- **`location_descriptions/`**: Im√°genes con descripciones de ubicaciones
- **`test_images/`**: Im√°genes de prueba para CLIP

### Directorio `assets/`

Recursos gr√°ficos y multimedia:
- **`grid.png`**: Imagen de cuadr√≠cula para visualizaci√≥n
- **`poke_map.gif`**: GIF animado del mapa de Pok√©mon
- **`youtube.jpg`**: Miniatura del video de YouTube
- **`sblogo.png`**: Logo de Stable Baselines
- **`pyboy.svg`**: Logo de PyBoy

## Archivos Clave para Comportamiento del Agente

### 1. **Pol√≠ticas y Acciones del Agente**

**`red_gym_env.py`** (l√≠neas clave):
- **Espacio de acciones**: Define qu√© botones puede presionar el agente
- **Funci√≥n de recompensa**: Determina c√≥mo se eval√∫a el comportamiento
- **Observaciones**: Qu√© informaci√≥n recibe el agente del juego
- **Exploraci√≥n**: Sistema de recompensas por explorar nuevas √°reas

**`memory_addresses.py`**:
- **Posici√≥n del jugador**: `X_POS_ADDRESS`, `Y_POS_ADDRESS`
- **Informaci√≥n del party**: `PARTY_SIZE_ADDRESS`, `LEVELS_ADDRESSES`
- **Estado del juego**: `BADGE_COUNT_ADDRESS`, `MONEY_ADDRESS_*`
- **Eventos**: `EVENT_FLAGS_START_ADDRESS`, `EVENT_FLAGS_END_ADDRESS`

### 2. **Configuraci√≥n de Estados Iniciales**

**Estados disponibles** (archivos `.state`):
- Modifica `init_state` en la configuraci√≥n del entorno para cambiar d√≥nde inicia el agente
- Cada estado representa un punto diferente en el progreso del juego

**En `run_pretrained_interactive.py`**:
```python
env_config = {
    'init_state': '../has_pokedex_nballs.state',  # CAMBIAR AQU√ç el estado inicial
    'action_freq': 24,                            # Frecuencia de acciones
    'headless': False,                            # Mostrar ventana del juego
    'max_steps': ep_length,                       # M√°ximo de pasos por episodio
    # ... m√°s configuraciones
}
```

### 3. **Par√°metros Editables del Comportamiento**

**En `red_gym_env.py`** (configuraciones importantes):
- **`explore_weight`**: Peso de la recompensa por exploraci√≥n
- **`reward_scale`**: Escala general de recompensas
- **`action_freq`**: Frecuencia de ejecuci√≥n de acciones
- **`similar_frame_dist`**: Distancia para considerar frames similares
- **`use_screen_explore`**: Usar exploraci√≥n basada en pantalla

**En `memory_addresses.py`**:
- Puedes agregar nuevas direcciones de memoria para acceder a m√°s datos del juego
- √ötil para crear nuevas recompensas o condiciones

## Entrenamiento del Modelo

### Versi√≥n V2 (Recomendada)
```bash
cd v2
python baseline_fast_v2.py
```

### Versi√≥n Original
```bash
cd baselines
python run_baseline_parallel_fast.py
```

## Monitoreo del Progreso

### TensorBoard (Local)
```bash
cd [directorio_de_sesi√≥n]
tensorboard --logdir .
```
Luego navegar a `localhost:6006`

### Transmisi√≥n en Vivo
El proyecto incluye capacidad para transmitir sesiones de entrenamiento a un mapa global compartido usando `stream_agent_wrapper.py`.

### Visualizaci√≥n Est√°tica
Usar los notebooks en el directorio `visualization/` para an√°lisis detallado del comportamiento del agente.

## Uso Interactivo

Una vez ejecutando `run_pretrained_interactive.py`:
- **Teclas de flecha**: Movimiento
- **A y S**: Botones A y B del Game Boy
- **Pausar IA**: Editar `agent_enabled.txt` (cambiar a `False`)

## Personalizaci√≥n Avanzada

### Modificar Recompensas
Editar la funci√≥n `_calculate_reward()` en `red_gym_env.py` para cambiar c√≥mo el agente es recompensado.

### Cambiar Estado Inicial
Modificar `init_state` en la configuraci√≥n del entorno para comenzar desde diferentes puntos del juego.

### Agregar Nuevas Observaciones
A√±adir direcciones de memoria en `memory_addresses.py` y modificar `_get_obs()` en el entorno.

### Configurar Exploraci√≥n
Ajustar `explore_weight` y `use_screen_explore` para cambiar el comportamiento exploratorio.

## Notas Importantes

- El archivo `PokemonRed.gb` DEBE estar en el directorio principal
- El directorio actual DEBE ser `baselines/` o `v2/` al ejecutar scripts
- Python 3.10+ es altamente recomendado para compatibilidad
- Para GPUs AMD, seguir la gu√≠a de instalaci√≥n de PyTorch con ROCm

## Recursos Adicionales

- [Video explicativo en YouTube](https://youtu.be/DcYLT37ImBY)
- [Servidor Discord del proyecto](http://discord.gg/RvadteZk4G)
- [Visualizaci√≥n en vivo del mapa](https://pwhiddy.github.io/pokerl-map-viz/)
- [Repositorio original](https://github.com/PWhiddy/PokemonRedExperiments)

## Soluci√≥n de Problemas

1. **Error de ROM**: Verificar que `PokemonRed.gb` est√© en el directorio correcto con el hash correcto
2. **Problemas de dependencias**: Usar entorno virtual y versi√≥n espec√≠fica de Python
3. **Errores de SDL**: Instalar bibliotecas SDL por separado si es necesario
4. **Rendimiento lento**: Considerar usar la versi√≥n V2 y ajustar `action_freq`

---

*Este proyecto es una implementaci√≥n educativa de aprendizaje por refuerzo aplicado a videojuegos retro. Requiere una copia legal de Pok√©mon Red.*
