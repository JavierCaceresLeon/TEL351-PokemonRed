# Entrenamiento de Agentes de Aprendizaje por Refuerzo para Pok√©mon Red

Este proyecto### üîß Script de An√°lisis Autom√°tico - analyze_session.py

#### üìã Uso Detallado del Script

**Sintaxis:**
```bash
python analyze_session.py [directorio_de_sesi√≥n]
```

**Ejemplos Pr√°cticos:**
```bash
# Desde el directorio ra√≠z del proyecto
python analyze_session.py v2/session_752558fa

# Desde dentro del directorio v2/
cd v2
python ../analyze_session.py session_752558fa

# Analizar la sesi√≥n m√°s reciente (encontrar autom√°ticamente)
python ../analyze_session.py $(ls -td session_*/ | head -1)
```

**Lo que hace el script paso a paso:**

1. **Verificaci√≥n de archivos**: Busca `agent_stats_*.csv.gz` en el directorio
2. **Carga de datos**: Lee el archivo CSV comprimido con pandas
3. **An√°lisis estad√≠stico**: Calcula m√©tricas de rendimiento
4. **Visualizaci√≥n**: Genera gr√°ficos de progreso autom√°ticamente
5. **Reporte**: Muestra resumen en consola

**Salida del script:**
```
üîç Analizando sesi√≥n: session_752558fa
==================================================
üìä Cargando: agent_stats_abc123.csv.gz

üìà Estad√≠sticas B√°sicas:
  ‚Ä¢ Total de pasos: 15,432
  ‚Ä¢ Duraci√≥n: ~257.2 minutos de juego
  ‚Ä¢ Ubicaciones √∫nicas: 1,234
  ‚Ä¢ Nivel m√°ximo alcanzado: 12
  ‚Ä¢ Medallas obtenidas: 1
  ‚Ä¢ Muertes: 3

üó∫Ô∏è Mapas visitados (8):
  ‚Ä¢ Pallet Town: 5,432 pasos
  ‚Ä¢ Route 1: 3,221 pasos
  ‚Ä¢ Viridian City: 2,100 pasos
  ...

üìä Gr√°fico guardado en: session_752558fa/analysis_plot.png
üèÜ Resumen de ejecuciones (1 runs):
  ‚Ä¢ event: 245.67
  ‚Ä¢ level: 89.32
  ‚Ä¢ explore: 156.78
  ...
üì∏ Screenshots disponibles: 308
üéØ Estados finales: 2
```

#### üíæ C√≥mo Guardar Sesiones Completas (Sin Ctrl+C)

**Problema**: Ctrl+C termina abruptamente y no guarda datos.

**Soluciones:**

**M√©todo 1: Usar el archivo agent_enabled.txt**
```bash
# 1. Iniciar el agente
python run_pretrained_interactive.py

# 2. En otra terminal, pausar el agente
echo "False" > agent_enabled.txt

# 3. Para reanudar
echo "True" > agent_enabled.txt

# 4. Para terminar limpiamente
echo "False" > agent_enabled.txt
# Luego presionar 'q' en la ventana del juego
```

**M√©todo 2: Modificar max_steps en run_pretrained_interactive.py**
```python
# L√≠nea ~33 en run_pretrained_interactive.py
env_config = {
    'max_steps': 5000,  # Cambiar de 2**23 a un n√∫mero menor
    # ... resto de configuraci√≥n
}
```

**M√©todo 3: Usar timeout en la terminal**
```bash
# Ejecutar por m√°ximo 10 minutos
timeout 600 python run_pretrained_interactive.py
```

**M√©todo 4: Script de ejecuci√≥n autom√°tica** (recomendado para an√°lisis)
```bash
# Usar el script incluido para sesiones controladas
python run_controlled_session.py 5        # 5 minutos con ventana
python run_controlled_session.py 10 True  # 10 minutos sin ventana (headless)

# El script autom√°ticamente:
# 1. Ejecuta el agente por el tiempo especificado
# 2. Termina limpiamente el proceso
# 3. Encuentra la sesi√≥n generada
# 4. Ejecuta el an√°lisis autom√°ticamente
```

**M√©todo 5: Control manual con agent_enabled.txt**
```python
# Crear archivo: run_analysis_session.py
import subprocess
import time
import signal
import os

def run_timed_session(duration_minutes=10):
    process = subprocess.Popen(['python', 'run_pretrained_interactive.py'])
    time.sleep(duration_minutes * 60)
    process.send_signal(signal.SIGTERM)  # Terminaci√≥n limpia
    process.wait()

if __name__ == "__main__":
    run_timed_session(5)  # 5 minutos de sesi√≥n
```no de aprendizaje por refuerzo para entrenar agentes de IA que jueguen Pok√©mon Red autom√°ticamente. El agente aprende a nave## üìä Monitoreo del Progreso y Revisi√≥n de Sesiones

### üéÆ Sesiones de Juego Interactivas

Cuando ejecutas `run_pretrained_interactive.py`, se generan varios tipos de archivos:

#### Directorios de Sesi√≥n
- **Ubicaci√≥n**: `v2/session_[ID_√öNICO]/` (ej: `session_752558fa/`)
- **Contenido**: Screenshots, estad√≠sticas, estados finales
- **Cu√°ndo se crean**: Solo cuando el agente completa una sesi√≥n completa o llega al l√≠mite de pasos

#### Archivos Generados por Sesi√≥n:
```
session_[ID]/
‚îú‚îÄ‚îÄ curframe_[ID].jpeg          # Screenshot del frame actual (cada 50 pasos)
‚îú‚îÄ‚îÄ agent_stats_[ID].csv.gz     # Estad√≠sticas detalladas del agente
‚îú‚îÄ‚îÄ all_runs_[ID].json          # Resumen de todas las ejecuciones
‚îî‚îÄ‚îÄ final_states/               # Estados finales cuando termina
    ‚îú‚îÄ‚îÄ frame_r[score]_full.jpeg
    ‚îî‚îÄ‚îÄ frame_r[score]_small.jpeg
```

#### ‚≠ê Archivos Clave para Analizar el Comportamiento:

**1. `agent_stats_[ID].csv.gz`** - Contiene datos paso a paso:
- Posici√≥n (x, y) del jugador
- Mapa actual
- √öltima acci√≥n tomada
- Niveles de Pok√©mon
- Puntos de vida (HP)
- Exploraci√≥n realizada
- Medallas obtenidas
- Eventos completados

**2. `curframe_[ID].jpeg`** - Screenshots autom√°ticos cada 50 pasos

**3. `all_runs_[ID].json`** - Puntuaciones de recompensas

## ÔøΩ An√°lisis T√©cnico Detallado del Entrenamiento (Para Agentes Inteligentes)

### üî¨ Diferencia Fundamental: B√∫squeda vs Aprendizaje por Refuerzo

#### Algoritmos de B√∫squeda Cl√°sicos (Lo que NO usa este proyecto):
- **Tab√∫ Search**: Mantiene lista de movimientos prohibidos para evitar ciclos
- **Greedy**: Toma decisiones basadas en el mejor resultado inmediato
- **A***: Busca camino √≥ptimo con heur√≠stica
- **Limitaci√≥n**: Requieren conocimiento previo del espacio de estados y funci√≥n objetivo

#### Aprendizaje por Refuerzo (Lo que S√ç usa):
- **No requiere conocimiento previo** del juego
- **Aprende mediante interacci√≥n** con el entorno
- **Optimiza recompensas a largo plazo**, no solo inmediatas
- **Generaliza** a situaciones no vistas durante entrenamiento

### üéØ Sistema de Recompensas/Penalizaciones Detallado

#### üìç Ubicaci√≥n del C√≥digo de Recompensas:
- **Archivo principal**: `v2/red_gym_env_v2.py`
- **Funci√≥n clave**: `get_game_state_reward()` (l√≠nea ~400)
- **Configuraci√≥n**: `baseline_fast_v2.py` (par√°metros de entrenamiento)

#### üèÜ Tipos de Recompensas (red_gym_env_v2.py):

**1. Recompensas de Exploraci√≥n:**
```python
# L√≠nea ~380 en red_gym_env_v2.py
def get_knn_reward(self):
    # Coordenadas √∫nicas visitadas
    cur_size = len(self.seen_coords)
    base = self.base_explore * 0.005  # Exploraci√≥n base
    post = cur_size * 0.01 if self.levels_satisfied else 0
    return base + post
```
- **Prop√≥sito**: Incentiva explorar nuevas √°reas del mapa
- **Implementaci√≥n**: +0.005 puntos por coordenada nueva antes de nivel 22
- **Bonus**: +0.01 puntos despu√©s del nivel 22 (exploraci√≥n avanzada)

**2. Recompensas de Progreso de Niveles:**
```python
# L√≠nea ~350
def get_levels_reward(self):
    level_sum = self.get_levels_sum()
    if level_sum < 22:  # Exploraci√≥n temprana
        scaled = level_sum
    else:  # Progreso avanzado
        scaled = (level_sum-22) / 4 + 22
    return max(self.max_level_rew, scaled)
```
- **Prop√≥sito**: Recompensa por subir niveles de Pok√©mon
- **Escalamiento**: Lineal hasta nivel 22, luego escalado para evitar sobreajuste

**3. Recompensas por Eventos del Juego:**
```python
# L√≠nea ~420
def get_all_events_reward(self):
    # Suma flags de eventos (obtener Pok√©dex, items, etc.)
    base_event_flags = 13  # Estados iniciales
    total_events = sum([self.bit_count(self.read_m(i)) 
                       for i in range(event_flags_start, event_flags_end)])
    return max(total_events - base_event_flags, 0)
```
- **Prop√≥sito**: Recompensa objetivos espec√≠ficos del juego
- **Ejemplos**: Obtener Pok√©dex (+puntos), conseguir items (+puntos)

**4. Recompensas por Medallas:**
```python
# L√≠nea ~440 en get_game_state_reward()
'badge': self.reward_scale * self.get_badges() * 5
```
- **Prop√≥sito**: Gran bonificaci√≥n por derrotar l√≠deres de gimnasio
- **Multiplicador**: x5 la escala base de recompensas

**5. Sistema de Salud y Penalizaciones:**
```python
# L√≠nea ~310
def update_heal_reward(self):
    cur_health = self.read_hp_fraction()
    if cur_health > self.last_health:
        heal_amount = cur_health - self.last_health
        self.total_healing_rew += heal_amount * 4  # +4 por curaci√≥n
    else:
        self.died_count += 1  # Contador de muertes

# En get_game_state_reward():
'heal': self.reward_scale * self.total_healing_rew,
'dead': self.reward_scale * -0.1 * self.died_count,  # -0.1 por muerte
```

#### üö´ Por Qu√© V2 No Se Queda en Men√∫s (Soluci√≥n al Problema de V1)

**Problema en V1**: El agente se quedaba atascado en men√∫s sin recompensas claras.

**Soluciones en V2:**

**1. Observaciones Enriquecidas:**
```python
# V2 usa observaciones estructuradas (l√≠nea ~185)
observation_space = spaces.Dict({
    "screens": spaces.Box(...),      # Pantalla del juego
    "health": spaces.Box(...),       # Estado de salud
    "level": spaces.Box(...),        # Niveles codificados
    "badges": spaces.MultiBinary(8), # Medallas obtenidas
    "events": spaces.MultiBinary(...), # Eventos del juego
    "map": spaces.Box(...),          # Posici√≥n en mapa
    "recent_actions": spaces.MultiDiscrete(...) # Historial de acciones
})
```

**2. Penalizaciones Impl√≠citas por Inactividad:**
```python
# Si no hay progreso en coordenadas nuevas = sin recompensa de exploraci√≥n
# Si no hay progreso en niveles = sin recompensa de niveles
# Resultado: El agente aprende que estar inm√≥vil = sin recompensas
```

**3. Recompensas por Exploraci√≥n Continua:**
```python
# update_seen_coords() premia movimiento constante
self.seen_coords[coord_string] = self.step_count
# Solo obtiene recompensas si visita coordenadas nuevas
```

### ü§ñ Algoritmo de Entrenamiento: PPO (Proximal Policy Optimization)

#### üìö Fundamentos Te√≥ricos:

**¬øQu√© es PPO?**
- **Familia**: Actor-Critic (combina Policy Gradient + Value Function)
- **Innovaci√≥n**: Actualiza pol√≠tica de forma conservadora para evitar degradaci√≥n
- **Ventaja**: M√°s estable que TRPO, m√°s eficiente que A2C

**Arquitectura del Agente:**
```python
# baseline_fast_v2.py l√≠nea ~85
model = PPO("MultiInputPolicy", env, 
    n_steps=2048,           # Pasos por actualizaci√≥n de pol√≠tica
    batch_size=512,         # Tama√±o de lote para entrenamiento
    n_epochs=1,             # √âpocas por actualizaci√≥n
    gamma=0.997,            # Factor de descuento (0.997 = considera futuro lejano)
    ent_coef=0.01,          # Coeficiente de entrop√≠a (fomenta exploraci√≥n)
)
```

#### üîÑ Ciclo de Entrenamiento Detallado:

**Fase 1: Recolecci√≥n de Experiencias**
```python
# 64 entornos en paralelo √ó 2048 pasos = 131,072 experiencias por iteraci√≥n
num_cpu = 64
n_steps = 2048
```

1. **64 agentes independientes** juegan simult√°neamente
2. Cada uno recolecta **2048 pasos** de experiencia
3. Total: **131,072 transiciones** (estado, acci√≥n, recompensa, siguiente_estado)

**Fase 2: C√°lculo de Ventajas**
```python
# Temporal Difference Learning con Œ≥=0.997
advantage = reward + Œ≥ * V(next_state) - V(current_state)
```

**Fase 3: Actualizaci√≥n de Pol√≠tica**
```python
# Funci√≥n objetivo de PPO
L_PPO = min(
    ratio * advantage,  # ratio = œÄ_new(a|s) / œÄ_old(a|s)
    clip(ratio, 1-Œµ, 1+Œµ) * advantage  # Œµ ‚âà 0.2
)
```

**Fase 4: Repetici√≥n**
- **Total timesteps**: 26,214,400 (n√∫mero en el archivo del modelo)
- **Iteraciones**: ~200 (26M √∑ 131K por iteraci√≥n)
- **Tiempo real**: ~18-24 horas en hardware moderno

### üèóÔ∏è Arquitectura de Red Neural

#### üß† Estructura del Modelo (MultiInputPolicy):

**Encoder de Im√°genes (CNN):**
```python
# Procesa pantallas del juego (72√ó80√ó3)
Conv2D(32, 8√ó8, stride=4) ‚Üí ReLU
Conv2D(64, 4√ó4, stride=2) ‚Üí ReLU  
Conv2D(64, 3√ó3, stride=1) ‚Üí ReLU
Flatten() ‚Üí Dense(512)
```

**Encoder de Features Categ√≥ricas:**
```python
# Procesa badges, events, levels, health
Concatenate(all_features) ‚Üí Dense(256) ‚Üí ReLU
```

**Fusion Layer:**
```python
# Combina visual + categorical
Concatenate(image_features, categorical_features) ‚Üí Dense(512)
```

**Heads de Salida:**
```python
# Actor (pol√≠tica)
Dense(256) ‚Üí ReLU ‚Üí Dense(n_actions) ‚Üí Softmax

# Critic (funci√≥n de valor)
Dense(256) ‚Üí ReLU ‚Üí Dense(1) ‚Üí Linear
```

### üìä M√©tricas de Evaluaci√≥n del Agente

#### üéØ KPIs Principales:
1. **Reward Total**: Suma ponderada de todas las recompensas
2. **Episode Length**: Pasos antes de terminar/morir
3. **Exploration Coverage**: Coordenadas √∫nicas visitadas
4. **Level Progress**: Suma de niveles de Pok√©mon
5. **Event Completion**: Objetivos del juego completados

#### üìà Evoluci√≥n del Entrenamiento:
```
Iteraci√≥n 1-20:   Random exploration, mucho dying
Iteraci√≥n 21-50:  Aprende movimientos b√°sicos
Iteraci√≥n 51-100: Optimiza rutas, evita peligros
Iteraci√≥n 101-150: Estrategias de combate
Iteraci√≥n 151-200: Comportamiento experto
```

### üîß Par√°metros Cr√≠ticos del Entrenamiento

#### ‚öôÔ∏è Hiperpar√°metros Clave:
```python
# En baseline_fast_v2.py
env_config = {
    'reward_scale': 0.5,      # Escala global de recompensas
    'explore_weight': 0.25,   # Peso de exploraci√≥n vs progreso
    'action_freq': 24,        # Frames entre acciones (control de velocidad)
    'max_steps': 163840,      # L√≠mite de pasos por episodio
}
```

#### üéÆ Por Qu√© Estos Valores Funcionan:
- **reward_scale=0.5**: Evita recompensas excesivamente altas que desestabilicen entrenamiento
- **explore_weight=0.25**: Balance 75% progreso / 25% exploraci√≥n
- **action_freq=24**: ~2.5 acciones/segundo (velocidad humana razonable)

### üöÄ Innovaciones T√©cnicas Implementadas

#### üî¨ T√©cnicas Avanzadas Utilizadas:

**1. Exploration Scheduling:**
```python
# Cambia estrategia de exploraci√≥n seg√∫n progreso
if self.get_levels_sum() >= 22:
    self.levels_satisfied = True
    self.base_explore = len(self.seen_coords)
    self.seen_coords = {}  # Reset para nueva fase
```

**2. Multi-Modal Observations:**
- Combina informaci√≥n visual + simb√≥lica
- Permite al agente "entender" el estado del juego m√°s all√° de p√≠xeles

**3. Curriculum Learning Impl√≠cito:**
- Recompensas escaladas seg√∫n progreso
- Diferentes objetivos en diferentes fases del juego

**4. Parallel Environment Training:**
- 64 entornos simult√°neos aumentan diversidad de experiencias
- Acelera convergencia significativamente

### ÔøΩ Script de An√°lisis Autom√°tico

Hemos incluido un script para analizar f√°cilmente las sesiones:

```bash
# Analizar una sesi√≥n espec√≠fica
python analyze_session.py v2/session_752558fa

# Si ya est√°s en el directorio v2/
cd v2
python ../analyze_session.py session_752558fa
```

**El script muestra:**
- Estad√≠sticas b√°sicas (pasos, duraci√≥n, exploraci√≥n)
- Mapas visitados y tiempo en cada uno
- Progreso de niveles y salud
- Gr√°ficos de progreso (guardados como PNG)
- Resumen de recompensas
- Conteo de screenshots disponibles

### üéÆ Consejos para Generar Sesiones Completas

Para que se guarden datos completos:
1. **Deja que el agente complete m√°s pasos**: No termines inmediatamente con Ctrl+C
2. **Configura l√≠mites apropiados**: El agente guarda datos al completar episodios
3. **Usa modo no interactivo**: Para entrenar y generar datos autom√°ticamente

#### 1. Usar Pandas para Analizar Estad√≠sticas:
```python
import pandas as pd
import gzip

# Cargar estad√≠sticas de una sesi√≥n
df = pd.read_csv('session_[ID]/agent_stats_[ID].csv.gz', compression='gzip')

# Ver progreso del agente
print(df[['step', 'x', 'y', 'map', 'levels_sum', 'badge']].head(20))

# Analizar exploraci√≥n
print(f"Ubicaciones √∫nicas visitadas: {df[['x', 'y', 'map']].drop_duplicates().shape[0]}")

# Ver progreso de niveles
print(df['levels_sum'].plot())
```

#### 2. Examinar Screenshots:
Las im√°genes `curframe_*.jpeg` muestran exactamente lo que vio el agente en diferentes momentos.

#### 3. Usar TensorBoard para M√©tricas Detalladas:
```bash
cd v2/runs
tensorboard --logdir .
```
Navegar a `localhost:6006` para ver:
- Curvas de recompensa
- P√©rdidas del modelo
- M√©tricas de exploraci√≥n
- Progreso de entrenamiento

### üéì Oportunidades de Innovaci√≥n para Proyecto Universitario

#### ÔøΩ √Åreas de Investigaci√≥n Sugeridas:

**1. Hybrid Search-RL Approaches:**
```python
# Combinar b√∫squeda cl√°sica con RL
class HybridAgent:
    def __init__(self):
        self.rl_policy = PPO_policy()      # Para exploraci√≥n general
        self.astar_navigator = AStarAgent() # Para navegaci√≥n espec√≠fica
        self.tabu_search = TabuAgent()     # Para evitar loops locales
    
    def select_action(self, state):
        if self.is_navigation_task(state):
            return self.astar_navigator.get_action(state)
        elif self.is_stuck_situation(state):
            return self.tabu_search.get_action(state)
        else:
            return self.rl_policy.get_action(state)
```

**2. Hierarchical Goal Planning:**
```python
# Sistema jer√°rquico de objetivos
class GoalHierarchy:
    def __init__(self):
        self.long_term_goals = ["get_badge_1", "reach_cerulean"]
        self.short_term_goals = ["find_pokecenter", "buy_pokeballs"]
        self.immediate_actions = ["move_up", "interact", "menu"]
    
    def decompose_goal(self, high_level_goal):
        # Descomponer objetivo en sub-tareas alcanzables
        return self.goal_decomposition[high_level_goal]
```

**3. Multi-Agent Cooperation:**
```python
# M√∫ltiples agentes especializados
class SpecializedAgents:
    def __init__(self):
        self.explorer_agent = ExplorerAgent()    # Especialista en exploraci√≥n
        self.battle_agent = BattleAgent()        # Especialista en combate
        self.navigator_agent = NavigatorAgent()  # Especialista en navegaci√≥n
        self.coordinator = CoordinatorAgent()    # Decide qui√©n act√∫a
```

**4. Transfer Learning Entre Juegos:**
```python
# Transferir conocimiento a otros juegos de Pok√©mon
class TransferAgent:
    def __init__(self):
        self.base_policy = load_pokemon_red_policy()
        self.adaptation_layer = AdaptationNetwork()
    
    def adapt_to_new_game(self, new_game_env):
        # Reutilizar conocimiento previo para nuevo entorno
        pass
```

#### üõ†Ô∏è Implementaciones T√©cnicas Sugeridas:

**1. An√°lisis de Decisiones con Explicabilidad:**
```python
def explain_decision(state, action, model):
    """Explica por qu√© el agente tom√≥ cierta decisi√≥n"""
    attention_weights = model.get_attention_weights(state)
    feature_importance = model.get_feature_importance(state)
    
    explanation = {
        "action_taken": action,
        "confidence": model.get_action_probability(state, action),
        "key_features": feature_importance.top_k(5),
        "attention_focus": attention_weights.max_regions(),
        "alternative_actions": model.get_top_k_actions(state, k=3)
    }
    return explanation
```

**2. Dynamic Reward Shaping:**
```python
class AdaptiveRewardSystem:
    def __init__(self):
        self.reward_weights = {"exploration": 0.25, "progress": 0.75}
        self.performance_history = []
    
    def update_reward_weights(self, recent_performance):
        """Ajusta pesos de recompensa seg√∫n rendimiento"""
        if recent_performance["stuck_episodes"] > 5:
            self.reward_weights["exploration"] += 0.1
        if recent_performance["progress_rate"] < 0.1:
            self.reward_weights["progress"] += 0.1
```

**3. Curriculum Learning Expl√≠cito:**
```python
class PokemonCurriculum:
    def __init__(self):
        self.stages = [
            {"name": "basic_movement", "max_steps": 1000, "focus": "navigation"},
            {"name": "pokemon_capture", "max_steps": 5000, "focus": "interaction"},
            {"name": "battle_training", "max_steps": 10000, "focus": "combat"},
            {"name": "gym_challenges", "max_steps": 50000, "focus": "strategy"},
        ]
        self.current_stage = 0
    
    def should_advance_stage(self, agent_performance):
        current = self.stages[self.current_stage]
        return agent_performance.meets_criteria(current["focus"])
```

#### üìä M√©tricas de Evaluaci√≥n Innovadoras:

**1. Eficiencia de Exploraci√≥n:**
```python
def calculate_exploration_efficiency(visited_coords, total_steps):
    """Mide qu√© tan eficientemente explora el agente"""
    unique_locations = len(set(visited_coords))
    return unique_locations / total_steps
```

**2. Consistencia de Pol√≠tica:**
```python
def measure_policy_consistency(actions_history, states_history):
    """Eval√∫a si el agente toma decisiones consistentes en estados similares"""
    similar_state_pairs = find_similar_states(states_history)
    consistency_score = 0
    for state1, state2 in similar_state_pairs:
        action1 = actions_history[state1]
        action2 = actions_history[state2]
        consistency_score += similarity(action1, action2)
    return consistency_score / len(similar_state_pairs)
```

**3. Adaptabilidad a Cambios:**
```python
def test_adaptability(agent, modified_environment):
    """Eval√∫a capacidad de adaptaci√≥n a cambios en el entorno"""
    baseline_performance = agent.evaluate(original_environment)
    modified_performance = agent.evaluate(modified_environment)
    return modified_performance / baseline_performance
```

#### üéØ Proyectos Espec√≠ficos Sugeridos:

**1. "Agente H√≠brido con Planificaci√≥n Jer√°rquica"**
- Combinar PPO con A* para navegaci√≥n
- Implementar planificaci√≥n de objetivos
- Comparar eficiencia vs agente RL puro

**2. "Sistema de Explicabilidad para Decisiones RL"**
- Visualizar por qu√© el agente toma decisiones
- Implementar attention mechanisms
- Crear dashboard de explicaciones en tiempo real

**3. "Curriculum Learning Adaptativo"**
- Sistema que ajusta dificultad autom√°ticamente
- M√©tricas de progreso personalizadas
- Comparaci√≥n con entrenamiento est√°tico

**4. "Multi-Agent Pokemon Ecosystem"**
- M√∫ltiples agentes con roles especializados
- Comunicaci√≥n entre agentes
- Estrategias emergentes de cooperaci√≥n

#### üìù Metodolog√≠a de Investigaci√≥n Sugerida:

**Fase 1: An√°lisis del Estado Actual**
1. Reproducir resultados de V2
2. Analizar limitaciones y puntos de mejora
3. Identificar casos donde falla el agente actual

**Fase 2: Dise√±o de Innovaci√≥n**
1. Seleccionar √°rea de mejora espec√≠fica
2. Dise√±ar soluci√≥n t√©cnica detallada
3. Definir m√©tricas de evaluaci√≥n

**Fase 3: Implementaci√≥n**
1. Desarrollar prototipo de la innovaci√≥n
2. Integrar con sistema existente
3. Realizar pruebas comparativas

**Fase 4: Evaluaci√≥n**
1. Comparar rendimiento vs baseline
2. Analizar trade-offs (tiempo vs precisi√≥n)
3. Documentar casos de uso donde la innovaci√≥n es superior

#### üèÜ Criterios de √âxito del Proyecto:

1. **Mejora Medible**: ‚â•10% mejora en alguna m√©trica clave
2. **Innovaci√≥n T√©cnica**: Implementaci√≥n de t√©cnica no utilizada previamente
3. **Aplicabilidad**: Soluci√≥n generalizable a otros problemas RL
4. **Explicabilidad**: Capacidad de explicar por qu√© la soluci√≥n funciona
5. **Reproducibilidad**: C√≥digo documentado y ejecutable por otrosdo## üö® Soluci√≥n de Problemas

1. **Error de ROM**: Verificar que `PokemonRed.gb` est√© en el directorio correcto con el hash correcto
2. **Problemas de dependencias**: Usar entorno virtual y versi√≥n espec√≠fica de Python
3. **Errores de SDL**: Instalar bibliotecas SDL por separado si es necesario
4. **Rendimiento lento**: Considerar usar la versi√≥n V2 y ajustar `action_freq`
5. **Error PyBoy V2 - APIs obsoletas**: 
   - **Problema 1 - Memoria**: `AttributeError: 'PyBoy' object has no attribute 'memory'`
     ```python
     # En v2/red_gym_env_v2.py, l√≠nea ~461, cambiar:
     return self.pyboy.memory[addr]
     # A:
     return self.pyboy.get_memory_value(addr)
     ```
   - **Problema 2 - Pantalla**: `AttributeError: 'PyBoy' object has no attribute 'screen'`
     ```python
     # En v2/red_gym_env_v2.py, descomentar l√≠nea ~117:
     self.screen = self.pyboy.botsupport_manager().screen()
     # Y en l√≠nea ~171, cambiar:
     game_pixels_render = self.pyboy.screen.ndarray[:,:,0:1]
     # A:
     game_pixels_render = self.screen.screen_ndarray()[:,:,0:1]
     ```
   - **Problema 3 - Tick**: `TypeError: tick() takes exactly 0 positional arguments`
     ```python
     # En v2/red_gym_env_v2.py, m√©todo run_action_on_emulator, cambiar:
     self.pyboy.tick(press_step, render_screen)
     # A loops individuales:
     for i in range(press_step):
         self.pyboy.tick()
     ```
   - **Nota**: Estos errores ya est√°n corregidos en este repositorio
6. **Error "Could not deserialize object tensorboard_log"**: 
   - Es una advertencia, no afecta la ejecuci√≥n
   - Relacionado con compatibilidad de rutas entre Windows y sistemas Unixuego, capturar Pok√©mon, luchar en batallas y completar objetivos usando t√©cnicas de aprendizaje profundo.

## Descripci√≥n General del Proyecto

El proyecto utiliza PyBoy (un emulador de Game Boy) junto con Stable Baselines3 para crear un entorno de gimnasio donde los agentes pueden interactuar con Pok√©mon Red. El agente observa las pantallas del juego y aprende pol√≠ticas √≥ptimas mediante algoritmos como PPO (Proximal Policy Optimization).

## Instalaci√≥n y Configuraci√≥n R√°pida

### Requisitos Previos
- Python 3.10+ (recomendado)
- ffmpeg instalado y disponible en la l√≠nea de comandos
- ROM de Pok√©mon Red legalmente obtenida (1MB, sha1: `ea9bcae617fdf159b045185467ae58b2e4a48b9a`)

### Pasos de Instalaci√≥n

1. **Clonar el repositorio original:**
   ```bash
   git clone https://github.com/PWhiddy/PokemonRedExperiments.git
   cd PokemonRedExperiments
   ```

2. **Crear y activar entorno conda:**
   ```bash
   conda create -n pokeenv python=3.10
   conda activate pokeenv
   ```

3. **Actualizar herramientas de Python:**
   ```bash
   pip install --upgrade pip setuptools wheel
   ```

4. **Colocar la ROM de Pok√©mon Red:**
   - Copiar la ROM legalmente obtenida al directorio base
   - Renombrarla a `PokemonRed.gb`
   - Verificar con: `shasum PokemonRed.gb` (debe coincidir con el hash mencionado arriba)

5. **Instalar dependencias:**
   ```bash
   cd baselines
   pip install -r requirements.txt
   ```
   Para macOS con V2: usar `v2/macos_requirements.txt`

6. **Ejecutar el modelo preentrenado:**
   ```bash
   python run_pretrained_interactive.py
   ```

## Estructura del Proyecto y Descripci√≥n de Archivos

### Directorio Ra√≠z
```
TEL351-PokemonRed/
‚îú‚îÄ‚îÄ PokemonRed.gb              # ROM del juego (debe ser proporcionada por el usuario)
‚îú‚îÄ‚îÄ README.md                  # Este archivo de documentaci√≥n
‚îú‚îÄ‚îÄ README_BASE.md             # Documentaci√≥n original del proyecto
‚îú‚îÄ‚îÄ LICENSE                    # Licencia del proyecto
‚îú‚îÄ‚îÄ windows-setup-guide.md     # Gu√≠a espec√≠fica para Windows
‚îú‚îÄ‚îÄ VisualizeProgress.ipynb    # Notebook para visualizar el progreso del entrenamiento
‚îî‚îÄ‚îÄ *.state                    # Estados guardados del juego
```

#### Estados del Juego (*.state)
- **`init.state`**: Estado inicial b√°sico del juego
- **`has_pokedex.state`**: Estado donde el jugador ya tiene la Pok√©dex
- **`has_pokedex_nballs.state`**: Estado con Pok√©dex y Pok√©balls
- **`fast_text_start.state`**: Estado optimizado para texto r√°pido

### Directorio `baselines/` (Versi√≥n Original)

**Archivos principales de entrenamiento:**
- **`red_gym_env.py`**: **ARCHIVO CLAVE** - Define el entorno de gimnasio principal donde el agente interact√∫a con el juego
- **`run_baseline_parallel.py`**: Script para entrenar m√∫ltiples agentes en paralelo
- **`run_baseline_parallel_fast.py`**: Versi√≥n optimizada del entrenamiento paralelo
- **`run_pretrained_interactive.py`**: **EJECUTAR MODELO** - Script para ejecutar el modelo preentrenado de forma interactiva

**Archivos de configuraci√≥n del agente:**
- **`memory_addresses.py`**: **CONFIGURACI√ìN DEL JUEGO** - Define las direcciones de memoria para acceder a datos del juego (posici√≥n, salud, dinero, etc.)
- **`agent_enabled.txt`**: Archivo de control para pausar/reanudar la IA durante la ejecuci√≥n
- **`global_map.py`**: Manejo del mapa global del juego
- **`map_data.json`**: Datos del mapa en formato JSON

**Archivos de utilidades:**
- **`stream_agent_wrapper.py`**: Wrapper para transmitir sesiones de entrenamiento en vivo
- **`tensorboard_callback.py`**: Callbacks para logging con TensorBoard
- **`tile_vids_to_grid.py`**: Utilidad para crear videos en cuadr√≠cula
- **`render_all_needed_grids.py`**: Renderizado de cuadr√≠culas necesarias
- **`delete_empty_imgs.txt`**: Script para limpiar im√°genes vac√≠as

**Archivos de datos:**
- **`requirements.txt`**: Dependencias de Python requeridas
- **`events.json`**: Eventos del juego en formato JSON
- **`saves_to_record.txt`**: Lista de estados guardados para grabar

### Directorio `v2/` (Versi√≥n Mejorada - Recomendada)

**Mejoras de la V2:**
- Entrenamiento m√°s r√°pido y eficiente en memoria
- Alcanza Cerulean City
- Streaming al mapa habilitado por defecto
- Recompensa de exploraci√≥n basada en coordenadas en lugar de KNN de frames

**Archivos principales:**
- **`red_gym_env_v2.py`**: **ENTORNO MEJORADO** - Versi√≥n optimizada del entorno de gimnasio
- **`baseline_fast_v2.py`**: **ENTRENAMIENTO V2** - Script principal de entrenamiento de la versi√≥n 2
- **`run_pretrained_interactive.py`**: Ejecutor del modelo preentrenado para V2
- **`requirements.txt`** / **`macos_requirements.txt`**: Dependencias espec√≠ficas para cada SO

### Directorio `visualization/`

**Notebooks y scripts de visualizaci√≥n:**
- **`Agent_Visualization.ipynb`**: Visualizaci√≥n del comportamiento del agente
- **`BetterMapVis.ipynb`**: Visualizaci√≥n mejorada del mapa
- **`BetterMapVis_script_version.py`**: Versi√≥n en script de la visualizaci√≥n del mapa
- **`BetterMapVis_script_version_FLOW.py`**: Visualizaci√≥n con flujo de movimiento
- **`Create_Video_Grids.ipynb`**: Creaci√≥n de videos en cuadr√≠cula
- **`Map_Stitching.ipynb`**: Uni√≥n de mapas
- **`MapWalkingVis.ipynb`**: Visualizaci√≥n de caminatas en el mapa

### Directorio `clip_experiment/`

Experimentos con CLIP (Contrastive Language-Image Pre-training):
- **`Interacting_with_CLIP_Pokemon.ipynb`**: Notebook para experimentos con CLIP
- **`location_descriptions/`**: Im√°genes con descripciones de ubicaciones
- **`test_images/`**: Im√°genes de prueba para CLIP

### Directorio `assets/`

Recursos gr√°ficos y multimedia:
- **`grid.png`**: Imagen de cuadr√≠cula para visualizaci√≥n
- **`poke_map.gif`**: GIF animado del mapa de Pok√©mon
- **`youtube.jpg`**: Miniatura del video de YouTube
- **`sblogo.png`**: Logo de Stable Baselines
- **`pyboy.svg`**: Logo de PyBoy

## Archivos Clave para Comportamiento del Agente

### 1. **Pol√≠ticas y Acciones del Agente**

**`red_gym_env.py`** (l√≠neas clave):
- **Espacio de acciones**: Define qu√© botones puede presionar el agente
- **Funci√≥n de recompensa**: Determina c√≥mo se eval√∫a el comportamiento
- **Observaciones**: Qu√© informaci√≥n recibe el agente del juego
- **Exploraci√≥n**: Sistema de recompensas por explorar nuevas √°reas

**`memory_addresses.py`**:
- **Posici√≥n del jugador**: `X_POS_ADDRESS`, `Y_POS_ADDRESS`
- **Informaci√≥n del party**: `PARTY_SIZE_ADDRESS`, `LEVELS_ADDRESSES`
- **Estado del juego**: `BADGE_COUNT_ADDRESS`, `MONEY_ADDRESS_*`
- **Eventos**: `EVENT_FLAGS_START_ADDRESS`, `EVENT_FLAGS_END_ADDRESS`

### 2. **Configuraci√≥n de Estados Iniciales**

**Estados disponibles** (archivos `.state`):
- Modifica `init_state` en la configuraci√≥n del entorno para cambiar d√≥nde inicia el agente
- Cada estado representa un punto diferente en el progreso del juego

**En `run_pretrained_interactive.py`**:
```python
env_config = {
    'init_state': '../has_pokedex_nballs.state',  # CAMBIAR AQU√ç el estado inicial
    'action_freq': 24,                            # Frecuencia de acciones
    'headless': False,                            # Mostrar ventana del juego
    'max_steps': ep_length,                       # M√°ximo de pasos por episodio
    # ... m√°s configuraciones
}
```

### 3. **Par√°metros Editables del Comportamiento**

**En `red_gym_env.py`** (configuraciones importantes):
- **`explore_weight`**: Peso de la recompensa por exploraci√≥n
- **`reward_scale`**: Escala general de recompensas
- **`action_freq`**: Frecuencia de ejecuci√≥n de acciones
- **`similar_frame_dist`**: Distancia para considerar frames similares
- **`use_screen_explore`**: Usar exploraci√≥n basada en pantalla

**En `memory_addresses.py`**:
- Puedes agregar nuevas direcciones de memoria para acceder a m√°s datos del juego
- √ötil para crear nuevas recompensas o condiciones

## Entrenamiento del Modelo

### Versi√≥n V2 (Recomendada)
```bash
cd v2
python baseline_fast_v2.py
```

### Versi√≥n Original
```bash
cd baselines
python run_baseline_parallel_fast.py
```

## Monitoreo del Progreso

### TensorBoard (Local)
```bash
cd [directorio_de_sesi√≥n]
tensorboard --logdir .
```
Luego navegar a `localhost:6006`

### Transmisi√≥n en Vivo
El proyecto incluye capacidad para transmitir sesiones de entrenamiento a un mapa global compartido usando `stream_agent_wrapper.py`.

### Visualizaci√≥n Est√°tica
Usar los notebooks en el directorio `visualization/` para an√°lisis detallado del comportamiento del agente.

## Uso Interactivo

Una vez ejecutando `run_pretrained_interactive.py`:
- **Teclas de flecha**: Movimiento
- **A y S**: Botones A y B del Game Boy
- **Pausar IA**: Editar `agent_enabled.txt` (cambiar a `False`)

## Personalizaci√≥n Avanzada

### Modificar Recompensas
Editar la funci√≥n `_calculate_reward()` en `red_gym_env.py` para cambiar c√≥mo el agente es recompensado.

### Cambiar Estado Inicial
Modificar `init_state` en la configuraci√≥n del entorno para comenzar desde diferentes puntos del juego.

### Agregar Nuevas Observaciones
A√±adir direcciones de memoria en `memory_addresses.py` y modificar `_get_obs()` en el entorno.

### Configurar Exploraci√≥n
Ajustar `explore_weight` y `use_screen_explore` para cambiar el comportamiento exploratorio.

## Notas Importantes

- El archivo `PokemonRed.gb` DEBE estar en el directorio principal
- El directorio actual DEBE ser `baselines/` o `v2/` al ejecutar scripts
- Python 3.10+ es altamente recomendado para compatibilidad
- Para GPUs AMD, seguir la gu√≠a de instalaci√≥n de PyTorch con ROCm

## Recursos Adicionales

- [Video explicativo en YouTube](https://youtu.be/DcYLT37ImBY)
- [Servidor Discord del proyecto](http://discord.gg/RvadteZk4G)
- [Visualizaci√≥n en vivo del mapa](https://pwhiddy.github.io/pokerl-map-viz/)
- [Repositorio original](https://github.com/PWhiddy/PokemonRedExperiments)

## Soluci√≥n de Problemas

1. **Error de ROM**: Verificar que `PokemonRed.gb` est√© en el directorio correcto con el hash correcto
2. **Problemas de dependencias**: Usar entorno virtual y versi√≥n espec√≠fica de Python
3. **Errores de SDL**: Instalar bibliotecas SDL por separado si es necesario
4. **Rendimiento lento**: Considerar usar la versi√≥n V2 y ajustar `action_freq`

---

*Este proyecto es una implementaci√≥n educativa de aprendizaje por refuerzo aplicado a videojuegos retro. Requiere una copia legal de Pok√©mon Red.*
