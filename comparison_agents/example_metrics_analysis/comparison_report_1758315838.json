{
  "summary_statistics": {
    "mean_reward": {
      "count": 2.0,
      "mean": 26.3,
      "std": 1.4142135623730951,
      "min": 25.3,
      "25%": 25.8,
      "50%": 26.3,
      "75%": 26.8,
      "max": 27.3
    },
    "median_reward": {
      "count": 2.0,
      "mean": 28.25,
      "std": 1.0606601717798212,
      "min": 27.5,
      "25%": 27.875,
      "50%": 28.25,
      "75%": 28.625,
      "max": 29.0
    },
    "std_reward": {
      "count": 2.0,
      "mean": 9.517443891759147,
      "std": 0.23774678637782093,
      "min": 9.34933152690608,
      "25%": 9.433387709332614,
      "50%": 9.517443891759147,
      "75%": 9.601500074185681,
      "max": 9.685556256612214
    },
    "max_reward": {
      "count": 2.0,
      "mean": 39.5,
      "std": 0.7071067811865476,
      "min": 39.0,
      "25%": 39.25,
      "50%": 39.5,
      "75%": 39.75,
      "max": 40.0
    },
    "min_reward": {
      "count": 2.0,
      "mean": 9.0,
      "std": 1.4142135623730951,
      "min": 8.0,
      "25%": 8.5,
      "50%": 9.0,
      "75%": 9.5,
      "max": 10.0
    },
    "reward_range": {
      "count": 2.0,
      "mean": 30.5,
      "std": 0.7071067811865476,
      "min": 30.0,
      "25%": 30.25,
      "50%": 30.5,
      "75%": 30.75,
      "max": 31.0
    },
    "mean_episode_length": {
      "count": 2.0,
      "mean": 847.5,
      "std": 28.991378028648448,
      "min": 827.0,
      "25%": 837.25,
      "50%": 847.5,
      "75%": 857.75,
      "max": 868.0
    },
    "median_episode_length": {
      "count": 2.0,
      "mean": 825.0,
      "std": 21.213203435596427,
      "min": 810.0,
      "25%": 817.5,
      "50%": 825.0,
      "75%": 832.5,
      "max": 840.0
    },
    "std_episode_length": {
      "count": 2.0,
      "mean": 103.52693590084397,
      "std": 14.189682377174234,
      "min": 93.49331526906082,
      "25%": 98.51012558495239,
      "50%": 103.52693590084397,
      "75%": 108.54374621673554,
      "max": 113.56055653262712
    },
    "convergence_episode": {
      "count": 2.0,
      "mean": 10.0,
      "std": 0.0,
      "min": 10.0,
      "25%": 10.0,
      "50%": 10.0,
      "75%": 10.0,
      "max": 10.0
    },
    "learning_rate": {
      "count": 2.0,
      "mean": 3.2484848484848485,
      "std": 0.09428090415820663,
      "min": 3.1818181818181817,
      "25%": 3.215151515151515,
      "50%": 3.2484848484848485,
      "75%": 3.281818181818182,
      "max": 3.3151515151515154
    },
    "improvement_rate": {
      "count": 2.0,
      "mean": 0.8359302325581396,
      "std": 0.14980797154905687,
      "min": 0.7300000000000001,
      "25%": 0.7829651162790698,
      "50%": 0.8359302325581396,
      "75%": 0.8888953488372093,
      "max": 0.9418604651162791
    },
    "plateau_episodes": {
      "count": 2.0,
      "mean": 0.0,
      "std": 0.0,
      "min": 0.0,
      "25%": 0.0,
      "50%": 0.0,
      "75%": 0.0,
      "max": 0.0
    },
    "steps_per_second": {
      "count": 2.0,
      "mean": 16.424062049062048,
      "std": 0.021682567244175423,
      "min": 16.408730158730158,
      "25%": 16.416396103896105,
      "50%": 16.424062049062048,
      "75%": 16.43172799422799,
      "max": 16.439393939393938
    },
    "reward_per_step": {
      "count": 2.0,
      "mean": 0.031079174073186635,
      "std": 0.0027318485507339474,
      "min": 0.02914746543778802,
      "25%": 0.030113319755487325,
      "50%": 0.031079174073186635,
      "75%": 0.032045028390885945,
      "max": 0.03301088270858525
    },
    "reward_per_second": {
      "count": 2.0,
      "mean": 0.5104166666666666,
      "std": 0.04419417382415918,
      "min": 0.4791666666666667,
      "25%": 0.4947916666666667,
      "50%": 0.5104166666666666,
      "75%": 0.5260416666666666,
      "max": 0.5416666666666666
    },
    "exploration_efficiency": {
      "count": 2.0,
      "mean": 0.0035994990499222663,
      "std": 3.969765138808579e-05,
      "min": 0.0035714285714285713,
      "25%": 0.003585463810675419,
      "50%": 0.0035994990499222663,
      "75%": 0.003613534289169114,
      "max": 0.0036275695284159614
    },
    "performance_stability": {
      "count": 2.0,
      "mean": 0.3626473334174791,
      "std": 0.028540211545827303,
      "min": 0.342466356296926,
      "25%": 0.35255684485720257,
      "50%": 0.3626473334174791,
      "75%": 0.3727378219777556,
      "max": 0.3828283105380322
    },
    "consistency_score": {
      "count": 2.0,
      "mean": 0.6879591586870355,
      "std": 0.0005606751302511983,
      "min": 0.6875627015003922,
      "25%": 0.6877609300937139,
      "50%": 0.6879591586870355,
      "75%": 0.6881573872803571,
      "max": 0.6883556158736788
    },
    "volatility": {
      "count": 2.0,
      "mean": 0.16527215174873777,
      "std": 0.009677928916638523,
      "min": 0.1584288225839413,
      "25%": 0.16185048716633954,
      "50%": 0.16527215174873777,
      "75%": 0.16869381633113603,
      "max": 0.17211548091353426
    },
    "badges_collected": {
      "count": 2.0,
      "mean": 0.0,
      "std": 0.0,
      "min": 0.0,
      "25%": 0.0,
      "50%": 0.0,
      "75%": 0.0,
      "max": 0.0
    },
    "events_completed": {
      "count": 2.0,
      "mean": 0.0,
      "std": 0.0,
      "min": 0.0,
      "25%": 0.0,
      "50%": 0.0,
      "75%": 0.0,
      "max": 0.0
    },
    "areas_explored": {
      "count": 2.0,
      "mean": 0.0,
      "std": 0.0,
      "min": 0.0,
      "25%": 0.0,
      "50%": 0.0,
      "75%": 0.0,
      "max": 0.0
    },
    "pokemon_caught": {
      "count": 2.0,
      "mean": 0.0,
      "std": 0.0,
      "min": 0.0,
      "25%": 0.0,
      "50%": 0.0,
      "75%": 0.0,
      "max": 0.0
    },
    "pareto_efficiency": {
      "count": 2.0,
      "mean": 0.5804810412259891,
      "std": 0.005394340359708312,
      "min": 0.5766666665776111,
      "25%": 0.5785738539018002,
      "50%": 0.5804810412259891,
      "75%": 0.5823882285501781,
      "max": 0.5842954158743672
    },
    "risk_adjusted_return": {
      "count": 2.0,
      "mean": 2.766065936887167,
      "std": 0.2176883702536046,
      "min": 2.6121370140953952,
      "25%": 2.689101475491281,
      "50%": 2.766065936887167,
      "75%": 2.843030398283053,
      "max": 2.9199948596789387
    },
    "sharpe_ratio": {
      "count": 2.0,
      "mean": 2.766065936887167,
      "std": 0.2176883702536046,
      "min": 2.6121370140953952,
      "25%": 2.689101475491281,
      "50%": 2.766065936887167,
      "75%": 2.843030398283053,
      "max": 2.9199948596789387
    }
  },
  "statistical_tests": {
    "mean_reward": {
      "PPO_vs_Epsilon_Greedy": {
        "significant": "False",
        "p_value": 0.813952686045242,
        "effect_size": 0.8952879140180268
      }
    },
    "exploration_efficiency": {
      "PPO_vs_Epsilon_Greedy": {
        "significant": "False",
        "p_value": 0.8806335770479291,
        "effect_size": 0.7628278180498682
      }
    },
    "performance_stability": {
      "PPO_vs_Epsilon_Greedy": {
        "significant": "False",
        "p_value": 0.7546778394493412,
        "effect_size": -0.20050643384913602
      }
    }
  },
  "rankings": {
    "mean_reward": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "max_reward": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "learning_rate": {
      "Epsilon_Greedy": 1,
      "PPO": 2
    },
    "improvement_rate": {
      "Epsilon_Greedy": 1,
      "PPO": 2
    },
    "steps_per_second": {
      "Epsilon_Greedy": 1,
      "PPO": 2
    },
    "reward_per_step": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "consistency_score": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "exploration_efficiency": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "pareto_efficiency": {
      "Epsilon_Greedy": 1,
      "PPO": 2
    },
    "risk_adjusted_return": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "sharpe_ratio": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "std_reward": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "convergence_episode": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "plateau_episodes": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "performance_stability": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "volatility": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    },
    "overall": {
      "PPO": 1,
      "Epsilon_Greedy": 2
    }
  },
  "correlations": {
    "mean_reward_vs_median_reward": 1.0,
    "mean_reward_vs_std_reward": -1.0000000000000027,
    "mean_reward_vs_max_reward": 1.0,
    "mean_reward_vs_min_reward": 1.0,
    "mean_reward_vs_reward_range": -1.0,
    "mean_reward_vs_mean_episode_length": -1.0,
    "mean_reward_vs_median_episode_length": -1.0,
    "mean_reward_vs_std_episode_length": -1.0,
    "mean_reward_vs_learning_rate": -1.0,
    "mean_reward_vs_improvement_rate": -1.0,
    "mean_reward_vs_steps_per_second": -1.0,
    "mean_reward_vs_reward_per_step": 1.0,
    "mean_reward_vs_reward_per_second": 0.9999999999999996,
    "mean_reward_vs_exploration_efficiency": 1.0,
    "mean_reward_vs_performance_stability": -0.9999999999999993,
    "mean_reward_vs_consistency_score": 1.0,
    "mean_reward_vs_volatility": -1.000000000000001,
    "mean_reward_vs_pareto_efficiency": -1.0000000000000073,
    "mean_reward_vs_risk_adjusted_return": 1.0000000000000007,
    "mean_reward_vs_sharpe_ratio": 1.0000000000000007,
    "median_reward_vs_std_reward": -1.0000000000000027,
    "median_reward_vs_max_reward": 1.0,
    "median_reward_vs_min_reward": 1.0,
    "median_reward_vs_reward_range": -1.0,
    "median_reward_vs_mean_episode_length": -1.0,
    "median_reward_vs_median_episode_length": -1.0,
    "median_reward_vs_std_episode_length": -1.0,
    "median_reward_vs_learning_rate": -1.0,
    "median_reward_vs_improvement_rate": -1.0,
    "median_reward_vs_steps_per_second": -1.0,
    "median_reward_vs_reward_per_step": 1.0,
    "median_reward_vs_reward_per_second": 0.9999999999999996,
    "median_reward_vs_exploration_efficiency": 1.0,
    "median_reward_vs_performance_stability": -0.9999999999999993,
    "median_reward_vs_consistency_score": 1.0,
    "median_reward_vs_volatility": -1.000000000000001,
    "median_reward_vs_pareto_efficiency": -1.0000000000000073,
    "median_reward_vs_risk_adjusted_return": 1.0000000000000007,
    "median_reward_vs_sharpe_ratio": 1.0000000000000007,
    "std_reward_vs_max_reward": -0.9999999999999973,
    "std_reward_vs_min_reward": -0.9999999999999973,
    "std_reward_vs_reward_range": 0.9999999999999973,
    "std_reward_vs_mean_episode_length": 0.9999999999999974,
    "std_reward_vs_median_episode_length": 0.9999999999999973,
    "std_reward_vs_std_episode_length": 0.9999999999999973,
    "std_reward_vs_learning_rate": 0.9999999999999976,
    "std_reward_vs_improvement_rate": 0.9999999999999974,
    "std_reward_vs_steps_per_second": 0.9999999999999974,
    "std_reward_vs_reward_per_step": -0.9999999999999973,
    "std_reward_vs_reward_per_second": -0.9999999999999969,
    "std_reward_vs_exploration_efficiency": -0.9999999999999973,
    "std_reward_vs_performance_stability": 0.9999999999999967,
    "std_reward_vs_consistency_score": -0.9999999999999973,
    "std_reward_vs_volatility": 0.9999999999999984,
    "std_reward_vs_pareto_efficiency": 1.0000000000000047,
    "std_reward_vs_risk_adjusted_return": -0.9999999999999981,
    "std_reward_vs_sharpe_ratio": -0.9999999999999981,
    "max_reward_vs_min_reward": 1.0,
    "max_reward_vs_reward_range": -1.0,
    "max_reward_vs_mean_episode_length": -1.0,
    "max_reward_vs_median_episode_length": -1.0,
    "max_reward_vs_std_episode_length": -1.0,
    "max_reward_vs_learning_rate": -1.0,
    "max_reward_vs_improvement_rate": -1.0,
    "max_reward_vs_steps_per_second": -1.0,
    "max_reward_vs_reward_per_step": 1.0,
    "max_reward_vs_reward_per_second": 0.9999999999999996,
    "max_reward_vs_exploration_efficiency": 1.0,
    "max_reward_vs_performance_stability": -0.9999999999999993,
    "max_reward_vs_consistency_score": 1.0,
    "max_reward_vs_volatility": -1.000000000000001,
    "max_reward_vs_pareto_efficiency": -1.0000000000000073,
    "max_reward_vs_risk_adjusted_return": 1.0000000000000007,
    "max_reward_vs_sharpe_ratio": 1.0000000000000007,
    "min_reward_vs_reward_range": -1.0,
    "min_reward_vs_mean_episode_length": -1.0,
    "min_reward_vs_median_episode_length": -1.0,
    "min_reward_vs_std_episode_length": -1.0,
    "min_reward_vs_learning_rate": -1.0,
    "min_reward_vs_improvement_rate": -1.0,
    "min_reward_vs_steps_per_second": -1.0,
    "min_reward_vs_reward_per_step": 1.0,
    "min_reward_vs_reward_per_second": 0.9999999999999996,
    "min_reward_vs_exploration_efficiency": 1.0,
    "min_reward_vs_performance_stability": -0.9999999999999993,
    "min_reward_vs_consistency_score": 1.0,
    "min_reward_vs_volatility": -1.000000000000001,
    "min_reward_vs_pareto_efficiency": -1.0000000000000073,
    "min_reward_vs_risk_adjusted_return": 1.0000000000000007,
    "min_reward_vs_sharpe_ratio": 1.0000000000000007,
    "reward_range_vs_mean_episode_length": 1.0,
    "reward_range_vs_median_episode_length": 1.0,
    "reward_range_vs_std_episode_length": 1.0,
    "reward_range_vs_learning_rate": 1.0,
    "reward_range_vs_improvement_rate": 1.0,
    "reward_range_vs_steps_per_second": 1.0,
    "reward_range_vs_reward_per_step": -1.0,
    "reward_range_vs_reward_per_second": -0.9999999999999996,
    "reward_range_vs_exploration_efficiency": -1.0,
    "reward_range_vs_performance_stability": 0.9999999999999993,
    "reward_range_vs_consistency_score": -1.0,
    "reward_range_vs_volatility": 1.000000000000001,
    "reward_range_vs_pareto_efficiency": 1.0000000000000073,
    "reward_range_vs_risk_adjusted_return": -1.0000000000000007,
    "reward_range_vs_sharpe_ratio": -1.0000000000000007,
    "mean_episode_length_vs_median_episode_length": 1.0,
    "mean_episode_length_vs_std_episode_length": 1.0,
    "mean_episode_length_vs_learning_rate": 1.0,
    "mean_episode_length_vs_improvement_rate": 1.0,
    "mean_episode_length_vs_steps_per_second": 1.0,
    "mean_episode_length_vs_reward_per_step": -1.0,
    "mean_episode_length_vs_reward_per_second": -0.9999999999999997,
    "mean_episode_length_vs_exploration_efficiency": -1.0,
    "mean_episode_length_vs_performance_stability": 0.9999999999999993,
    "mean_episode_length_vs_consistency_score": -1.0,
    "mean_episode_length_vs_volatility": 1.0000000000000009,
    "mean_episode_length_vs_pareto_efficiency": 1.0000000000000073,
    "mean_episode_length_vs_risk_adjusted_return": -1.0000000000000007,
    "mean_episode_length_vs_sharpe_ratio": -1.0000000000000007,
    "median_episode_length_vs_std_episode_length": 1.0,
    "median_episode_length_vs_learning_rate": 1.0,
    "median_episode_length_vs_improvement_rate": 1.0,
    "median_episode_length_vs_steps_per_second": 1.0,
    "median_episode_length_vs_reward_per_step": -1.0,
    "median_episode_length_vs_reward_per_second": -0.9999999999999996,
    "median_episode_length_vs_exploration_efficiency": -1.0,
    "median_episode_length_vs_performance_stability": 0.9999999999999992,
    "median_episode_length_vs_consistency_score": -1.0,
    "median_episode_length_vs_volatility": 1.000000000000001,
    "median_episode_length_vs_pareto_efficiency": 1.0000000000000073,
    "median_episode_length_vs_risk_adjusted_return": -1.0000000000000007,
    "median_episode_length_vs_sharpe_ratio": -1.0000000000000007,
    "std_episode_length_vs_learning_rate": 1.0,
    "std_episode_length_vs_improvement_rate": 1.0,
    "std_episode_length_vs_steps_per_second": 1.0,
    "std_episode_length_vs_reward_per_step": -1.0000000000000002,
    "std_episode_length_vs_reward_per_second": -0.9999999999999997,
    "std_episode_length_vs_exploration_efficiency": -1.0,
    "std_episode_length_vs_performance_stability": 0.9999999999999993,
    "std_episode_length_vs_consistency_score": -1.0,
    "std_episode_length_vs_volatility": 1.000000000000001,
    "std_episode_length_vs_pareto_efficiency": 1.0000000000000073,
    "std_episode_length_vs_risk_adjusted_return": -1.0000000000000007,
    "std_episode_length_vs_sharpe_ratio": -1.0000000000000007,
    "learning_rate_vs_improvement_rate": 1.0,
    "learning_rate_vs_steps_per_second": 1.0,
    "learning_rate_vs_reward_per_step": -1.0,
    "learning_rate_vs_reward_per_second": -0.9999999999999993,
    "learning_rate_vs_exploration_efficiency": -0.9999999999999999,
    "learning_rate_vs_performance_stability": 0.9999999999999993,
    "learning_rate_vs_consistency_score": -1.0,
    "learning_rate_vs_volatility": 1.000000000000001,
    "learning_rate_vs_pareto_efficiency": 1.0000000000000073,
    "learning_rate_vs_risk_adjusted_return": -1.0000000000000009,
    "learning_rate_vs_sharpe_ratio": -1.0000000000000009,
    "improvement_rate_vs_steps_per_second": 1.0,
    "improvement_rate_vs_reward_per_step": -1.0,
    "improvement_rate_vs_reward_per_second": -0.9999999999999994,
    "improvement_rate_vs_exploration_efficiency": -1.0,
    "improvement_rate_vs_performance_stability": 0.9999999999999994,
    "improvement_rate_vs_consistency_score": -1.0,
    "improvement_rate_vs_volatility": 1.0000000000000009,
    "improvement_rate_vs_pareto_efficiency": 1.0000000000000073,
    "improvement_rate_vs_risk_adjusted_return": -1.0000000000000007,
    "improvement_rate_vs_sharpe_ratio": -1.0000000000000007,
    "steps_per_second_vs_reward_per_step": -1.0,
    "steps_per_second_vs_reward_per_second": -0.9999999999999996,
    "steps_per_second_vs_exploration_efficiency": -1.0,
    "steps_per_second_vs_performance_stability": 0.9999999999999993,
    "steps_per_second_vs_consistency_score": -1.0,
    "steps_per_second_vs_volatility": 1.000000000000001,
    "steps_per_second_vs_pareto_efficiency": 1.0000000000000073,
    "steps_per_second_vs_risk_adjusted_return": -1.0000000000000007,
    "steps_per_second_vs_sharpe_ratio": -1.0000000000000007,
    "reward_per_step_vs_reward_per_second": 0.9999999999999996,
    "reward_per_step_vs_exploration_efficiency": 1.0,
    "reward_per_step_vs_performance_stability": -0.9999999999999993,
    "reward_per_step_vs_consistency_score": 1.0,
    "reward_per_step_vs_volatility": -1.000000000000001,
    "reward_per_step_vs_pareto_efficiency": -1.0000000000000073,
    "reward_per_step_vs_risk_adjusted_return": 1.0000000000000007,
    "reward_per_step_vs_sharpe_ratio": 1.0000000000000007,
    "reward_per_second_vs_exploration_efficiency": 1.0000000000000004,
    "reward_per_second_vs_performance_stability": -0.9999999999999998,
    "reward_per_second_vs_consistency_score": 1.0000000000000004,
    "reward_per_second_vs_volatility": -1.0000000000000016,
    "reward_per_second_vs_pareto_efficiency": -1.0000000000000078,
    "reward_per_second_vs_risk_adjusted_return": 1.000000000000001,
    "reward_per_second_vs_sharpe_ratio": 1.000000000000001,
    "exploration_efficiency_vs_performance_stability": -0.9999999999999992,
    "exploration_efficiency_vs_consistency_score": 1.0,
    "exploration_efficiency_vs_volatility": -1.000000000000001,
    "exploration_efficiency_vs_pareto_efficiency": -1.000000000000007,
    "exploration_efficiency_vs_risk_adjusted_return": 1.0000000000000009,
    "exploration_efficiency_vs_sharpe_ratio": 1.0000000000000009,
    "performance_stability_vs_consistency_score": -1.0000000000000007,
    "performance_stability_vs_volatility": 1.0000000000000016,
    "performance_stability_vs_pareto_efficiency": 1.000000000000008,
    "performance_stability_vs_risk_adjusted_return": -1.0000000000000013,
    "performance_stability_vs_sharpe_ratio": -1.0000000000000013,
    "consistency_score_vs_volatility": -1.000000000000001,
    "consistency_score_vs_pareto_efficiency": -1.0000000000000073,
    "consistency_score_vs_risk_adjusted_return": 1.0000000000000007,
    "consistency_score_vs_sharpe_ratio": 1.0000000000000007,
    "volatility_vs_pareto_efficiency": 1.0000000000000064,
    "volatility_vs_risk_adjusted_return": -0.9999999999999998,
    "volatility_vs_sharpe_ratio": -0.9999999999999998,
    "pareto_efficiency_vs_risk_adjusted_return": -0.9999999999999936,
    "pareto_efficiency_vs_sharpe_ratio": -0.9999999999999936,
    "risk_adjusted_return_vs_sharpe_ratio": 1.0
  },
  "recommendations": [
    "Overall best performing agent: PPO",
    "Highest reward agent: PPO",
    "Most stable agent: PPO",
    "Best exploration agent: PPO"
  ]
}