"""
Interactive Tabu Search Agent for Pokemon Red (Game Boy Interface)
==================================================================

This script launches the Tabu Search agent in the v2 environment with a real-time Game Boy window.
Uses the same metrics system as Epsilon Greedy and PPO agents.
"""

import time
import numpy as np
import os
import psutil
import json
import csv
import sys
from pathlib import Path
from datetime import datetime
import random
import signal

# Import v2 environment components
sys.path.append('../v2')
from red_gym_env_v2 import RedGymEnv
from stream_agent_wrapper import StreamWrapper

sys.path.append('../gym_scenarios')
from gym_memory_addresses import (
    BADGE_COUNT_ADDRESS,
    BAG_ITEM_COUNT,
    BAG_ITEMS_START,
    HP_ADDRESSES,
)

# Import the Tabu Search agent
from search_algorithms.tabu_agent import TabuSearchAgent, GameScenario

class InterruptHandler:
    """Handle Ctrl+C gracefully"""
    def __init__(self):
        self.interrupted = False
        signal.signal(signal.SIGINT, self.handle_interrupt)
    
    def handle_interrupt(self, signum, frame):
        print("\nüõë Interrupci√≥n detectada. Guardando m√©tricas...")
        self.interrupted = True


def unwrap_core_env(env):
    target = getattr(env, 'env', env)
    visited = set()
    while hasattr(target, 'env') and id(target) not in visited:
        visited.add(id(target))
        target = target.env
    return target


def read_badges_from_env(env) -> int:
    try:
        return int(env.read_m(BADGE_COUNT_ADDRESS))
    except Exception:
        return 0


def read_bag_inventory(env) -> dict:
    inventory = {}
    try:
        count = int(env.read_m(BAG_ITEM_COUNT))
        cursor = BAG_ITEMS_START
        for _ in range(count):
            item_id = int(env.read_m(cursor))
            qty = int(env.read_m(cursor + 1))
            if item_id == 0:
                break
            inventory[item_id] = qty
            cursor += 2
    except Exception:
        pass
    return inventory


def read_team_hp_values(env) -> list:
    team_hp = []
    try:
        for addr in HP_ADDRESSES:
            hp_value = int(env.read_hp(addr))
            if hp_value > 0:
                team_hp.append(hp_value)
    except Exception:
        pass
    return team_hp


def summarize_item_usage(start_inv: dict, end_inv: dict) -> dict:
    usage = {}
    for item_id, start_qty in start_inv.items():
        diff = start_qty - end_inv.get(item_id, 0)
        if diff > 0:
            usage[item_id] = diff
    return usage

def extract_game_state(observation) -> dict:
    """Extract game state from v2 environment observation"""
    try:
        # Convert observation to dict format that Tabu Search agent expects
        if hasattr(observation, 'keys'):
            # If observation is already a dict
            obs_dict = observation
        else:
            # If observation is an array or other format, create a dict
            obs_dict = {
                'screen': observation if isinstance(observation, np.ndarray) else np.zeros((144, 160, 3)),
            }
        
        # Extract relevant game information (using same pattern as v2_agent.py)
        game_state = {
            'hp': int(obs_dict.get('health', np.array([100]))[0]) if 'health' in obs_dict else 100,
            'max_hp': 100,  # Default max HP
            'level': int(np.sum(obs_dict.get('level', np.zeros(8)))),
            'badges': int(np.sum(obs_dict.get('badges', np.zeros(8)))),
            'pcount': int(np.sum(obs_dict.get('pcount', np.zeros(8)))),
            'events': int(np.sum(obs_dict.get('events', np.zeros(100)))),
            'x': 0,  # Position will be extracted differently
            'y': 0,
            'battle': False,  # Will be detected from screen if possible
        }
        
        return game_state
        
    except Exception as e:
        # Return default state if extraction fails
        return {
            'hp': 100, 'max_hp': 100, 'level': 1, 'badges': 0, 
            'pcount': 0, 'events': 0, 'x': 0, 'y': 0, 'battle': False
        }

def save_metrics(agent, env, step, episode_reward, start_time, process, action_history, 
                reward_history, scenario_detections, position_history, detailed_stats, reason=""):
    """Funci√≥n COMPLETA para guardar m√©tricas de Tabu Search"""
    elapsed = time.time() - start_time
    mem_info = process.memory_info()
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    timestamp = int(time.time())
    
    # Calcular estad√≠sticas avanzadas
    avg_reward_per_step = episode_reward / max(step, 1)
    steps_per_second = step / max(elapsed, 1)
    avg_memory = sum(detailed_stats["memory_usage_history"]) / max(len(detailed_stats["memory_usage_history"]), 1)
    avg_cpu = sum(detailed_stats["cpu_usage_history"]) / max(len(detailed_stats["cpu_usage_history"]), 1)
    
    # Obtener m√©tricas espec√≠ficas del agente Tabu Search
    exploration_metrics = agent.get_exploration_metrics()
    
    core_env = unwrap_core_env(env)
    badge_start = detailed_stats.get("badge_start", read_badges_from_env(core_env))
    badge_end = detailed_stats.get("badge_end", read_badges_from_env(core_env))
    bag_start = detailed_stats.get("bag_start", read_bag_inventory(core_env))
    bag_end = detailed_stats.get("bag_end", read_bag_inventory(core_env))
    items_used_detail = summarize_item_usage(bag_start, bag_end)
    team_hp_end = detailed_stats.get("team_hp_end", read_team_hp_values(core_env))
    pokemon_lost = sum(1 for hp in team_hp_end if hp <= 0)

    game_state = {
        "hp": team_hp_end,
        "badges": badge_end,
        "items_used": items_used_detail,
    }
    
    # === 1. DATOS CRUDOS (JSON) ===
    raw_data = {
        "session_info": {
            "timestamp": timestamp,
            "datetime": datetime.now().isoformat(),
            "agent_type": "tabu_search",
            "reason_for_stop": reason,
            "duration_seconds": elapsed,
            "total_steps": step
        },
        "game_performance": {
            "total_reward": float(episode_reward),
            "average_reward_per_step": avg_reward_per_step,
            "max_reward": detailed_stats["max_reward"],
            "min_reward": detailed_stats["min_reward"],
            "steps_per_second": steps_per_second,
            "final_game_state": game_state,
            "badge_start": badge_start,
            "badge_end": badge_end,
            "pokemon_lost": pokemon_lost
        },
        "tabu_search_metrics": {
            "tabu_list_size": len(agent.tabu_list),
            "iteration_count": agent.iteration_count,
            "best_solution_quality": agent.best_solution_quality,
            "current_solution_quality": agent.current_solution_quality,
            "stuck_counter": agent.stuck_counter,
            "tabu_tenure": agent.tabu_tenure,
            "aspiration_threshold": agent.aspiration_threshold
        },
        "exploration_analysis": exploration_metrics,
        "scenario_analysis": {
            "total_detections": sum(scenario_detections.values()),
            "scenario_distribution": scenario_detections,
            "scenario_percentages": {
                scenario: (count / max(sum(scenario_detections.values()), 1)) * 100 
                for scenario, count in scenario_detections.items()
            }
        },
        "action_analysis": {
            "total_actions": len(action_history),
            "action_distribution": {str(i): action_history.count(i) for i in range(7)},
            "action_percentages": {
                str(i): (action_history.count(i) / max(len(action_history), 1)) * 100 
                for i in range(7)
            },
            "recent_actions": action_history[-20:] if len(action_history) >= 20 else action_history,
            "action_patterns": {
                "most_used_action": max(range(7), key=lambda x: action_history.count(x)) if action_history else 0,
                "action_diversity": len(set(action_history)) if action_history else 0
            }
        },
        "temporal_analysis": {
            "reward_progression": reward_history[-50:] if len(reward_history) >= 50 else reward_history,
            "position_diversity": len(set(position_history)) if position_history else 0,
            "exploration_efficiency": exploration_metrics.get('exploration_efficiency', 0),
            "time_analysis": {
                "avg_time_per_100_steps": sum(detailed_stats["time_per_100_steps"]) / max(len(detailed_stats["time_per_100_steps"]), 1),
                "total_time_measurements": len(detailed_stats["time_per_100_steps"])
            }
        },
        "system_performance": {
            "memory_usage_mb": mem_info.rss / 1024 / 1024,
            "avg_memory_mb": avg_memory,
            "memory_history": detailed_stats["memory_usage_history"][-20:],
            "cpu_usage_percent": psutil.cpu_percent(),
            "avg_cpu_percent": avg_cpu,
            "cpu_history": detailed_stats["cpu_usage_history"][-20:]
        },
        "raw_data": {
            "action_history": action_history,
            "reward_history": reward_history,
            "position_history": position_history[-100:] if len(position_history) >= 100 else position_history
        }
    }
    
    # Guardar JSON
    json_file = results_dir / f"tabu_search_raw_data_{timestamp}.json"
    with open(json_file, 'w') as f:
        json.dump(raw_data, f, indent=2, default=str)
    
    # === 2. RESUMEN (CSV) ===
    csv_data = {
        'timestamp': timestamp,
        'agent_type': 'tabu_search',
        'duration_seconds': elapsed,
        'total_steps': step,
        'total_reward': episode_reward,
        'avg_reward_per_step': avg_reward_per_step,
        'steps_per_second': steps_per_second,
        'badge_start': badge_start,
        'badge_end': badge_end,
        'badge_delta': badge_end - badge_start,
        'final_pcount': game_state.get('pcount', 0),
        'final_level': game_state.get('level', 1),
        'pokemon_lost': pokemon_lost,
        'remaining_team_hp': '|'.join(str(hp) for hp in team_hp_end) if team_hp_end else 'n/a',
        'tabu_list_size': len(agent.tabu_list),
        'best_solution_quality': agent.best_solution_quality,
        'unique_positions': exploration_metrics.get('unique_positions_visited', 0),
        'exploration_efficiency': exploration_metrics.get('exploration_efficiency', 0),
        'stuck_episodes': agent.stuck_counter,
        'items_used_summary': json.dumps(items_used_detail),
        'most_used_action': max(range(7), key=lambda x: action_history.count(x)) if action_history else 0,
        'action_diversity': len(set(action_history)) if action_history else 0,
        'exploration_scenario_pct': scenario_detections.get('exploration', 0) / max(sum(scenario_detections.values()), 1) * 100,
        'battle_scenario_pct': scenario_detections.get('battle', 0) / max(sum(scenario_detections.values()), 1) * 100,
        'stuck_scenario_pct': scenario_detections.get('stuck', 0) / max(sum(scenario_detections.values()), 1) * 100,
        'avg_memory_mb': avg_memory,
        'avg_cpu_percent': avg_cpu,
        'reason_for_stop': reason
    }
    
    csv_file = results_dir / f"tabu_search_summary_{timestamp}.csv"
    with open(csv_file, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_data.keys())
        writer.writeheader()
        writer.writerow(csv_data)
    
    # === 3. REPORTE LEGIBLE (MARKDOWN) ===
    team_hp_str = ", ".join(str(hp) for hp in team_hp_end) if team_hp_end else "n/a"
    item_usage_str = ", ".join(f"{item_id}: -{count}" for item_id, count in items_used_detail.items()) if items_used_detail else "sin consumo"

    markdown_content = f"""# Reporte de M√©tricas - Tabu Search Agent

## üìä Informaci√≥n de la Sesi√≥n
- **Fecha y Hora**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Agente**: Tabu Search
- **Duraci√≥n**: {elapsed:.1f} segundos ({elapsed/60:.1f} minutos)
- **Pasos Totales**: {step:,}
- **Raz√≥n de Parada**: {reason}

## üéÆ Rendimiento del Juego
- **Recompensa Total**: {episode_reward:.2f}
- **Recompensa Promedio por Paso**: {avg_reward_per_step:.4f}
- **Recompensa M√°xima**: {detailed_stats['max_reward']:.2f}
- **Recompensa M√≠nima**: {detailed_stats['min_reward']:.2f}
- **Pasos por Segundo**: {steps_per_second:.2f}

## üîç M√©tricas Espec√≠ficas de Tabu Search
- **Tama√±o de Lista Tab√∫**: {len(agent.tabu_list)}
- **Iteraciones Totales**: {agent.iteration_count:,}
- **Mejor Calidad de Soluci√≥n**: {agent.best_solution_quality:.4f}
- **Calidad Actual**: {agent.current_solution_quality:.4f}
- **Episodios de Atascamiento**: {agent.stuck_counter}
- **Tenure Tab√∫**: {agent.tabu_tenure}
- **Umbral de Aspiraci√≥n**: {agent.aspiration_threshold:.2f}

## üó∫Ô∏è An√°lisis de Exploraci√≥n
- **Posiciones √önicas Visitadas**: {exploration_metrics.get('unique_positions_visited', 0):,}
- **Visitas Totales**: {exploration_metrics.get('total_position_visits', 0):,}
- **Eficiencia de Exploraci√≥n**: {exploration_metrics.get('exploration_efficiency', 0):.2%}
- **Diversidad de Posiciones**: {len(set(position_history)):,}

## üéØ Distribuci√≥n de Escenarios
"""
    
    total_scenarios = sum(scenario_detections.values())
    for scenario, count in scenario_detections.items():
        percentage = (count / max(total_scenarios, 1)) * 100
        markdown_content += f"- **{scenario.title()}**: {count:,} detecciones ({percentage:.1f}%)\n"
    
    markdown_content += f"""
## üéÆ An√°lisis de Acciones
- **Acciones Totales**: {len(action_history):,}
- **Diversidad de Acciones**: {len(set(action_history))} de 7 posibles
- **Acci√≥n M√°s Usada**: {max(range(7), key=lambda x: action_history.count(x)) if action_history else 0}

### Distribuci√≥n de Acciones:
"""
    
    action_names = ['‚Üì Down', '‚Üê Left', '‚Üí Right', '‚Üë Up', 'üÖ∞ A Button', 'üÖ± B Button', '‚è∏ Start']
    for i in range(7):
        count = action_history.count(i)
        percentage = (count / max(len(action_history), 1)) * 100
        markdown_content += f"- **{action_names[i]}**: {count:,} ({percentage:.1f}%)\n"
    
    markdown_content += f"""
## üéØ Estado Final del Juego
- **Medallas**: {badge_start} ‚Üí {badge_end} (Œî {badge_end - badge_start})
- **Pok√©mon Capturados**: {game_state.get('pcount', 0)}
- **Nivel**: {game_state.get('level', 1)}
- **Pok√©mon Debilitados**: {pokemon_lost}
- **HP del Equipo**: {team_hp_str}
- **Objetos Consumidos**: {item_usage_str}
- **Posici√≥n**: ({game_state.get('x', 0)}, {game_state.get('y', 0)})

## üíª Rendimiento del Sistema
- **Uso de Memoria**: {mem_info.rss / 1024 / 1024:.1f} MB
- **Memoria Promedio**: {avg_memory:.1f} MB
- **CPU Actual**: {psutil.cpu_percent():.1f}%
- **CPU Promedio**: {avg_cpu:.1f}%

## üìà M√©tricas de Tiempo
- **Tiempo por 100 Pasos**: {sum(detailed_stats['time_per_100_steps']) / max(len(detailed_stats['time_per_100_steps']), 1):.2f} segundos
- **Eficiencia Temporal**: {(step/elapsed)*60:.0f} pasos/minuto

## üìÅ Archivos Generados
- **Datos Crudos**: `{json_file.name}`
- **Resumen CSV**: `{csv_file.name}`
- **Este Reporte**: `tabu_search_metrics_{timestamp}.md`

---
*Reporte generado autom√°ticamente por el sistema de m√©tricas de Tabu Search*
"""
    
    markdown_file = results_dir / f"tabu_search_metrics_{timestamp}.md"
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"üìä Guardando datos en {results_dir.name}/tabu_search_*_{timestamp}.*")
    print(f"üìÑ Generando reporte en {markdown_file.name}")
    print(f"‚úÖ M√©tricas guardadas exitosamente")
    print(f"üìã Resumen: {step:,} pasos, {episode_reward:.1f} recompensa, {elapsed:.1f}s")

if __name__ == "__main__":
    print("üîç Iniciando Interactive Tabu Search Agent...")
    
    # Configurar manejador de interrupciones
    interrupt_handler = InterruptHandler()
    
    # Configuraci√≥n del entorno (usando misma estructura que epsilon greedy)
    sess_path = Path(f'session_{str(time.time_ns())[:8]}')
    ep_length = 2**23
    env_config = {
        'headless': False,
        'save_final_state': False,
        'early_stop': False,
        'action_freq': 24,
        'init_state': '../init.state',
        'max_steps': ep_length,
        'print_rewards': True,
        'save_video': False,
        'fast_video': True,
        'session_path': sess_path,
        'gb_path': '../PokemonRed.gb',
        'debug': False,
        'sim_frame_dist': 2_000_000.0,
        'extra_buttons': False
    }
    
    print("üìã Configuraci√≥n del entorno lista")
    
    # Inicializar entorno y agente
    try:
        print("üîß Inicializando entorno v2...")
        # Usar la misma estructura que epsilon_greedy_interactive.py
        env = StreamWrapper(
            RedGymEnv(env_config),
            stream_metadata={
                "user": "tabu-search-v2",
                "env_id": 0,
                "color": "#aa4477",
                "extra": "Tabu Search Algorithm Agent",
            }
        )
        
        print("üîß Inicializando agente Tabu Search...")
        agent = TabuSearchAgent(
            tabu_tenure=7,
            max_tabu_size=50,
            aspiration_threshold=1.5,
            scenario_detection_enabled=True
        )
        
        print("‚úÖ Entorno y agente creados correctamente")
        
        print("üîÑ Reseteando entorno...")
        observation, info = env.reset()
        print("‚úÖ Entorno reseteado correctamente")
        
    except Exception as e:
        print(f"‚ùå Error inicializando: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
    
    # Variables para m√©tricas
    process = psutil.Process()
    start_time = time.time()
    step = 0
    episode_reward = 0
    
    # Historiales para m√©tricas
    action_history = []
    reward_history = []
    scenario_detections = {
        "exploration": 0, "battle": 0, "navigation": 0, 
        "progression": 0, "stuck": 0
    }
    position_history = []
    detailed_stats = {
        "max_reward": 0,
        "min_reward": float('inf'),
        "total_actions": 0,
        "unique_positions": set(),
        "time_per_100_steps": [],
        "memory_usage_history": [],
        "cpu_usage_history": []
    }
    
    print("üéÆ Iniciando Pok√©mon Red con m√©tricas mejoradas...")
    print("üìä Capturando m√©tricas en tiempo real...")
    print("‚èπÔ∏è  Presiona Ctrl+C para parar y guardar m√©tricas")
    print("-" * 60)
    
    # Bucle principal del juego
    step_100_timer = time.time()
    
    try:
        while not interrupt_handler.interrupted:
            # Obtener estado del juego desde la observaci√≥n
            try:
                game_state = extract_game_state(observation)
                current_pos = (game_state.get('x', 0), game_state.get('y', 0))
                position_history.append(current_pos)
                detailed_stats["unique_positions"].add(current_pos)
            except:
                game_state = {'hp': 100, 'max_hp': 100, 'level': 1, 'badges': 0, 'pcount': 0}
                current_pos = (0, 0)
            
            # Seleccionar acci√≥n con el agente Tabu Search
            action, decision_info = agent.select_action(observation, game_state)
            
            # Ejecutar acci√≥n en el entorno v2
            observation, reward, done, truncated, info = env.step(action)
            
            # Actualizar m√©tricas del agente
            agent.update_performance(action, reward, observation, game_state)
            
            # Actualizar historiales
            action_history.append(action)
            reward_history.append(reward)
            episode_reward += reward
            step += 1
            
            # Actualizar estad√≠sticas detalladas
            detailed_stats["max_reward"] = max(detailed_stats["max_reward"], reward)
            detailed_stats["min_reward"] = min(detailed_stats["min_reward"], reward)
            detailed_stats["total_actions"] += 1
            
            # Registrar detecci√≥n de escenarios
            scenario = decision_info.get('scenario', 'exploration')
            if scenario in scenario_detections:
                scenario_detections[scenario] += 1
            
            # M√©tricas de sistema cada 100 pasos
            if step % 100 == 0:
                elapsed_100 = time.time() - step_100_timer
                detailed_stats["time_per_100_steps"].append(elapsed_100)
                
                mem_usage = process.memory_info().rss / 1024 / 1024
                detailed_stats["memory_usage_history"].append(mem_usage)
                detailed_stats["cpu_usage_history"].append(psutil.cpu_percent())
                
                step_100_timer = time.time()
            
            # Mostrar progreso cada 500 pasos
            if step % 500 == 0:
                elapsed = time.time() - start_time
                print(f"‚è∞ Tiempo: {elapsed/60:.0f}:{elapsed%60:02.0f} | "
                      f"üèÉ Pasos: {step:,} | "
                      f"üéØ Recompensa: {episode_reward:.1f} | "
                      f"üîç Escenario: {scenario} | "
                      f"üìä Lista Tab√∫: {len(agent.tabu_list)}")
            
            # Condici√≥n de parada por episodio terminado
            if done or truncated:
                print("üèÅ Episodio terminado")
                break
                
    except KeyboardInterrupt:
        print("\nüõë Interrupci√≥n manual detectada")
    except Exception as e:
        print(f"\n‚ùå Error durante ejecuci√≥n: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Guardar m√©tricas al final
        reason = "manual_interrupt" if interrupt_handler.interrupted else "episode_complete"
        save_metrics(agent, env, step, episode_reward, start_time, process, 
                    action_history, reward_history, scenario_detections, 
                    position_history, detailed_stats, reason)
        
        print("üéÆ Cerrando entorno...")
        try:
            env.close()
        except:
            pass
        
        print("‚úÖ Sesi√≥n de Tabu Search completada")