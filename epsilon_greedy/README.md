# Epsilon Greedy Search Algorithm

Esta carpeta contiene la implementaci√≥n del algoritmo **Epsilon Greedy** para Pokemon Red.

## ¬øPor qu√© Epsilon Greedy?

El algoritmo **Eps#### Uso del Triple Demo Mejorado:
```bash
# Ejecutar las tres demos simult√°neamente con identificaci√≥n m√∫ltiple
python epsilon_greedy/run_triple_epsilon_demo.py

# Ver√°s 3 ventanas CLARAMENTE IDENTIFICADAS:
# üì∫ Izquierda: "EPSILON 0.3 MODERADO" + [EPSILON-0.3-MODERADO]
# üì∫ Centro: "EPSILON 0.9 CAOTICO" + [EPSILON-0.9-CAOTICO]
# üì∫ Derecha: "EPSILON VARIABLE INTERACTIVO" + [EPSILON-VARIABLE-INTERACTIVO]
```

#### Identificaci√≥n Sin Elementos Visuales:
Si los t√≠tulos o prefijos no son visibles, consulta:
- **[GUIA_IDENTIFICACION_VISUAL.md](GUIA_IDENTIFICACION_VISUAL.md)** - M√©todos para distinguir agentes por comportamiento
- **[VIDEO_GUION_COMPARACION_AGENTES.md](VIDEO_GUION_COMPARACION_AGENTES.md)** - An√°lisis completo y justificaci√≥n de elecci√≥n

**Resumen R√°pido de Identificaci√≥n:**
- **Epsilon 0.9**: Movimientos completamente err√°ticos y ca√≥ticos (el M√ÅS F√ÅCIL de identificar)
- **PPO**: Movimientos rob√≥ticos y s√∫per eficientes (l√≠nea directa a objetivos)
- **Epsilon 0.3**: Balance visible entre exploraci√≥n y direccionalidad
- **Adaptativo**: Comportamiento "inteligente" que cambia seg√∫n el contexto

Esta funcionalidad hace que la comparaci√≥n visual sea mucho m√°s intuitiva y educativa.* fue seleccionado para este proyecto por las siguientes razones:

### Ventajas del Epsilon Greedy
- **Simplicidad**: Es un algoritmo f√°cil de entender e implementar
- **Balance exploraci√≥n-explotaci√≥n**: Permite controlar el balance entre explorar nuevas acciones y explotar el conocimiento actual
- **Adaptabilidad**: El par√°metro epsilon se puede ajustar seg√∫n las necesidades del entorno
- **Eficiencia computacional**: No requiere estructuras de datos complejas ni c√°lculos intensivos
- **Robustez**: Funciona bien en entornos con recompensas escasas como Pokemon Red

### Comparaci√≥n con otros algoritmos
- **A*** requiere conocimiento previo del entorno y funci√≥n heur√≠stica, que no siempre est√° disponible
- **Tab√∫ Search** necesita memoria adicional para rastrear estados visitados y puede ser m√°s lento
- **Algoritmos evolutivos** requieren poblaciones y m√∫ltiples generaciones, siendo computacionalmente m√°s costosos
- **Q-Learning/DQN** necesitan entrenamiento extenso y pueden requerir m√°s recursos

En Pokemon Red, donde el objetivo es navegar eficientemente sin conocimiento previo completo del entorno, Epsilon Greedy ofrece una soluci√≥n pr√°ctica y efectiva.

## Comparaci√≥n con PPO Preentrenado

### ¬øPor qu√© Epsilon Greedy Adaptativo sobre PPO?

Aunque el **PPO preentrenado** (disponible en `/v2`) es **m√°s r√°pido** para completar Pokemon Red, el **Epsilon Greedy Adaptativo** ofrece ventajas fundamentales:

#### Ventajas del PPO:
- ‚ö° **Velocidad**: Completa objetivos en 2-4 minutos
- üéØ **Eficiencia**: Movimientos optimizados y directos
- üìà **Rendimiento**: Maximiza recompensas acumulativas

#### Limitaciones del PPO:
- üîí **Dependiente de entrenamiento**: Requiere miles de episodios previos
- üé≠ **Caja negra**: Dif√≠cil explicar decisiones espec√≠ficas
- üéÆ **Especializaci√≥n**: Optimizado solo para Pokemon Red espec√≠fico
- üí∞ **Costo computacional**: Entrenamiento requiere recursos significativos
- üîÑ **Falta de adaptabilidad**: No se ajusta a variaciones del entorno

#### Ventajas del Epsilon Greedy Adaptativo:
- üöÄ **Sin entrenamiento**: Funciona inmediatamente "out of the box"
- üîç **Transparencia**: Cada decisi√≥n es explicable mediante heur√≠sticas
- üåç **Generalizable**: Se adapta a Pokemon Blue, hacks, modificaciones
- ‚öñÔ∏è **Robusto**: Funciona en situaciones no vistas previamente
- üí° **Comportamiento humano**: Decisiones similares a las que tomar√≠a un jugador
- üîß **Modificable**: F√°cil ajustar heur√≠sticas seg√∫n necesidades

#### El Factor Crucial: Generalizaci√≥n vs Optimizaci√≥n
- **PPO**: Optimizado para Pokemon Red ‚Üí R√°pido pero especializado
- **Epsilon Adaptativo**: Principios generales ‚Üí Robusto y adaptable

**Conclusi√≥n**: Elegimos Epsilon Greedy Adaptativo porque prioriza **robustez, transparencia y generalizaci√≥n** sobre velocidad pura, principios fundamentales en investigaci√≥n y desarrollo de IA.

## Adaptabilidad del Epsilon: Funcionamiento Detallado

### C√≥mo opera la adaptabilidad en `run_epsilon_greedy_interactive.py`

El agente epsilon greedy implementa un sistema de **adaptabilidad multicapa** que ajusta el comportamiento seg√∫n el contexto del juego:

#### 1. **Detecci√≥n de Escenarios en Tiempo Real**
El agente detecta autom√°ticamente el escenario actual analizando la observaci√≥n del juego:

```python
# Escenarios detectados autom√°ticamente:
- EXPLORATION: Exploraci√≥n general del mapa
- BATTLE: Combate activo 
- NAVIGATION: Navegaci√≥n hacia objetivos espec√≠ficos
- PROGRESSION: Eventos clave (gimnasios, captura de Pok√©mon)
- STUCK: Detecci√≥n de comportamiento repetitivo
```

#### 2. **Heur√≠sticas Adaptativas por Escenario**
Cada escenario tiene pesos heur√≠sticos espec√≠ficos que priorizan diferentes comportamientos:

**Exploraci√≥n** (EXPLORATION):
- Prioriza descubrir nuevas √°reas (exploration=1.5)
- Moderado enfoque en objetivos (objective_distance=0.8)
- Balance en todas las m√©tricas

**Combate** (BATTLE):
- M√°xima prioridad en salud (health_consideration=2.0)
- Enfoque en progresi√≥n de nivel (level_progression=1.5)
- M√≠nima exploraci√≥n (exploration=0.2)

**Navegaci√≥n** (NAVIGATION):
- M√°xima prioridad en distancia a objetivos (objective_distance=2.0)
- Alta familiaridad con el mapa (map_familiarity=1.2)
- Exploraci√≥n reducida pero presente

**Progresi√≥n** (PROGRESSION):
- M√°xima prioridad en eventos clave (event_completion=2.5)
- Alta priorizaci√≥n de nivel (level_progression=2.0)
- Balance entre exploraci√≥n y explotaci√≥n

**Atascado** (STUCK):
- M√°xima exploraci√≥n forzada (exploration=2.0)
- Penalizaci√≥n de familiaridad (map_familiarity=0.3)
- Fomenta salir de patrones repetitivos

#### 3. **C√°lculo Din√°mico de Epsilon**
Aunque epsilon tiene un valor base, el comportamiento efectivo cambia seg√∫n el escenario:

```python
# Proceso de selecci√≥n de acci√≥n:
if random() < epsilon:
    # EXPLORACI√ìN: Acci√≥n aleatoria
    action = random_choice(valid_actions)
else:
    # EXPLOTACI√ìN: Mejor acci√≥n seg√∫n heur√≠sticas del escenario actual
    action = best_heuristic_action(current_scenario)
```

#### 4. **Decay Autom√°tico y Progresivo**
El epsilon se reduce gradualmente durante la ejecuci√≥n:

```python
# Despu√©s de cada acci√≥n:
epsilon = max(epsilon_min, epsilon * epsilon_decay)
# T√≠picamente: epsilon_decay = 0.995, epsilon_min = 0.05
```

#### 5. **Detecci√≥n Inteligente de Progreso**
El sistema monitorea indicadores de progreso para ajustar el comportamiento:

- **Badges obtenidas**: Detecta completar gimnasios
- **Eventos del juego**: Captura de Pok√©mon, obtenci√≥n de √≠tems
- **Suma de niveles**: Progreso en entrenamiento
- **Datos del mapa**: Nuevas √°reas exploradas
- **Tama√±o del equipo**: Pok√©mon en el equipo

#### 6. **Sistema Anti-Estancamiento**
Detecta cuando el agente est√° repitiendo comportamientos:

```python
# Detecci√≥n de bucles:
- Mismas posiciones repetidas
- Mismas acciones en secuencia
- Falta de progreso en m√©tricas clave

# Respuesta autom√°tica:
- Cambio a escenario STUCK
- Aumento temporal de exploraci√≥n
- Penalizaci√≥n de acciones familiares
```

### Ventajas de este Sistema Adaptativo

1. **Respuesta contextual**: El agente se comporta diferente en combate vs exploraci√≥n
2. **Prevenci√≥n de bucles**: Detecta y rompe patrones repetitivos autom√°ticamente  
3. **Optimizaci√≥n progresiva**: Epsilon decrece conforme el agente "aprende" el entorno
4. **Sin entrenamiento previo**: Funciona inmediatamente sin necesidad de entrenamiento
5. **Transparencia**: Cada decisi√≥n se basa en heur√≠sticas comprensibles

### Diferencia con Epsilon Fijo

**Epsilon Fijo** (demos):
- Comportamiento constante durante toda la ejecuci√≥n
- No se adapta al contexto del juego
- √ötil para benchmarks y comparaciones

**Epsilon Adaptativo** (interactive):
- Comportamiento din√°mico seg√∫n el escenario
- Se adapta a la situaci√≥n del juego
- Optimiza rendimiento seg√∫n el contexto
- Previene estancamiento autom√°ticamente

Esta adaptabilidad hace que `run_epsilon_greedy_interactive.py` sea m√°s inteligente y eficiente que una implementaci√≥n con epsilon fijo, permitiendo un comportamiento m√°s humano y estrat√©gico.

## Identificaci√≥n Visual MEJORADA de Ventanas PyBoy

### Triple Demo con Identificaci√≥n Visual M√∫ltiple y Robusta

El script `run_triple_epsilon_demo.py` ejecuta simult√°neamente tres agentes epsilon greedy con **m√∫ltiples m√©todos de identificaci√≥n visual** para eliminar cualquier confusi√≥n:

#### 1. T√≠tulos de Ventana S√∫per Descriptivos:
- **Esquina Superior Izquierda**: `"POKEMON RED ===>>> EPSILON 0.3 MODERADO <<<==== 30-EXPLORA 70-EXPLOTA"`
  - 30% exploraci√≥n, 70% explotaci√≥n  
  - Comportamiento balanceado y estrat√©gico

- **Centro Superior**: `"POKEMON RED ===>>> EPSILON 0.9 CAOTICO <<<==== 90-EXPLORA 10-EXPLOTA"`
  - 90% exploraci√≥n, 10% explotaci√≥n
  - Comportamiento muy exploratorio y aleatorio

- **Esquina Superior Derecha**: `"POKEMON RED ===>>> EPSILON VARIABLE INTERACTIVO <<<==== ADAPTATIVO"`
  - Epsilon adaptativo seg√∫n escenario
  - Comportamiento que puedes modificar en tiempo real

#### 2. Posicionamiento Autom√°tico de Ventanas:
- **Demo 0.3**: Posici√≥n (100, 100) - Esquina superior izquierda
- **Demo 0.9**: Posici√≥n (500, 100) - Centro superior  
- **Demo Variable**: Posici√≥n (900, 100) - Esquina superior derecha

#### 3. Prefijos √önicos en Salida de Consola:
- **Demo 0.3**: `[EPSILON-0.3-MODERADO]` en todas las l√≠neas de output
- **Demo 0.9**: `[EPSILON-0.9-CAOTICO]` en todas las l√≠neas de output
- **Demo Variable**: `[EPSILON-VARIABLE-INTERACTIVO]` en todas las l√≠neas de output

#### 4. Metadatos del Stream Diferenciados:
- **Epsilon 0.3**: ID `"E03"`, identifier `"MODERADO"`, behavior `"30% Exploraci√≥n - 70% Explotaci√≥n"`
- **Epsilon 0.9**: ID `"E09"`, identifier `"CAOTICO"`, behavior `"90% Exploraci√≥n - 10% Explotaci√≥n"`  
- **Epsilon Variable**: ID `"EVAR"`, identifier `"INTERACTIVO"`, behavior `"Adaptativo seg√∫n escenario detectado"`

#### Ventajas de la Identificaci√≥n Visual Mejorada:
1. **IMPOSIBLE DE CONFUNDIR**: M√∫ltiples m√©todos redundantes de identificaci√≥n
2. **T√≠tulos s√∫per largos**: Inmediatamente visibles en la barra de t√≠tulo
3. **Posiciones diferentes**: Cada ventana aparece en ubicaci√≥n espec√≠fica
4. **Prefijos en consola**: Cada output tiene su identificador √∫nico
5. **Comparaci√≥n f√°cil**: Observa diferencias de comportamiento simult√°neamente
6. **Sin dependencia de colores**: Funciona incluso si los colores no se muestran

#### Uso del Triple Demo Mejorado:
```bash
# Ejecutar las tres demos simult√°neamente con identificaci√≥n m√∫ltiple
python epsilon_greedy/run_triple_epsilon_demo.py

# Ver√°s 3 ventanas CLARAMENTE IDENTIFICADAS:
# ÔøΩ Izquierda: "EPSILON 0.3 MODERADO" + [EPSILON-0.3-MODERADO]
# ÔøΩ Centro: "EPSILON 0.9 CAOTICO" + [EPSILON-0.9-CAOTICO]
# ÔøΩ Derecha: "EPSILON VARIABLE INTERACTIVO" + [EPSILON-VARIABLE-INTERACTIVO]
```

Esta funcionalidad hace que la comparaci√≥n visual sea mucho m√°s intuitiva y educativa.

## Archivos

- `epsilon_greedy_agent.py` - Implementaci√≥n original del agente epsilon greedy que funciona muy bien
- `epsilon_variable_agent.py` - Versi√≥n avanzada con epsilon configurable para estudiar impacto en rendimiento
- `run_epsilon_greedy_interactive.py` - Script interactivo para ejecutar el agente original
- `demo_pyboy_epsilon_03.py` - Demo autom√°tico con epsilon fijo 0.3 (exploraci√≥n moderada)
- `demo_pyboy_epsilon_09.py` - Demo autom√°tico con epsilon fijo 0.9 (exploraci√≥n muy alta)
- `test_epsilon_variants.py` - Script para probar diferentes valores de epsilon

## Diferencias entre los Scripts Principales

### `run_epsilon_greedy_interactive.py`
- **Prop√≥sito**: Script principal interactivo para uso general
- **Epsilon**: Variable seg√∫n el escenario detectado (adaptativo)
- **Control**: Permite interacci√≥n manual y control fino
- **Objetivo**: Ejecutar hasta obtener el primer Pok√©mon o elegir inicial
- **Uso**: Desarrollo y testing principal del algoritmo

### `demo_pyboy_epsilon_03.py`
- **Prop√≥sito**: Demostraci√≥n autom√°tica con comportamiento balanceado
- **Epsilon**: Fijo en 0.3 (30% exploraci√≥n, 70% explotaci√≥n)
- **Control**: Completamente autom√°tico, sin intervenci√≥n del usuario
- **Objetivo**: Mostrar comportamiento moderado y eficiente
- **Uso**: Demonstraciones y comparaciones de rendimiento

### `demo_pyboy_epsilon_09.py`
- **Prop√≥sito**: Demostraci√≥n autom√°tica con comportamiento exploratorio extremo
- **Epsilon**: Fijo en 0.9 (90% exploraci√≥n, 10% explotaci√≥n)
- **Control**: Completamente autom√°tico, sin intervenci√≥n del usuario
- **Objetivo**: Mostrar comportamiento altamente exploratorio y ca√≥tico
- **Uso**: An√°lisis de comportamiento exploratorio extremo

### M√©tricas Compartidas
Todos los scripts generan m√©tricas id√©nticas en la carpeta `/results`:
- **Markdown**: Informes detallados con estad√≠sticas completas
- **JSON**: Datos crudos para an√°lisis program√°tico
- **CSV**: Resumen de m√©tricas para importar en hojas de c√°lculo

## Variaciones de Epsilon

El archivo `epsilon_variable_agent.py` incluye configuraciones predefinidas:

- **very_high_exploration** (Œµ=0.9): 90% exploraci√≥n - casi aleatorio
- **high_exploration** (Œµ=0.7): 70% exploraci√≥n - mucha exploraci√≥n  
- **balanced** (Œµ=0.5): 50% exploraci√≥n - enfoque balanceado
- **moderate_exploitation** (Œµ=0.3): 30% exploraci√≥n - m√°s explotaci√≥n
- **low_exploration** (Œµ=0.1): 10% exploraci√≥n - principalmente explotaci√≥n
- **very_low_exploration** (Œµ=0.05): 5% exploraci√≥n - casi pura explotaci√≥n
- **pure_exploitation** (Œµ=0.01): 1% exploraci√≥n - casi greedy

## Uso

### Ejecutar agente principal (adaptativo):
```bash
python epsilon_greedy/run_epsilon_greedy_interactive.py
```

### Ejecutar demo con epsilon moderado (0.3):
```bash
python epsilon_greedy/demo_pyboy_epsilon_03.py
```

### Ejecutar demo con epsilon alto (0.9):
```bash
python epsilon_greedy/demo_pyboy_epsilon_09.py
```

### Ejecutar triple demo simult√°neo con identificaci√≥n visual:
```bash
python epsilon_greedy/run_triple_epsilon_demo.py
```

### Probar diferentes valores de epsilon:
```bash
python epsilon_greedy/test_epsilon_variants.py
```

### Crear agente con epsilon espec√≠fico:
```python
from epsilon_greedy.epsilon_variable_agent import VariableEpsilonGreedyAgent
agent = VariableEpsilonGreedyAgent(env, epsilon=0.3)  # 30% exploraci√≥n
```

### Usar configuraciones predefinidas:
```python
from epsilon_greedy.epsilon_variable_agent import create_agent_with_preset
agent = create_agent_with_preset(env, 'balanced')  # Œµ=0.5
```

## Rendimiento

El epsilon greedy original ha demostrado **excelente rendimiento** en Pokemon Red. Los experimentos con diferentes valores de epsilon permiten estudiar:

- **Alto epsilon** ‚Üí M√°s exploraci√≥n, descubre nuevas √°reas pero puede ser err√°tico
- **Bajo epsilon** ‚Üí M√°s explotaci√≥n, m√°s eficiente pero puede quedarse atascado
- **Epsilon balanceado** ‚Üí Combina exploraci√≥n y explotaci√≥n efectivamente