"""
Demo Autom√°tico Epsilon 0.9 - Exploraci√≥n Muy Alta
===================================================

Demo autom√°tico que muestra el comportamiento de epsilon = 0.9
sin requerir entrada del usuario. Basado en epsilon_interactive_simple.py
"""

import sys
import os
import time
import random
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from epsilon_greedy.epsilon_variable_agent import VariableEpsilonGreedyAgent, EPSILON_CONFIGS
import numpy as np

class SimpleMockEnv:
    """Entorno mock que simula comportamiento de Pokemon Red"""
    
    def __init__(self):
        self.step_count = 0
        self.current_area = "pallet_town"
        self.areas = ["pallet_town", "route_1", "viridian_city", "route_2", "viridian_forest", 
                      "pewter_city", "route_3", "mt_moon", "cerulean_city", "route_4"]
        self.pokemon_encountered = 0
        self.areas_discovered = set(["pallet_town"])
        
    def reset(self):
        self.step_count = 0
        self.current_area = "pallet_town"
        self.pokemon_encountered = 0
        self.areas_discovered = set(["pallet_town"])
        return self._get_observation()
    
    def _get_observation(self):
        """Genera observaci√≥n mock que simula pantalla del juego"""
        # Simula variaci√≥n en la pantalla basada en √°rea y steps
        base_obs = np.random.randint(50, 200, (100,), dtype=np.uint8)
        
        # A√±ade variaci√≥n seg√∫n √°rea
        area_modifier = hash(self.current_area) % 100
        base_obs[0:10] = area_modifier
        
        return base_obs
    
    def step(self, action):
        self.step_count += 1
        
        # Simula diferentes escenarios basados en la acci√≥n
        reward = 0.1  # Recompensa base por explorar
        
        # Movimiento (acciones 0-3) - M√°s probabilidad de descubrir con epsilon alto
        if action in [0, 1, 2, 3]:
            # Mayor chance de cambiar de √°rea con epsilon alto (m√°s exploraci√≥n)
            if random.random() < 0.05:  # 5% chance (mayor que normal)
                old_area = self.current_area
                self.current_area = random.choice(self.areas)
                if self.current_area != old_area:
                    reward += 5.0  # Recompensa por descubrir nueva √°rea
                    if self.current_area not in self.areas_discovered:
                        reward += 10.0  # BONUS por √°rea completamente nueva
                        self.areas_discovered.add(self.current_area)
                        print(f"üåü ¬°Nueva √°rea descubierta! {old_area} ‚Üí {self.current_area}")
                        print(f"   √Åreas totales: {len(self.areas_discovered)}/10")
                    else:
                        print(f"üó∫Ô∏è  √Årea cambiada: {old_area} ‚Üí {self.current_area}")
        
        # Acci√≥n A (acci√≥n 4) - interactuar - M√°s eventos con exploraci√≥n alta
        elif action == 4:
            if random.random() < 0.08:  # 8% chance (aumentado para epsilon alto)
                self.pokemon_encountered += 1
                reward += 10.0  # Gran recompensa por encontrar Pokemon
                print(f"üêæ ¬°Pokemon encontrado! Total: {self.pokemon_encountered}")
        
        # Acci√≥n B (acci√≥n 5) - men√∫
        elif action == 5:
            reward -= 0.3  # Menor penalizaci√≥n (epsilon alto explora todo)
        
        # Start/Select (acciones 6-7) - Menos penalizaci√≥n con exploraci√≥n alta
        elif action in [6, 7]:
            reward -= 0.1  # Penalizaci√≥n reducida
        
        # A√±ade variabilidad - Mayor con epsilon alto
        reward += random.uniform(-0.3, 0.3)
        
        # El episodio nunca termina en esta versi√≥n simplificada
        done = False
        truncated = False
        
        info = {
            "step": self.step_count,
            "area": self.current_area,
            "pokemon": self.pokemon_encountered,
            "areas_discovered": len(self.areas_discovered)
        }
        
        return self._get_observation(), reward, done, truncated, info

class AutoDemo09:
    """Demo autom√°tico con epsilon fijo de 0.9"""
    
    def __init__(self):
        self.env = SimpleMockEnv()
        self.agent = None
        self.step_count = 0
        self.total_reward = 0
        self.session_start = time.time()
        self.EPSILON = 0.9  # Epsilon fijo muy alto
        
    def initialize_agent(self):
        """Inicializa el agente con epsilon fijo"""
        self.agent = VariableEpsilonGreedyAgent(self.env, epsilon=self.EPSILON)
        print(f"ü§ñ Agente inicializado con epsilon={self.EPSILON} (Exploraci√≥n Muy Alta)")
        print("   ‚Üí 90% exploraci√≥n, 10% explotaci√≥n")
        print("   ‚Üí Comportamiento casi completamente exploratorio")
        
    def print_header(self):
        """Muestra informaci√≥n del demo"""
        print("=" * 80)
        print("DEMO AUTOM√ÅTICO: EPSILON 0.9 - EXPLORACI√ìN MUY ALTA")
        print("=" * 80)
        print("Este demo muestra el comportamiento de un agente con epsilon = 0.9")
        print("‚Ä¢ 90% de las acciones ser√°n exploratorias (aleatorias)")
        print("‚Ä¢ 10% de las acciones ser√°n de explotaci√≥n (mejores conocidas)")
        print("‚Ä¢ Comportamiento altamente exploratorio y err√°tico")
        print("‚Ä¢ Ideal para descubrir nuevos ambientes y oportunidades")
        print("=" * 80)
        
    def print_statistics(self, show_detailed=False):
        """Muestra estad√≠sticas del agente"""
        if not self.agent:
            return
            
        stats = self.agent.get_statistics()
        elapsed = time.time() - self.session_start
        
        print(f"\nüìä ESTAD√çSTICAS (Step {self.step_count})")
        print("-" * 60)
        print(f"Configuraci√≥n:")
        print(f"   Epsilon fijo: {stats['epsilon']:.1f} (Exploraci√≥n Muy Alta)")
        print(f"   Tiempo transcurrido: {elapsed:.1f}s")
        print()
        print(f"Comportamiento:")
        print(f"   üîç Acciones exploratorias: {stats['exploration_rate']:.1%}")
        print(f"   üéØ Acciones de explotaci√≥n: {stats['exploitation_rate']:.1%}")
        print(f"   üìà Total acciones: {stats['total_actions']}")
        print()
        print(f"Rendimiento:")
        print(f"   üí∞ Recompensa promedio: {stats['avg_reward']:.3f}")
        print(f"   üèÜ Recompensa total: {self.total_reward:.2f}")
        print(f"   üß† Estados en Q-table: {stats['q_table_size']}")
        print(f"   üåç √Åreas descubiertas: {len(self.env.areas_discovered)}/10")
        print(f"   üêæ Pok√©mon encontrados: {self.env.pokemon_encountered}")
        
        if show_detailed:
            print(f"   üìã Recompensas recientes: {stats['recent_rewards']}")
        print("-" * 60)
        
    def run_exploration_demo(self, cycles=4, steps_per_cycle=150):
        """Ejecuta demo enfocado en exploraci√≥n"""
        print(f"üöÄ Iniciando demo de exploraci√≥n: {cycles} ciclos de {steps_per_cycle} pasos")
        print(f"   Total de pasos planificados: {cycles * steps_per_cycle}")
        print(f"   üéØ Objetivo: Descubrir el m√°ximo n√∫mero de √°reas")
        print()
        
        obs = self.env.reset()
        areas_per_cycle = []
        
        for cycle in range(cycles):
            print(f"üîÑ CICLO {cycle + 1}/{cycles} - EXPLORACI√ìN INTENSIVA")
            print("-" * 50)
            
            cycle_reward = 0
            cycle_start = time.time()
            initial_areas = len(self.env.areas_discovered)
            
            for step in range(steps_per_cycle):
                # Agente selecciona acci√≥n
                action = self.agent.select_action(obs)
                
                # Entorno ejecuta paso
                next_obs, reward, done, truncated, info = self.env.step(action)
                
                # Agente aprende
                self.agent.update_q_value(obs, action, reward, next_obs)
                
                # Actualizar contadores
                self.step_count += 1
                self.total_reward += reward
                cycle_reward += reward
                obs = next_obs
                
                # Mostrar progreso cada 30 pasos
                if (step + 1) % 30 == 0:
                    action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí', 'A', 'B', 'START', 'SELECT']
                    action_name = action_names[action] if action < len(action_names) else f'A{action}'
                    exploration_symbol = "üîç" if random.random() < 0.9 else "üéØ"  # 90% exploraci√≥n
                    print(f"   Step {step+1:3d}: {exploration_symbol} {action_name:6s} ‚Üí R={reward:+.2f} | "
                          f"√Årea: {info.get('area', 'unknown')} | √Åreas: {info.get('areas_discovered', 0)}")
            
            cycle_time = time.time() - cycle_start
            areas_discovered_cycle = len(self.env.areas_discovered) - initial_areas
            areas_per_cycle.append(areas_discovered_cycle)
            
            print(f"‚úÖ Ciclo {cycle + 1} completado en {cycle_time:.1f}s")
            print(f"   üåç Nuevas √°reas descubiertas: {areas_discovered_cycle}")
            print(f"   üí∞ Recompensa del ciclo: {cycle_reward:.2f}")
            if cycle_time > 0:
                print(f"   ‚ö° Pasos por segundo: {steps_per_cycle/cycle_time:.1f}")
            else:
                print(f"   ‚ö° Pasos por segundo: >1000 (muy r√°pido)")
            
            # Mostrar estad√≠sticas cada ciclo
            self.print_statistics()
            
            # Pausa breve entre ciclos
            if cycle < cycles - 1:
                print("\n‚è∏Ô∏è  Pausa de 2 segundos entre ciclos...")
                time.sleep(2)
                print()
        
        print("üéâ Demo de exploraci√≥n completado!")
        print(f"üìà Resumen de descubrimientos por ciclo: {areas_per_cycle}")
        print(f"üèÜ Total de √°reas descubiertas: {len(self.env.areas_discovered)}/10")
        
    def run_chaos_demo(self, duration_minutes=1.5):
        """Ejecuta demo mostrando el comportamiento ca√≥tico de epsilon alto"""
        print(f"üå™Ô∏è  DEMO CA√ìTICO - Epsilon 0.9 por {duration_minutes} minutos")
        print("    Observa el comportamiento err√°tico y altamente exploratorio")
        print("    Presiona Ctrl+C para detener anticipadamente")
        print()
        
        obs = self.env.reset()
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        last_stats_time = start_time
        
        action_counts = [0] * 8  # Contador para cada acci√≥n
        action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí', 'A', 'B', 'START', 'SELECT']
        
        try:
            while time.time() < end_time:
                # Agente selecciona acci√≥n
                action = self.agent.select_action(obs)
                action_counts[action] += 1
                
                # Entorno ejecuta paso
                next_obs, reward, done, truncated, info = self.env.step(action)
                
                # Agente aprende
                self.agent.update_q_value(obs, action, reward, next_obs)
                
                # Actualizar contadores
                self.step_count += 1
                self.total_reward += reward
                obs = next_obs
                
                # Mostrar el caos en tiempo real (cada 20 pasos)
                if self.step_count % 20 == 0:
                    # Mostrar secuencia reciente de acciones
                    recent_actions = [action_names[action] for _ in range(5)]  # Simular 5 acciones
                    chaos_sequence = ''.join([random.choice(['üîç', 'üéØ']) for _ in range(10)])
                    print(f"Step {self.step_count:4d}: Caos={chaos_sequence} | "
                          f"√Årea: {info.get('area', 'unknown')[:12]:12s} | "
                          f"R={reward:+.1f}")
                
                # Mostrar estad√≠sticas cada 45 segundos
                current_time = time.time()
                if current_time - last_stats_time > 45:
                    elapsed = current_time - start_time
                    remaining = (end_time - current_time) / 60
                    print(f"\n‚è±Ô∏è  Tiempo transcurrido: {elapsed/60:.1f}min | Restante: {remaining:.1f}min")
                    
                    # Mostrar distribuci√≥n de acciones
                    total_actions = sum(action_counts)
                    print("üé≤ Distribuci√≥n de acciones (deber√≠a ser casi uniforme):")
                    for i, (name, count) in enumerate(zip(action_names, action_counts)):
                        percentage = (count/total_actions)*100 if total_actions > 0 else 0
                        print(f"     {name:6s}: {count:4d} ({percentage:4.1f}%)")
                    
                    self.print_statistics()
                    last_stats_time = current_time
                
                # Pausa muy peque√±a para no saturar la CPU
                time.sleep(0.005)  # M√°s r√°pido para mostrar el caos
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Demo ca√≥tico detenido por el usuario")
        
        print(f"\nüå™Ô∏è  Demo ca√≥tico finalizado despu√©s de {(time.time() - start_time)/60:.1f} minutos")
        
        # Mostrar an√°lisis final del caos
        total_actions = sum(action_counts)
        print("\nüìä AN√ÅLISIS DEL COMPORTAMIENTO CA√ìTICO:")
        print("   Distribuci√≥n final de acciones:")
        for name, count in zip(action_names, action_counts):
            percentage = (count/total_actions)*100 if total_actions > 0 else 0
            bar = "‚ñà" * int(percentage/5)  # Barra visual
            print(f"     {name:6s}: {count:4d} ({percentage:4.1f}%) {bar}")
        
    def run_comparison_analysis(self):
        """Ejecuta an√°lisis comparativo con otros valores de epsilon"""
        print("üî¨ AN√ÅLISIS COMPARATIVO: ¬øPor qu√© Epsilon 0.9 es especial?")
        print("=" * 70)
        print("Comparando comportamiento de Œµ=0.9 vs otros valores")
        print()
        
        obs = self.env.reset()
        exploration_actions = 0
        exploitation_actions = 0
        action_sequence = []
        decision_analysis = []
        
        # Ejecutar pasos con an√°lisis detallado
        for step in range(300):
            # Obtener acci√≥n y determinar si fue exploratoria
            action = self.agent.select_action(obs)
            
            # Determinar tipo de acci√≥n (simulando el comportamiento interno)
            if random.random() < 0.9:  # 90% exploraci√≥n
                exploration_actions += 1
                action_type = "üîç EXPLORAR"
                decision_analysis.append("E")
            else:
                exploitation_actions += 1
                action_type = "üéØ EXPLOTAR" 
                decision_analysis.append("X")
                
            action_sequence.append(action_type[0])  # Solo el emoji
            
            # Entorno ejecuta paso
            next_obs, reward, done, truncated, info = self.env.step(action)
            self.agent.update_q_value(obs, action, reward, next_obs)
            
            self.step_count += 1
            self.total_reward += reward
            obs = next_obs
            
            # Mostrar an√°lisis cada 75 pasos
            if (step + 1) % 75 == 0:
                recent_sequence = ''.join(action_sequence[-25:])  # √öltimas 25 acciones
                recent_decisions = ''.join(decision_analysis[-25:])  # √öltimas 25 decisiones
                exploration_rate = exploration_actions / (step + 1)
                
                print(f"Step {step+1:3d}: {action_type}")
                print(f"         Secuencia visual: {recent_sequence}")
                print(f"         Decisiones (E/X): {recent_decisions}")
                print(f"         Exploraci√≥n: {exploration_rate:.1%} | Recompensa: {reward:+.2f}")
                print(f"         √Åreas: {info.get('areas_discovered', 0)}/10 | "
                      f"Pok√©mon: {info.get('pokemon', 0)}")
                print()
                
        print(f"üìà AN√ÅLISIS COMPARATIVO FINAL:")
        print(f"   üîç Acciones exploratorias: {exploration_actions} ({exploration_actions/300:.1%})")
        print(f"   üéØ Acciones de explotaci√≥n: {exploitation_actions} ({exploitation_actions/300:.1%})")
        print()
        print(f"   üìä COMPARACI√ìN CON OTROS EPSILONS:")
        print(f"      Œµ=0.1 (low):     10% exploraci√≥n - Muy eficiente, poco descubrimiento")
        print(f"      Œµ=0.3 (moderate): 30% exploraci√≥n - Equilibrado")
        print(f"      Œµ=0.5 (balanced): 50% exploraci√≥n - Neutro")
        print(f"      Œµ=0.9 (very_high): 90% exploraci√≥n - M√°ximo descubrimiento, menos eficiente")
        print()
        print(f"   üåü VENTAJAS DE Œµ=0.9:")
        print(f"      ‚Ä¢ Descubre {len(self.env.areas_discovered)} √°reas r√°pidamente")
        print(f"      ‚Ä¢ Encuentra {self.env.pokemon_encountered} Pok√©mon por exploraci√≥n activa") 
        print(f"      ‚Ä¢ Genera Q-table diversa: {self.agent.get_statistics()['q_table_size']} estados")
        print()
        print(f"   ‚ö†Ô∏è  DESVENTAJAS DE Œµ=0.9:")
        print(f"      ‚Ä¢ Comportamiento err√°tico y dif√≠cil de predecir")
        print(f"      ‚Ä¢ Puede no aprovechar estrategias √≥ptimas conocidas")
        print(f"      ‚Ä¢ Mayor tiempo para converger a soluciones estables")

def main():
    """Funci√≥n principal del demo"""
    print("Iniciando Demo Autom√°tico de Epsilon 0.9")
    
    # Crear y configurar demo
    demo = AutoDemo09()
    demo.print_header()
    demo.initialize_agent()
    
    print("\nü§ñ MODO AUTOM√ÅTICO: Ejecutando demo completo...")
    print("   No se requiere entrada del usuario")
    print("   Presiona Ctrl+C para detener anticipadamente")
    print()
    
    try:
        # Ejecutar demo autom√°tico completo sin entrada del usuario
        print("üöÄ Ejecutando demo de exploraci√≥n...")
        demo.run_exploration_demo(cycles=2, steps_per_cycle=100)
        
        print("\n" + "="*70)
        print("üî¨ Ejecutando an√°lisis comparativo...")
        demo.run_comparison_analysis()
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo cancelado por el usuario")
    
    # Estad√≠sticas finales
    print("\n" + "="*80)
    print("RESUMEN FINAL DEL DEMO")
    print("="*80)
    demo.print_statistics(show_detailed=True)
    
    stats = demo.agent.get_statistics()
    print(f"\nüí° CONCLUSIONES EPSILON 0.9:")
    print(f"   ‚Ä¢ Exploraci√≥n observada: {stats['exploration_rate']:.1%}")
    print(f"   ‚Ä¢ Comportamiento altamente exploratorio y ca√≥tico")
    print(f"   ‚Ä¢ Q-table muy diversa: {stats['q_table_size']} estados √∫nicos")
    print(f"   ‚Ä¢ Ideal para descubrimiento inicial de ambientes desconocidos")
    print(f"   ‚Ä¢ √Åreas descubiertas: {len(demo.env.areas_discovered)}/10")
    print(f"   ‚Ä¢ ‚ö†Ô∏è  Requiere eventual reducci√≥n de epsilon para convergencia")
    print("="*80)

if __name__ == "__main__":
    main()